{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "standing-values",
   "metadata": {},
   "source": [
    "# Proyecto Final - Deep Learning\n",
    "## Parte 1 - Feed Forward Network\n",
    "\n",
    "**Curso:** Statistical Learning II\n",
    "\n",
    "**Catedrático:** Ing. Luis Leal\n",
    "\n",
    "**Estudiante:** Dany Rafael Díaz Lux (21000864)\n",
    "\n",
    "**Objetivo:** Entrenar una red Feed Forward (convencional) para aproximar una función que modele la relación entre ciertas variables independientes y una o más variables dependientes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "marine-phone",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from keras.layers import Dense, Dropout, BatchNormalization\n",
    "from keras.models import Sequential, load_model\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import matplotlib.pylab as plt\n",
    "import datetime as dt\n",
    "import math\n",
    "import numpy as np \n",
    "import os.path\n",
    "import pandas as pd\n",
    "import random as python_random\n",
    "import sklearn.metrics as mts\n",
    "import tensorflow as tf\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amended-increase",
   "metadata": {},
   "source": [
    "## Conjunto de datos: California Housing Data (1990)\n",
    "Es un conjunto de datos utilizado en el libro: https://github.com/ageron/handson-ml/tree/master/datasets/housing, para ilustrar un flujo de trabajo completo de un proyecto de aprendizaje máquina.\n",
    "\n",
    "Este conjunto de datos puede ser descargado aquí: https://www.kaggle.com/datasets/harrywang/housing?select=housing.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "disciplinary-opposition",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>housing_median_age</th>\n",
       "      <th>total_rooms</th>\n",
       "      <th>total_bedrooms</th>\n",
       "      <th>population</th>\n",
       "      <th>households</th>\n",
       "      <th>median_income</th>\n",
       "      <th>ocean_proximity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-122.23</td>\n",
       "      <td>37.88</td>\n",
       "      <td>41.0</td>\n",
       "      <td>880.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>322.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>8.3252</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-122.22</td>\n",
       "      <td>37.86</td>\n",
       "      <td>21.0</td>\n",
       "      <td>7099.0</td>\n",
       "      <td>1106.0</td>\n",
       "      <td>2401.0</td>\n",
       "      <td>1138.0</td>\n",
       "      <td>8.3014</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-122.24</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1467.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>496.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>7.2574</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-122.25</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1274.0</td>\n",
       "      <td>235.0</td>\n",
       "      <td>558.0</td>\n",
       "      <td>219.0</td>\n",
       "      <td>5.6431</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-122.25</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1627.0</td>\n",
       "      <td>280.0</td>\n",
       "      <td>565.0</td>\n",
       "      <td>259.0</td>\n",
       "      <td>3.8462</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n",
       "0    -122.23     37.88                41.0        880.0           129.0   \n",
       "1    -122.22     37.86                21.0       7099.0          1106.0   \n",
       "2    -122.24     37.85                52.0       1467.0           190.0   \n",
       "3    -122.25     37.85                52.0       1274.0           235.0   \n",
       "4    -122.25     37.85                52.0       1627.0           280.0   \n",
       "\n",
       "   population  households  median_income ocean_proximity  \n",
       "0       322.0       126.0         8.3252        NEAR BAY  \n",
       "1      2401.0      1138.0         8.3014        NEAR BAY  \n",
       "2       496.0       177.0         7.2574        NEAR BAY  \n",
       "3       558.0       219.0         5.6431        NEAR BAY  \n",
       "4       565.0       259.0         3.8462        NEAR BAY  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0    452600.0\n",
       "1    358500.0\n",
       "2    352100.0\n",
       "3    341300.0\n",
       "4    342200.0\n",
       "Name: median_house_value, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cargar información\n",
    "infoCasas = pd.read_csv('housing.csv')\n",
    "# Separación de variables independientes (características) y variable de interés (etiqueta)\n",
    "caracteristicas = infoCasas.iloc[:,[0,1,2,3,4,5,6,7,9]]\n",
    "etiquetas = infoCasas.iloc[:,8]\n",
    "display(caracteristicas.head())\n",
    "display(etiquetas.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distant-seattle",
   "metadata": {},
   "source": [
    "* La variable objetivo a predecir será: __median_house_value__.\n",
    "* El resto de columnas serán variables dependientes.\n",
    "\n",
    "## Pre-procesamiento de la información\n",
    "Se realizarán actividades necesarias a la información incluso para proyectos de Redes Neuronales Artificiales como: Manejo de información faltante, codificación de información, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "peaceful-jacob",
   "metadata": {},
   "source": [
    "### Detectar columnas con información faltante (NaN), determinar porcentaje, e imputación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "center-forty",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Porcentaje NaN en  total_bedrooms : 1.0 %\n"
     ]
    }
   ],
   "source": [
    "# Listar columnas con valores NaN\n",
    "columnasConNaN = infoCasas.columns[infoCasas.isna().any()].tolist()\n",
    "for columna in columnasConNaN:\n",
    "    print('Porcentaje NaN en ', columna, ':', round(100 * \\\n",
    "            len(infoCasas[infoCasas[columna].isna()]) / len(infoCasas), 2), '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "overall-breeding",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dany\\anaconda3\\envs\\py_galileo_2021\\lib\\site-packages\\pandas\\core\\series.py:4463: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().fillna(\n"
     ]
    }
   ],
   "source": [
    "# Se imputará la información faltante con la media de los datos en la columna \"total_bedrooms\"\n",
    "caracteristicas['total_bedrooms'].fillna(value=caracteristicas['total_bedrooms'].mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "professional-vegetation",
   "metadata": {},
   "source": [
    "### Codificación de columna categórica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "searching-executive",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>housing_median_age</th>\n",
       "      <th>total_rooms</th>\n",
       "      <th>total_bedrooms</th>\n",
       "      <th>population</th>\n",
       "      <th>households</th>\n",
       "      <th>median_income</th>\n",
       "      <th>ocean_proximity_&lt;1H OCEAN</th>\n",
       "      <th>ocean_proximity_INLAND</th>\n",
       "      <th>ocean_proximity_ISLAND</th>\n",
       "      <th>ocean_proximity_NEAR BAY</th>\n",
       "      <th>ocean_proximity_NEAR OCEAN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-122.23</td>\n",
       "      <td>37.88</td>\n",
       "      <td>41.0</td>\n",
       "      <td>880.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>322.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>8.3252</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-122.22</td>\n",
       "      <td>37.86</td>\n",
       "      <td>21.0</td>\n",
       "      <td>7099.0</td>\n",
       "      <td>1106.0</td>\n",
       "      <td>2401.0</td>\n",
       "      <td>1138.0</td>\n",
       "      <td>8.3014</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-122.24</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1467.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>496.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>7.2574</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-122.25</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1274.0</td>\n",
       "      <td>235.0</td>\n",
       "      <td>558.0</td>\n",
       "      <td>219.0</td>\n",
       "      <td>5.6431</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-122.25</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1627.0</td>\n",
       "      <td>280.0</td>\n",
       "      <td>565.0</td>\n",
       "      <td>259.0</td>\n",
       "      <td>3.8462</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n",
       "0    -122.23     37.88                41.0        880.0           129.0   \n",
       "1    -122.22     37.86                21.0       7099.0          1106.0   \n",
       "2    -122.24     37.85                52.0       1467.0           190.0   \n",
       "3    -122.25     37.85                52.0       1274.0           235.0   \n",
       "4    -122.25     37.85                52.0       1627.0           280.0   \n",
       "\n",
       "   population  households  median_income  ocean_proximity_<1H OCEAN  \\\n",
       "0       322.0       126.0         8.3252                          0   \n",
       "1      2401.0      1138.0         8.3014                          0   \n",
       "2       496.0       177.0         7.2574                          0   \n",
       "3       558.0       219.0         5.6431                          0   \n",
       "4       565.0       259.0         3.8462                          0   \n",
       "\n",
       "   ocean_proximity_INLAND  ocean_proximity_ISLAND  ocean_proximity_NEAR BAY  \\\n",
       "0                       0                       0                         1   \n",
       "1                       0                       0                         1   \n",
       "2                       0                       0                         1   \n",
       "3                       0                       0                         1   \n",
       "4                       0                       0                         1   \n",
       "\n",
       "   ocean_proximity_NEAR OCEAN  \n",
       "0                           0  \n",
       "1                           0  \n",
       "2                           0  \n",
       "3                           0  \n",
       "4                           0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Realizar one hot encoding a columna: \"ocean_proximity\"\n",
    "caracteristicasConOhe = caracteristicas.join(pd.get_dummies(caracteristicas.ocean_proximity, prefix='ocean_proximity'))\n",
    "caracteristicasConOhe = caracteristicasConOhe.loc[:, ~caracteristicasConOhe.columns.isin(['ocean_proximity'])]\n",
    "display(caracteristicasConOhe.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cooperative-anime",
   "metadata": {},
   "source": [
    "### Comentario de división entre datos de entrenamiento y datos de validación\n",
    "* Se utilizará la separación de datos de validación por medio del métodod __.fit__ del modelo __Sequential__ de keras."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "balanced-melbourne",
   "metadata": {},
   "source": [
    "### Escalar características numéricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "younger-valuable",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Características de entrenamiento escaladas (20640, 13)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-1.32783522,  1.05254828,  0.98214266, ...,  0.        ,\n",
       "         1.        ,  0.        ],\n",
       "       [-1.32284391,  1.04318455, -0.60701891, ...,  0.        ,\n",
       "         1.        ,  0.        ],\n",
       "       [-1.33282653,  1.03850269,  1.85618152, ...,  0.        ,\n",
       "         1.        ,  0.        ],\n",
       "       ...,\n",
       "       [-0.8237132 ,  1.77823747, -0.92485123, ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [-0.87362627,  1.77823747, -0.84539315, ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [-0.83369581,  1.75014627, -1.00430931, ...,  0.        ,\n",
       "         0.        ,  0.        ]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Escalar las columnas: longitude, latitude, housing_median_age, total_rooms, \n",
    "# total_bedrooms, population, households, median_income\n",
    "ct = ColumnTransformer([('transformarColumnas', StandardScaler(), \\\n",
    "        ['longitude', 'latitude', 'housing_median_age', 'total_rooms', 'total_bedrooms', \\\n",
    "         'population', 'households', 'median_income'])], remainder='passthrough')\n",
    "caracteristicasEstandarizadas = ct.fit_transform(caracteristicasConOhe)\n",
    "print('Características de entrenamiento escaladas', caracteristicasEstandarizadas.shape)\n",
    "display(caracteristicasEstandarizadas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informative-initial",
   "metadata": {},
   "source": [
    "## Arquitectura de Red Neuronal\n",
    "A continuación se definirán funciones que construirán y llevarán a cabo el entrenamiento de una red neuronal feedforward con distintos hiper-parámetros. De estas funciones es importante destacar que:\n",
    "* Por motivos de reproducibilidad de resultados, utilizará un seed específico al inicio que limitará la aleatoriedad de los modelos producidos.\n",
    "* Guardará en una bitácora los hiperparámetros enviados con el desempeño de las métricas escogidas.\n",
    "* El modelo detendrá el entrenamiento auotmáticamente si el error en los datos de validación no mejorá después de 10 iteraciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "effective-satin",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funciones para manejar bitácora\n",
    "nombreBitacora = 'bitacora_modelos.csv'\n",
    "# Función que determinará si una configuración de un experimento en particular ya se encuentra en la bitácora\n",
    "def existeConfiguracion(configuracion):\n",
    "    # Si existe bitácora, buscar si la configuración ya existe\n",
    "    if os.path.exists(nombreBitacora):\n",
    "        dfBitacora = pd.read_csv(nombreBitacora)\n",
    "        if(not dfBitacora['configuracion'].where(dfBitacora['configuracion'] == configuracion).isna().all()):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# Función que agregará una configuración de un experimento a la bitácora con sus métricas más importantes\n",
    "def manejarBitacora(tipoModelo, configuracion, y_train = None, y_pred_train = None, y_test = None, \\\n",
    "                    y_pred_test = None, errorEntrenamiento = '', errorValidacion = '', epoca='', tiempoEntrenamiento = ''):\n",
    "    dfNewConfiguracion = pd.DataFrame({'tipo_red': [], 'fecha': [], 'configuracion': [], 'segundos_entrenamiento': [], \\\n",
    "                        'epoca': [], 'error_train': [], 'error_test': [], 'accuracy_train': [], 'precision_train': [], \\\n",
    "                        'recall_train': [], 'f1_score_train': [], 'accuracy_test': [], 'precision_test': [], \\\n",
    "                        'recall_test': [], 'f1_score_test': []})\n",
    "    dataConfiguracion = [tipoModelo, dt.datetime.now(), configuracion.replace(',', ';'), tiempoEntrenamiento, \\\n",
    "                         epoca, errorEntrenamiento, errorValidacion]\n",
    "    if(y_train != None and y_pred_train != None and y_test != None and y_pred_test != None):\n",
    "        dataConfiguracion.append(mts.accuracy_score(y_train, y_pred_train))\n",
    "        dataConfiguracion.append(mts.precision_score(y_train, y_pred_train))\n",
    "        dataConfiguracion.append(mts.recall_score(y_train, y_pred_train))\n",
    "        dataConfiguracion.append(mts.f1_score(y_train, y_pred_train))\n",
    "        dataConfiguracion.append(mts.accuracy_score(y_test, y_pred_test))\n",
    "        dataConfiguracion.append(mts.precision_score(y_test, y_pred_test))\n",
    "        dataConfiguracion.append(mts.recall_score(y_test, y_pred_test))\n",
    "        dataConfiguracion.append(mts.f1_score(y_test, y_pred_test))\n",
    "    else:\n",
    "        for i in range(8):\n",
    "            dataConfiguracion.append('---')\n",
    "    dfNewConfiguracion.loc[len(dfNewConfiguracion)] = dataConfiguracion\n",
    "\n",
    "    # Agregar configuración en bitácora\n",
    "    if os.path.exists(nombreBitacora):\n",
    "        dfNewConfiguracion.to_csv(nombreBitacora, mode='a', index=False, header=False)\n",
    "    else:\n",
    "        dfNewConfiguracion.to_csv(nombreBitacora, index=False)\n",
    "\n",
    "# Función que construirá la red neuronal feed forward\n",
    "def construirANN(tamanioEntrada, capas, activaciones, inicializadorKernel = 'glorot_uniform', \\\n",
    "                 inicializadorSesgo='zeros', porcentajesDropout = [], aplicarBatchNormalization = 0):\n",
    "    # Crear cadena de configuración con parámetros más importantes\n",
    "    configuracion = 'capas=' + str(capas) + '_activaciones=' + str(activaciones) + \\\n",
    "        '_inicializadorKernel=' + inicializadorKernel + '_inicializadorSesgo=' + inicializadorSesgo\n",
    "    if(len(porcentajesDropout) > 0):\n",
    "        configuracion += '_porcentajesDropOut=' + str(porcentajesDropout)\n",
    "    if(aplicarBatchNormalization == 1):\n",
    "        configuracion += '_aplicarBatchNormalization=True'\n",
    "    # Especificar seed para reproducir resultados\n",
    "    np.random.seed(21000864)\n",
    "    tf.random.set_seed(21000864)\n",
    "    python_random.seed(21000864)\n",
    "    # Crear red neuronal con información enviada\n",
    "    modeloANN = Sequential()\n",
    "    for i, numNeuronas in enumerate(capas):\n",
    "        # Especificar seed para reproducir resultados\n",
    "        if(i == 0): # primer capa oculta\n",
    "            modeloANN.add(Dense(numNeuronas, input_dim=tamanioEntrada, activation=activaciones[i], \\\n",
    "                               kernel_initializer=inicializadorKernel, bias_initializer=inicializadorSesgo))\n",
    "        else: # Resto de capas\n",
    "            modeloANN.add(Dense(numNeuronas, activation=activaciones[i], \\\n",
    "                                kernel_initializer=inicializadorKernel, bias_initializer=inicializadorSesgo))\n",
    "        \n",
    "        if((i + 1) != len(capas)): # No aplicar a capa de salida\n",
    "            # Aplicar Batch normalization después de activación si bandera está activada\n",
    "            if(aplicarBatchNormalization == 1):\n",
    "                modeloANN.add(BatchNormalization())\n",
    "            # Agregar dropout si valor enviado es mayor a cero\n",
    "            if(len(porcentajesDropout) > 0 and porcentajesDropout[i] > 0.0):\n",
    "                modeloANN.add(Dropout(porcentajesDropout[i]))\n",
    "    \n",
    "    return modeloANN, configuracion\n",
    "    \n",
    "# Función que llevará a cabo el entrenamiento de la red neuronal feed forward\n",
    "def entrenarANN(modeloANN, configuracion, caracteristicas, etiquetas, metricaError, tamanioBatch = 32, \\\n",
    "                optimizador='adam', metricas= None, porcentajeValidacion = 0.15, epocas = 100, \\\n",
    "                paciencia = 10, mostrarResultados = 1, forzarEntrenamiento = 0, puntoRevision = None):\n",
    "    # Crear cadena de configuración para determinar si el experimento ya se realizó\n",
    "    tipoRed = 'ANN'\n",
    "    configuracion = tipoRed + '_' + configuracion + '_batchSize=' + str(tamanioBatch) + \\\n",
    "        '_optimizador=' + optimizador + '_paciencia=' + str(paciencia) + '_epocas=' + str(epocas)\n",
    "    configuracionYaExiste = existeConfiguracion(configuracion.replace(',', ';'))\n",
    "    \n",
    "    datosEntrenamiento = None\n",
    "    # Si no existe configuración en bitácora, realizar experimento y agregar configuración a bitácora con métricas\n",
    "    if(not configuracionYaExiste or forzarEntrenamiento == 1):\n",
    "        # Inicio de entrenamiento\n",
    "        inicio = time.time()\n",
    "        # Especificar seed para reproducir resultados\n",
    "        np.random.seed(21000864)\n",
    "        tf.random.set_seed(21000864)\n",
    "        python_random.seed(21000864)\n",
    "        # Definir parámetros para entrenamiento\n",
    "        modeloANN.compile(loss=metricaError, optimizer=optimizador, metrics=metricas)\n",
    "        # Manejar el criterio para parar temprano\n",
    "        if(paciencia != -1):\n",
    "            implementacionesCallbacks = [EarlyStopping(monitor='val_loss', patience = paciencia, verbose=mostrarResultados)]\n",
    "        else:\n",
    "            implementacionesCallbacks = None\n",
    "        # Agregar puntos de revisión si existen\n",
    "        if(puntoRevision != None):\n",
    "            if(implementacionesCallbacks == None):\n",
    "                implementacionesCallbacks = []\n",
    "            implementacionesCallbacks.append(puntoRevision)\n",
    "        # Entrenamiento\n",
    "        datosEntrenamiento = modeloANN.fit(caracteristicas, etiquetas, batch_size=tamanioBatch, \\\n",
    "                                           validation_split=porcentajeValidacion, epochs=epocas, \\\n",
    "                                           callbacks = implementacionesCallbacks, verbose=mostrarResultados)\n",
    "        # calcular tiempo que tomó la resolución de mínimos cuadrados\n",
    "        fin = time.time()\n",
    "        segundos = fin - inicio\n",
    "    \n",
    "    if(not configuracionYaExiste):\n",
    "        # Errores de entrenamiento y validación\n",
    "        erroresEntrenamiento = datosEntrenamiento.history['loss']\n",
    "        erroresValidacion = datosEntrenamiento.history['val_loss']\n",
    "        manejarBitacora(tipoRed, configuracion, \\\n",
    "                        errorEntrenamiento = math.sqrt(erroresEntrenamiento[len(erroresEntrenamiento)-1]), \\\n",
    "                        errorValidacion = math.sqrt(erroresValidacion[len(erroresValidacion)-1]), \\\n",
    "                        epoca=len(erroresValidacion), tiempoEntrenamiento=segundos)\n",
    "    \n",
    "    return datosEntrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "northern-original",
   "metadata": {},
   "source": [
    "## Primeros experimentos: Número de capas\n",
    "* Se construirán diferentes redes neuronales para experimentar un número conveniente de capas. Con cada capa con 10 neuronas se experimentará con una red de 2 capas ocultas, con 10 capas ocultas y con 50 capas ocultas. Con base en los resultados obtenidos en la bitácora se observará la red que mejor resultados ofrece sin aumentar demasiado el número de parámetros en el modelo y/o el tiempo de entrenamiento.\n",
    "* Hiper parámetros fijos que se utilizarán para estas pruebas: tamaño de batch: 32, épocas: 50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "brilliant-pasta",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 10)                140       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                110       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 261\n",
      "Trainable params: 261\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "549/549 [==============================] - 2s 2ms/step - loss: 56024342528.0000 - val_loss: 56400236544.0000\n",
      "Epoch 2/50\n",
      "549/549 [==============================] - 1s 1ms/step - loss: 55703801856.0000 - val_loss: 56006156288.0000\n",
      "Epoch 3/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 54595436544.0000 - val_loss: 55091331072.0000\n",
      "Epoch 4/50\n",
      "549/549 [==============================] - 1s 1ms/step - loss: 52483383296.0000 - val_loss: 53588643840.0000\n",
      "Epoch 5/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 49341325312.0000 - val_loss: 51507593216.0000\n",
      "Epoch 6/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 45317926912.0000 - val_loss: 48910647296.0000\n",
      "Epoch 7/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 40668631040.0000 - val_loss: 45881831424.0000\n",
      "Epoch 8/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 35701161984.0000 - val_loss: 42511052800.0000\n",
      "Epoch 9/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 30748942336.0000 - val_loss: 38887579648.0000\n",
      "Epoch 10/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 26140596224.0000 - val_loss: 35072421888.0000\n",
      "Epoch 11/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 22111522816.0000 - val_loss: 31112423424.0000\n",
      "Epoch 12/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 18769997824.0000 - val_loss: 27068946432.0000\n",
      "Epoch 13/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 16112759808.0000 - val_loss: 23137691648.0000\n",
      "Epoch 14/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 14072785920.0000 - val_loss: 19547648000.0000\n",
      "Epoch 15/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 12538323968.0000 - val_loss: 16463094784.0000\n",
      "Epoch 16/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 11374595072.0000 - val_loss: 13959060480.0000\n",
      "Epoch 17/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 10457806848.0000 - val_loss: 11963072512.0000\n",
      "Epoch 18/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 9707801600.0000 - val_loss: 10367001600.0000\n",
      "Epoch 19/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 9074177024.0000 - val_loss: 9075291136.0000\n",
      "Epoch 20/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 8533391872.0000 - val_loss: 8034918400.0000\n",
      "Epoch 21/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 8066689024.0000 - val_loss: 7158817280.0000\n",
      "Epoch 22/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 7668975104.0000 - val_loss: 6454456832.0000\n",
      "Epoch 23/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 7327006720.0000 - val_loss: 5869070336.0000\n",
      "Epoch 24/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 7035246592.0000 - val_loss: 5399217152.0000\n",
      "Epoch 25/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 6784420352.0000 - val_loss: 5031923712.0000\n",
      "Epoch 26/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 6566449664.0000 - val_loss: 4726355456.0000\n",
      "Epoch 27/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 6374917632.0000 - val_loss: 4498193408.0000\n",
      "Epoch 28/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 6205671424.0000 - val_loss: 4305818624.0000\n",
      "Epoch 29/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 6055844864.0000 - val_loss: 4154534912.0000\n",
      "Epoch 30/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 5921562112.0000 - val_loss: 4026058240.0000\n",
      "Epoch 31/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 5801029632.0000 - val_loss: 3927452928.0000\n",
      "Epoch 32/50\n",
      "549/549 [==============================] - 1s 1ms/step - loss: 5689995776.0000 - val_loss: 3847483392.0000\n",
      "Epoch 33/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 5588589056.0000 - val_loss: 3777000960.0000\n",
      "Epoch 34/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 5499154944.0000 - val_loss: 3723264000.0000\n",
      "Epoch 35/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 5421285888.0000 - val_loss: 3677131776.0000\n",
      "Epoch 36/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 5353329664.0000 - val_loss: 3642791936.0000\n",
      "Epoch 37/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 5293643776.0000 - val_loss: 3614014976.0000\n",
      "Epoch 38/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 5242084352.0000 - val_loss: 3592759808.0000\n",
      "Epoch 39/50\n",
      "549/549 [==============================] - 1s 1ms/step - loss: 5197192192.0000 - val_loss: 3578311680.0000\n",
      "Epoch 40/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 5159523328.0000 - val_loss: 3565604096.0000\n",
      "Epoch 41/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 5127292416.0000 - val_loss: 3556204544.0000\n",
      "Epoch 42/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 5099913728.0000 - val_loss: 3548190208.0000\n",
      "Epoch 43/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 5075846144.0000 - val_loss: 3542963456.0000\n",
      "Epoch 44/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 5055603200.0000 - val_loss: 3538140928.0000\n",
      "Epoch 45/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 5037450752.0000 - val_loss: 3534994944.0000\n",
      "Epoch 46/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 5021806592.0000 - val_loss: 3530373120.0000\n",
      "Epoch 47/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 5007455744.0000 - val_loss: 3526212864.0000\n",
      "Epoch 48/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4995079168.0000 - val_loss: 3522896384.0000\n",
      "Epoch 49/50\n",
      "549/549 [==============================] - 1s 1ms/step - loss: 4983144448.0000 - val_loss: 3520207360.0000\n",
      "Epoch 50/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4973058048.0000 - val_loss: 3517166080.0000\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_3 (Dense)             (None, 10)                140       \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 10)                110       \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 10)                110       \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 10)                110       \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 10)                110       \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 10)                110       \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 10)                110       \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,141\n",
      "Trainable params: 1,141\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "549/549 [==============================] - 2s 2ms/step - loss: 27047249920.0000 - val_loss: 5215436800.0000\n",
      "Epoch 2/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 5949537792.0000 - val_loss: 4180044544.0000\n",
      "Epoch 3/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 5048553472.0000 - val_loss: 3633853952.0000\n",
      "Epoch 4/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4764063232.0000 - val_loss: 3504971008.0000\n",
      "Epoch 5/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4631557120.0000 - val_loss: 3461106176.0000\n",
      "Epoch 6/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4546075648.0000 - val_loss: 3412245760.0000\n",
      "Epoch 7/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4482338304.0000 - val_loss: 3417894656.0000\n",
      "Epoch 8/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4422862848.0000 - val_loss: 3317604608.0000\n",
      "Epoch 9/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4365497344.0000 - val_loss: 3311751168.0000\n",
      "Epoch 10/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4309098496.0000 - val_loss: 3294696704.0000\n",
      "Epoch 11/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4256016896.0000 - val_loss: 3308773376.0000\n",
      "Epoch 12/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4206664448.0000 - val_loss: 3314130432.0000\n",
      "Epoch 13/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4152625920.0000 - val_loss: 3316651008.0000\n",
      "Epoch 14/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4106337792.0000 - val_loss: 3356848128.0000\n",
      "Epoch 15/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4087514624.0000 - val_loss: 3361255424.0000\n",
      "Epoch 16/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4051605760.0000 - val_loss: 3314725632.0000\n",
      "Epoch 17/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4021089024.0000 - val_loss: 3366913280.0000\n",
      "Epoch 18/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4000253952.0000 - val_loss: 3316391168.0000\n",
      "Epoch 19/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3995238912.0000 - val_loss: 3338683136.0000\n",
      "Epoch 20/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3972144640.0000 - val_loss: 3311035648.0000\n",
      "Epoch 20: early stopping\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_14 (Dense)            (None, 10)                140       \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_20 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_21 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_22 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_23 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_24 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_25 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_26 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_27 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_28 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_29 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_30 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_31 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_32 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_33 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_34 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_35 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_36 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_37 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_38 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_39 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_40 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_41 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_42 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_43 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_44 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_45 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_46 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_47 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_48 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_49 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_50 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_51 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_52 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_53 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_54 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_55 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_56 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_57 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_58 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_59 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_60 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_61 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_62 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_63 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_64 (Dense)            (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,541\n",
      "Trainable params: 5,541\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "549/549 [==============================] - 6s 6ms/step - loss: 25150113792.0000 - val_loss: 13462587392.0000\n",
      "Epoch 2/50\n",
      "549/549 [==============================] - 2s 5ms/step - loss: 13446401024.0000 - val_loss: 13656715264.0000\n",
      "Epoch 3/50\n",
      "549/549 [==============================] - 2s 4ms/step - loss: 13489436672.0000 - val_loss: 13529225216.0000\n",
      "Epoch 4/50\n",
      "549/549 [==============================] - 2s 4ms/step - loss: 13432271872.0000 - val_loss: 13600812032.0000\n",
      "Epoch 5/50\n",
      "549/549 [==============================] - 2s 4ms/step - loss: 13443657728.0000 - val_loss: 13465776128.0000\n",
      "Epoch 6/50\n",
      "549/549 [==============================] - 2s 5ms/step - loss: 13460631552.0000 - val_loss: 13475049472.0000\n",
      "Epoch 7/50\n",
      "549/549 [==============================] - 2s 4ms/step - loss: 13521530880.0000 - val_loss: 13429206016.0000\n",
      "Epoch 8/50\n",
      "549/549 [==============================] - 2s 4ms/step - loss: 13423222784.0000 - val_loss: 13417934848.0000\n",
      "Epoch 9/50\n",
      "549/549 [==============================] - 2s 4ms/step - loss: 13459391488.0000 - val_loss: 13451081728.0000\n",
      "Epoch 10/50\n",
      "549/549 [==============================] - 2s 4ms/step - loss: 13418501120.0000 - val_loss: 13996779520.0000\n",
      "Epoch 11/50\n",
      "549/549 [==============================] - 2s 4ms/step - loss: 13482378240.0000 - val_loss: 13463708672.0000\n",
      "Epoch 12/50\n",
      "549/549 [==============================] - 2s 4ms/step - loss: 13442018304.0000 - val_loss: 14319483904.0000\n",
      "Epoch 13/50\n",
      "549/549 [==============================] - 2s 4ms/step - loss: 13443528704.0000 - val_loss: 13616263168.0000\n",
      "Epoch 14/50\n",
      "549/549 [==============================] - 2s 4ms/step - loss: 13538635776.0000 - val_loss: 13543113728.0000\n",
      "Epoch 15/50\n",
      "549/549 [==============================] - 2s 4ms/step - loss: 13421335552.0000 - val_loss: 13420578816.0000\n",
      "Epoch 16/50\n",
      "549/549 [==============================] - 2s 4ms/step - loss: 13419922432.0000 - val_loss: 13551434752.0000\n",
      "Epoch 17/50\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 13420957696.0000 - val_loss: 13543399424.0000\n",
      "Epoch 18/50\n",
      "549/549 [==============================] - 2s 4ms/step - loss: 13425828864.0000 - val_loss: 14503812096.0000\n",
      "Epoch 18: early stopping\n"
     ]
    }
   ],
   "source": [
    "# Experimentos para número de capas\n",
    "experimentoNumeroCapas = [3, 11, 51]\n",
    "for experimento in experimentoNumeroCapas:\n",
    "    listaCapas = []\n",
    "    listaActivaciones = []\n",
    "    for iCapa in range(experimento - 1):\n",
    "        listaCapas.append(10)\n",
    "        listaActivaciones.append('relu')\n",
    "    listaCapas.append(1)\n",
    "    listaActivaciones.append('linear')\n",
    "    redAnn, configuracion = construirANN(caracteristicasEstandarizadas.shape[1], listaCapas, listaActivaciones)\n",
    "    redAnn.summary()\n",
    "    entrenarANN(redAnn, configuracion, caracteristicasEstandarizadas, etiquetas, 'mean_squared_error', tamanioBatch=32, epocas=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "naughty-horizon",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>configuracion</th>\n",
       "      <th>error_test</th>\n",
       "      <th>error_train</th>\n",
       "      <th>segundos_entrenamiento</th>\n",
       "      <th>epoca</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>ANN_capas=[64; 64; 64; 64; 64; 64; 64; 64; 1]_activaciones=['elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1</td>\n",
       "      <td>53376.532371</td>\n",
       "      <td>51400.826141</td>\n",
       "      <td>67.506442</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ANN_capas=[64; 64; 64; 64; 64; 64; 64; 64; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1</td>\n",
       "      <td>54231.385747</td>\n",
       "      <td>51958.811996</td>\n",
       "      <td>63.929625</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>ANN_capas=[32; 50; 64; 80; 80; 64; 50; 32; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1</td>\n",
       "      <td>54464.082256</td>\n",
       "      <td>52447.311771</td>\n",
       "      <td>65.858972</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>ANN_capas=[64; 64; 64; 64; 64; 64; 64; 64; 1]_activaciones=['elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_porcentajesDropOut=[0.05; 0.05; 0.05; 0.05; 0.05; 0.05; 0.05; 0.05]_batchSize=32_optimizador=adam_paciencia=-1_epocas=90</td>\n",
       "      <td>54796.879327</td>\n",
       "      <td>53966.714482</td>\n",
       "      <td>148.886218</td>\n",
       "      <td>90.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>ANN_capas=[64; 64; 64; 64; 64; 64; 64; 64; 1]_activaciones=['selu'; 'selu'; 'selu'; 'selu'; 'selu'; 'selu'; 'selu'; 'selu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1</td>\n",
       "      <td>55036.204520</td>\n",
       "      <td>50085.477017</td>\n",
       "      <td>63.393345</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>ANN_capas=[50; 50; 50; 50; 50; 50; 50; 50; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1</td>\n",
       "      <td>55458.095748</td>\n",
       "      <td>52839.030423</td>\n",
       "      <td>58.103801</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>ANN_capas=[80; 80; 64; 64; 50; 50; 32; 32; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1</td>\n",
       "      <td>55472.535331</td>\n",
       "      <td>51841.128383</td>\n",
       "      <td>63.146074</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>ANN_capas=[64; 64; 64; 64; 64; 64; 64; 64; 1]_activaciones=['elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=nadam_paciencia=-1</td>\n",
       "      <td>55548.626626</td>\n",
       "      <td>51087.213273</td>\n",
       "      <td>73.071667</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>ANN_capas=[64; 64; 64; 64; 64; 64; 64; 64; 1]_activaciones=['elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=rmsprop_paciencia=-1</td>\n",
       "      <td>55743.102174</td>\n",
       "      <td>51816.125367</td>\n",
       "      <td>60.515572</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>ANN_capas=[64; 64; 64; 64; 64; 64; 64; 64; 1]_activaciones=['elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'linear']_inicializadorKernel=glorot_normal_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1</td>\n",
       "      <td>55989.743061</td>\n",
       "      <td>51369.158062</td>\n",
       "      <td>66.137771</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>ANN_capas=[64; 64; 64; 64; 64; 64; 64; 64; 1]_activaciones=['elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'linear']_inicializadorKernel=random_normal_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1</td>\n",
       "      <td>56144.034198</td>\n",
       "      <td>54215.765678</td>\n",
       "      <td>66.496849</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>ANN_capas=[64; 64; 64; 64; 64; 64; 64; 64; 1]_activaciones=['elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_porcentajesDropOut=[0.1; 0.1; 0.1; 0.1; 0.1; 0.1; 0.1; 0.1]_batchSize=32_optimizador=adam_paciencia=-1</td>\n",
       "      <td>56164.539987</td>\n",
       "      <td>60837.986028</td>\n",
       "      <td>83.210063</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ANN_capas=[8; 8; 8; 8; 8; 8; 8; 8; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=10</td>\n",
       "      <td>56168.076912</td>\n",
       "      <td>65505.463589</td>\n",
       "      <td>52.175863</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ANN_capas=[80; 80; 80; 80; 80; 80; 80; 80; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1</td>\n",
       "      <td>56252.450400</td>\n",
       "      <td>50596.551345</td>\n",
       "      <td>68.058731</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ANN_capas=[10; 10; 10; 10; 10; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=10</td>\n",
       "      <td>56579.769105</td>\n",
       "      <td>65503.054219</td>\n",
       "      <td>46.783819</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                         configuracion  \\\n",
       "33                                                                                ANN_capas=[64; 64; 64; 64; 64; 64; 64; 64; 1]_activaciones=['elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1   \n",
       "14                                                                        ANN_capas=[64; 64; 64; 64; 64; 64; 64; 64; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1   \n",
       "24                                                                        ANN_capas=[32; 50; 64; 80; 80; 64; 50; 32; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1   \n",
       "60  ANN_capas=[64; 64; 64; 64; 64; 64; 64; 64; 1]_activaciones=['elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_porcentajesDropOut=[0.05; 0.05; 0.05; 0.05; 0.05; 0.05; 0.05; 0.05]_batchSize=32_optimizador=adam_paciencia=-1_epocas=90   \n",
       "32                                                                        ANN_capas=[64; 64; 64; 64; 64; 64; 64; 64; 1]_activaciones=['selu'; 'selu'; 'selu'; 'selu'; 'selu'; 'selu'; 'selu'; 'selu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1   \n",
       "18                                                                        ANN_capas=[50; 50; 50; 50; 50; 50; 50; 50; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1   \n",
       "22                                                                        ANN_capas=[80; 80; 64; 64; 50; 50; 32; 32; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1   \n",
       "38                                                                               ANN_capas=[64; 64; 64; 64; 64; 64; 64; 64; 1]_activaciones=['elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=nadam_paciencia=-1   \n",
       "34                                                                             ANN_capas=[64; 64; 64; 64; 64; 64; 64; 64; 1]_activaciones=['elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=rmsprop_paciencia=-1   \n",
       "46                                                                                 ANN_capas=[64; 64; 64; 64; 64; 64; 64; 64; 1]_activaciones=['elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'linear']_inicializadorKernel=glorot_normal_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1   \n",
       "47                                                                                 ANN_capas=[64; 64; 64; 64; 64; 64; 64; 64; 1]_activaciones=['elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'linear']_inicializadorKernel=random_normal_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1   \n",
       "40                    ANN_capas=[64; 64; 64; 64; 64; 64; 64; 64; 1]_activaciones=['elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_porcentajesDropOut=[0.1; 0.1; 0.1; 0.1; 0.1; 0.1; 0.1; 0.1]_batchSize=32_optimizador=adam_paciencia=-1   \n",
       "7                                                                                 ANN_capas=[8; 8; 8; 8; 8; 8; 8; 8; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=10   \n",
       "15                                                                        ANN_capas=[80; 80; 80; 80; 80; 80; 80; 80; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1   \n",
       "3                                                                                                             ANN_capas=[10; 10; 10; 10; 10; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=10   \n",
       "\n",
       "      error_test   error_train  segundos_entrenamiento  epoca  \n",
       "33  53376.532371  51400.826141               67.506442   50.0  \n",
       "14  54231.385747  51958.811996               63.929625   50.0  \n",
       "24  54464.082256  52447.311771               65.858972   50.0  \n",
       "60  54796.879327  53966.714482              148.886218   90.0  \n",
       "32  55036.204520  50085.477017               63.393345   50.0  \n",
       "18  55458.095748  52839.030423               58.103801   50.0  \n",
       "22  55472.535331  51841.128383               63.146074   50.0  \n",
       "38  55548.626626  51087.213273               73.071667   50.0  \n",
       "34  55743.102174  51816.125367               60.515572   50.0  \n",
       "46  55989.743061  51369.158062               66.137771   50.0  \n",
       "47  56144.034198  54215.765678               66.496849   50.0  \n",
       "40  56164.539987  60837.986028               83.210063   50.0  \n",
       "7   56168.076912  65505.463589               52.175863   50.0  \n",
       "15  56252.450400  50596.551345               68.058731   50.0  \n",
       "3   56579.769105  65503.054219               46.783819   50.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Función para mostrar resultados de bitácora\n",
    "def mostrarResultadosBitacora(tipoModelo):\n",
    "    dfBitacora = pd.read_csv(nombreBitacora)\n",
    "    dfBitacoraFiltrada = dfBitacora[dfBitacora['tipo_red'] == tipoModelo]\n",
    "    pd.set_option(\"display.max_colwidth\", 1000)\n",
    "    display(dfBitacoraFiltrada[['configuracion', 'error_test', 'error_train', 'segundos_entrenamiento', 'epoca']]\\\n",
    "            .sort_values(by='error_test').head(15))\n",
    "    \n",
    "mostrarResultadosBitacora('ANN')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "developed-better",
   "metadata": {},
   "source": [
    "De los experimentos anteriores, la mejor métrica en los datos de validación la presentó una red neuronal de 10 capas ocultas, se realizarán algunos experimentos más con 5, 15 y 20 capas ocultas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "hourly-authority",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_65 (Dense)            (None, 10)                140       \n",
      "                                                                 \n",
      " dense_66 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_67 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_68 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_69 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_70 (Dense)            (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 591\n",
      "Trainable params: 591\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 45913034752.0000 - val_loss: 15710573568.0000\n",
      "Epoch 2/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 8532599296.0000 - val_loss: 5699063296.0000\n",
      "Epoch 3/50\n",
      "549/549 [==============================] - 1s 1ms/step - loss: 6285244416.0000 - val_loss: 4353213440.0000\n",
      "Epoch 4/50\n",
      "549/549 [==============================] - 1s 1ms/step - loss: 5548746752.0000 - val_loss: 3849273344.0000\n",
      "Epoch 5/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 5148410368.0000 - val_loss: 3703222272.0000\n",
      "Epoch 6/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4949907968.0000 - val_loss: 3590649600.0000\n",
      "Epoch 7/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4848325120.0000 - val_loss: 3541003776.0000\n",
      "Epoch 8/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4780021248.0000 - val_loss: 3489543424.0000\n",
      "Epoch 9/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4723427840.0000 - val_loss: 3489832960.0000\n",
      "Epoch 10/50\n",
      "549/549 [==============================] - 1s 1ms/step - loss: 4682557440.0000 - val_loss: 3438371072.0000\n",
      "Epoch 11/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4655361536.0000 - val_loss: 3419573248.0000\n",
      "Epoch 12/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4632456704.0000 - val_loss: 3409599744.0000\n",
      "Epoch 13/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4606221824.0000 - val_loss: 3395224832.0000\n",
      "Epoch 14/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4590442496.0000 - val_loss: 3396600832.0000\n",
      "Epoch 15/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4573807104.0000 - val_loss: 3402842368.0000\n",
      "Epoch 16/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4553510912.0000 - val_loss: 3375444736.0000\n",
      "Epoch 17/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4537997312.0000 - val_loss: 3358321408.0000\n",
      "Epoch 18/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4522365440.0000 - val_loss: 3373249280.0000\n",
      "Epoch 19/50\n",
      "549/549 [==============================] - 1s 1ms/step - loss: 4513716224.0000 - val_loss: 3343672832.0000\n",
      "Epoch 20/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4495531008.0000 - val_loss: 3340609792.0000\n",
      "Epoch 21/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4490900480.0000 - val_loss: 3343505152.0000\n",
      "Epoch 22/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4475186176.0000 - val_loss: 3353645824.0000\n",
      "Epoch 23/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4469280256.0000 - val_loss: 3317188352.0000\n",
      "Epoch 24/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4459755520.0000 - val_loss: 3315448576.0000\n",
      "Epoch 25/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4446733824.0000 - val_loss: 3298768640.0000\n",
      "Epoch 26/50\n",
      "549/549 [==============================] - 1s 1ms/step - loss: 4437922304.0000 - val_loss: 3298996480.0000\n",
      "Epoch 27/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4430495744.0000 - val_loss: 3316158208.0000\n",
      "Epoch 28/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4421974016.0000 - val_loss: 3299375104.0000\n",
      "Epoch 29/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4416680960.0000 - val_loss: 3292659968.0000\n",
      "Epoch 30/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4409663488.0000 - val_loss: 3286470656.0000\n",
      "Epoch 31/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4403878400.0000 - val_loss: 3272504064.0000\n",
      "Epoch 32/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4395021824.0000 - val_loss: 3263783936.0000\n",
      "Epoch 33/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4385417728.0000 - val_loss: 3273132800.0000\n",
      "Epoch 34/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4377479680.0000 - val_loss: 3271926272.0000\n",
      "Epoch 35/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4370408448.0000 - val_loss: 3250111488.0000\n",
      "Epoch 36/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4366132736.0000 - val_loss: 3245942272.0000\n",
      "Epoch 37/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4359651328.0000 - val_loss: 3261617408.0000\n",
      "Epoch 38/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4351233536.0000 - val_loss: 3254963712.0000\n",
      "Epoch 39/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4345562624.0000 - val_loss: 3240596480.0000\n",
      "Epoch 40/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4342857216.0000 - val_loss: 3221443840.0000\n",
      "Epoch 41/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4331610624.0000 - val_loss: 3232670976.0000\n",
      "Epoch 42/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4333251072.0000 - val_loss: 3221260800.0000\n",
      "Epoch 43/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4321469440.0000 - val_loss: 3226308608.0000\n",
      "Epoch 44/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4319312384.0000 - val_loss: 3261416704.0000\n",
      "Epoch 45/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4312080896.0000 - val_loss: 3249463552.0000\n",
      "Epoch 46/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4307273728.0000 - val_loss: 3215453440.0000\n",
      "Epoch 47/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4303983104.0000 - val_loss: 3214059264.0000\n",
      "Epoch 48/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4300399104.0000 - val_loss: 3204472320.0000\n",
      "Epoch 49/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4292957696.0000 - val_loss: 3198596096.0000\n",
      "Epoch 50/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4290650112.0000 - val_loss: 3201270272.0000\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_71 (Dense)            (None, 10)                140       \n",
      "                                                                 \n",
      " dense_72 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_73 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_74 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_75 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_76 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_77 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_78 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_79 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_80 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_81 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_82 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_83 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_84 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_85 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_86 (Dense)            (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,691\n",
      "Trainable params: 1,691\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "549/549 [==============================] - 3s 3ms/step - loss: 21759021056.0000 - val_loss: 4509490176.0000\n",
      "Epoch 2/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 5414107648.0000 - val_loss: 3763279616.0000\n",
      "Epoch 3/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4833501184.0000 - val_loss: 3554163968.0000\n",
      "Epoch 4/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 4614726656.0000 - val_loss: 3456122880.0000\n",
      "Epoch 5/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4523874304.0000 - val_loss: 3454535424.0000\n",
      "Epoch 6/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4438233088.0000 - val_loss: 3395175936.0000\n",
      "Epoch 7/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4391109120.0000 - val_loss: 3383325952.0000\n",
      "Epoch 8/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4322044928.0000 - val_loss: 3386841600.0000\n",
      "Epoch 9/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4246787840.0000 - val_loss: 3383888128.0000\n",
      "Epoch 10/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4188673280.0000 - val_loss: 3498400512.0000\n",
      "Epoch 11/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4149872640.0000 - val_loss: 3416030976.0000\n",
      "Epoch 12/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4081307392.0000 - val_loss: 3521643776.0000\n",
      "Epoch 13/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4052214016.0000 - val_loss: 3424944640.0000\n",
      "Epoch 14/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4025678848.0000 - val_loss: 3411034112.0000\n",
      "Epoch 15/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4015409152.0000 - val_loss: 3468513024.0000\n",
      "Epoch 16/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3983148544.0000 - val_loss: 3453326592.0000\n",
      "Epoch 17/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3963712000.0000 - val_loss: 3493365760.0000\n",
      "Epoch 17: early stopping\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_87 (Dense)            (None, 10)                140       \n",
      "                                                                 \n",
      " dense_88 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_89 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_90 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_91 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_92 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_93 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_94 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_95 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_96 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_97 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_98 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_99 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_100 (Dense)           (None, 10)                110       \n",
      "                                                                 \n",
      " dense_101 (Dense)           (None, 10)                110       \n",
      "                                                                 \n",
      " dense_102 (Dense)           (None, 10)                110       \n",
      "                                                                 \n",
      " dense_103 (Dense)           (None, 10)                110       \n",
      "                                                                 \n",
      " dense_104 (Dense)           (None, 10)                110       \n",
      "                                                                 \n",
      " dense_105 (Dense)           (None, 10)                110       \n",
      "                                                                 \n",
      " dense_106 (Dense)           (None, 10)                110       \n",
      "                                                                 \n",
      " dense_107 (Dense)           (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,241\n",
      "Trainable params: 2,241\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "549/549 [==============================] - 3s 3ms/step - loss: 18977560576.0000 - val_loss: 4182225408.0000\n",
      "Epoch 2/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 4912896000.0000 - val_loss: 3581710336.0000\n",
      "Epoch 3/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 4437574144.0000 - val_loss: 3452121088.0000\n",
      "Epoch 4/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 4209128448.0000 - val_loss: 3436852480.0000\n",
      "Epoch 5/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 4091888896.0000 - val_loss: 3478511360.0000\n",
      "Epoch 6/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 3990070016.0000 - val_loss: 3572541184.0000\n",
      "Epoch 7/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 3959743488.0000 - val_loss: 3444718848.0000\n",
      "Epoch 8/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 3899578624.0000 - val_loss: 3510737152.0000\n",
      "Epoch 9/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 3875834624.0000 - val_loss: 3567529216.0000\n",
      "Epoch 10/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 3858276096.0000 - val_loss: 4015320576.0000\n",
      "Epoch 11/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 3863210240.0000 - val_loss: 3619354112.0000\n",
      "Epoch 12/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 3813757952.0000 - val_loss: 3779078400.0000\n",
      "Epoch 13/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 3781156352.0000 - val_loss: 3559198208.0000\n",
      "Epoch 14/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 3768828672.0000 - val_loss: 3532057344.0000\n",
      "Epoch 14: early stopping\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>configuracion</th>\n",
       "      <th>error_test</th>\n",
       "      <th>error_train</th>\n",
       "      <th>segundos_entrenamiento</th>\n",
       "      <th>epoca</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ANN_capas=[10; 10; 10; 10; 10; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=10</td>\n",
       "      <td>56579.769105</td>\n",
       "      <td>65503.054219</td>\n",
       "      <td>46.783819</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ANN_capas=[10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=10</td>\n",
       "      <td>57541.599283</td>\n",
       "      <td>63024.952519</td>\n",
       "      <td>25.163197</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ANN_capas=[10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=10</td>\n",
       "      <td>59104.701674</td>\n",
       "      <td>62958.017758</td>\n",
       "      <td>24.140647</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ANN_capas=[10; 10; 1]_activaciones=['relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=10</td>\n",
       "      <td>59305.700232</td>\n",
       "      <td>70519.912422</td>\n",
       "      <td>45.313531</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ANN_capas=[10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=10</td>\n",
       "      <td>59431.114275</td>\n",
       "      <td>61390.786540</td>\n",
       "      <td>22.732815</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ANN_capas=[10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=10</td>\n",
       "      <td>120431.773615</td>\n",
       "      <td>115869.879020</td>\n",
       "      <td>47.140597</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          configuracion  \\\n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              ANN_capas=[10; 10; 10; 10; 10; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=10   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  ANN_capas=[10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=10   \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                      ANN_capas=[10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=10   \n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  ANN_capas=[10; 10; 1]_activaciones=['relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=10   \n",
       "5                                                                                                                                                                                                                                                                                                                                                                          ANN_capas=[10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=10   \n",
       "2  ANN_capas=[10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=10   \n",
       "\n",
       "      error_test    error_train  segundos_entrenamiento  epoca  \n",
       "3   56579.769105   65503.054219               46.783819   50.0  \n",
       "1   57541.599283   63024.952519               25.163197   20.0  \n",
       "4   59104.701674   62958.017758               24.140647   17.0  \n",
       "0   59305.700232   70519.912422               45.313531   50.0  \n",
       "5   59431.114275   61390.786540               22.732815   14.0  \n",
       "2  120431.773615  115869.879020               47.140597   18.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Experimentos para número de capas\n",
    "experimentoNumeroCapas = [6, 16, 21]\n",
    "for experimento in experimentoNumeroCapas:\n",
    "    listaCapas = []\n",
    "    listaActivaciones = []\n",
    "    for iCapa in range(experimento - 1):\n",
    "        listaCapas.append(10)\n",
    "        listaActivaciones.append('relu')\n",
    "    listaCapas.append(1)\n",
    "    listaActivaciones.append('linear')\n",
    "    redAnn, configuracion = construirANN(caracteristicasEstandarizadas.shape[1], listaCapas, listaActivaciones)\n",
    "    redAnn.summary()\n",
    "    entrenarANN(redAnn, configuracion, caracteristicasEstandarizadas, etiquetas, 'mean_squared_error', tamanioBatch=32, epocas=50)\n",
    "\n",
    "mostrarResultadosBitacora('ANN')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defined-composite",
   "metadata": {},
   "source": [
    "De los datos anteriores se observa que un número adecuado de capas ocultas para este problema podría ser de 5 a 10. Se observa que entre mayor sea el número de capas más rápido se reducirá el error en los datos de entrenamiento pero no impactará tanto en el error en los datos de validación. Se decide que el número de capas ocultas que se utilizará será de __8 capas ocultas__.\n",
    "\n",
    "## Número de neuronas por capa\n",
    "* Después de determinar un número adecuado de capas ocultas (8), se realizarán experimentos para determinar un número adecuado de neuronas por capa: se realizarán experimentos con: 4 neuronas por capa, 8 , 16, 32, 64 y 128 neuronas por capa. Se observará que modelo brinda los mejores resultados sin aumentar demasiado el número de parámetros en el modelo y el tiempo de entrenamiento.\n",
    "* Hiper parámetros fijos que se utilizarán para estas pruebas: tamaño de batch: 32, épocas: 50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "realistic-indianapolis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_144 (Dense)           (None, 4)                 56        \n",
      "                                                                 \n",
      " dense_145 (Dense)           (None, 4)                 20        \n",
      "                                                                 \n",
      " dense_146 (Dense)           (None, 4)                 20        \n",
      "                                                                 \n",
      " dense_147 (Dense)           (None, 4)                 20        \n",
      "                                                                 \n",
      " dense_148 (Dense)           (None, 4)                 20        \n",
      "                                                                 \n",
      " dense_149 (Dense)           (None, 4)                 20        \n",
      "                                                                 \n",
      " dense_150 (Dense)           (None, 4)                 20        \n",
      "                                                                 \n",
      " dense_151 (Dense)           (None, 4)                 20        \n",
      "                                                                 \n",
      " dense_152 (Dense)           (None, 1)                 5         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 201\n",
      "Trainable params: 201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "549/549 [==============================] - 2s 2ms/step - loss: 56042758144.0000 - val_loss: 56455499776.0000\n",
      "Epoch 2/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 56042565632.0000 - val_loss: 56455249920.0000\n",
      "Epoch 3/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 56042409984.0000 - val_loss: 56455057408.0000\n",
      "Epoch 4/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 56042160128.0000 - val_loss: 56454782976.0000\n",
      "Epoch 5/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 56041869312.0000 - val_loss: 56454578176.0000\n",
      "Epoch 6/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 56041627648.0000 - val_loss: 56454356992.0000\n",
      "Epoch 7/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 56041426944.0000 - val_loss: 56454119424.0000\n",
      "Epoch 8/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 56041193472.0000 - val_loss: 56453918720.0000\n",
      "Epoch 9/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 56040972288.0000 - val_loss: 56453672960.0000\n",
      "Epoch 10/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 56040771584.0000 - val_loss: 56453464064.0000\n",
      "Epoch 11/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 56040521728.0000 - val_loss: 56453218304.0000\n",
      "Epoch 12/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 56040296448.0000 - val_loss: 56453001216.0000\n",
      "Epoch 13/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 56040062976.0000 - val_loss: 56452759552.0000\n",
      "Epoch 14/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 56039886848.0000 - val_loss: 56452550656.0000\n",
      "Epoch 15/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 56039632896.0000 - val_loss: 56452313088.0000\n",
      "Epoch 16/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 56039399424.0000 - val_loss: 56452108288.0000\n",
      "Epoch 17/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 56039219200.0000 - val_loss: 56451870720.0000\n",
      "Epoch 18/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 56038969344.0000 - val_loss: 56451620864.0000\n",
      "Epoch 19/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 56038731776.0000 - val_loss: 56451399680.0000\n",
      "Epoch 20/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 56038473728.0000 - val_loss: 56451170304.0000\n",
      "Epoch 21/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 56038268928.0000 - val_loss: 56450973696.0000\n",
      "Epoch 22/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 56038072320.0000 - val_loss: 56450723840.0000\n",
      "Epoch 23/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 56037793792.0000 - val_loss: 56450502656.0000\n",
      "Epoch 24/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 56037601280.0000 - val_loss: 56450265088.0000\n",
      "Epoch 25/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 56037363712.0000 - val_loss: 56450048000.0000\n",
      "Epoch 26/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 56037158912.0000 - val_loss: 56449814528.0000\n",
      "Epoch 27/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 56036913152.0000 - val_loss: 56449605632.0000\n",
      "Epoch 28/50\n",
      "549/549 [==============================] - 1s 1ms/step - loss: 56036720640.0000 - val_loss: 56449359872.0000\n",
      "Epoch 29/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 56036483072.0000 - val_loss: 56449155072.0000\n",
      "Epoch 30/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 56036249600.0000 - val_loss: 56448921600.0000\n",
      "Epoch 31/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 56036036608.0000 - val_loss: 56448700416.0000\n",
      "Epoch 32/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 56035815424.0000 - val_loss: 56448466944.0000\n",
      "Epoch 33/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 56035549184.0000 - val_loss: 56448229376.0000\n",
      "Epoch 34/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 56035344384.0000 - val_loss: 56448012288.0000\n",
      "Epoch 35/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 56035119104.0000 - val_loss: 56447778816.0000\n",
      "Epoch 36/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 56034926592.0000 - val_loss: 56447553536.0000\n",
      "Epoch 37/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 56034680832.0000 - val_loss: 56447332352.0000\n",
      "Epoch 38/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 56034435072.0000 - val_loss: 56447094784.0000\n",
      "Epoch 39/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 56034209792.0000 - val_loss: 56446881792.0000\n",
      "Epoch 40/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 56033968128.0000 - val_loss: 56446656512.0000\n",
      "Epoch 41/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 56033763328.0000 - val_loss: 56446423040.0000\n",
      "Epoch 42/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 56033525760.0000 - val_loss: 56446218240.0000\n",
      "Epoch 43/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 56033320960.0000 - val_loss: 56445952000.0000\n",
      "Epoch 44/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 56033112064.0000 - val_loss: 56445747200.0000\n",
      "Epoch 45/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 56032862208.0000 - val_loss: 56445497344.0000\n",
      "Epoch 46/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 56032632832.0000 - val_loss: 56445300736.0000\n",
      "Epoch 47/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 56032395264.0000 - val_loss: 56445059072.0000\n",
      "Epoch 48/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 56032169984.0000 - val_loss: 56444858368.0000\n",
      "Epoch 49/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 56031981568.0000 - val_loss: 56444596224.0000\n",
      "Epoch 50/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 56031678464.0000 - val_loss: 56444370944.0000\n",
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_153 (Dense)           (None, 8)                 112       \n",
      "                                                                 \n",
      " dense_154 (Dense)           (None, 8)                 72        \n",
      "                                                                 \n",
      " dense_155 (Dense)           (None, 8)                 72        \n",
      "                                                                 \n",
      " dense_156 (Dense)           (None, 8)                 72        \n",
      "                                                                 \n",
      " dense_157 (Dense)           (None, 8)                 72        \n",
      "                                                                 \n",
      " dense_158 (Dense)           (None, 8)                 72        \n",
      "                                                                 \n",
      " dense_159 (Dense)           (None, 8)                 72        \n",
      "                                                                 \n",
      " dense_160 (Dense)           (None, 8)                 72        \n",
      "                                                                 \n",
      " dense_161 (Dense)           (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 625\n",
      "Trainable params: 625\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "549/549 [==============================] - 2s 2ms/step - loss: 39612690432.0000 - val_loss: 7801553920.0000\n",
      "Epoch 2/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 6675529728.0000 - val_loss: 4425195008.0000\n",
      "Epoch 3/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 5454579200.0000 - val_loss: 3742568960.0000\n",
      "Epoch 4/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 5058916864.0000 - val_loss: 3656160512.0000\n",
      "Epoch 5/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4906948608.0000 - val_loss: 3530122496.0000\n",
      "Epoch 6/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4839406080.0000 - val_loss: 3478681600.0000\n",
      "Epoch 7/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4795753984.0000 - val_loss: 3487346944.0000\n",
      "Epoch 8/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4764287488.0000 - val_loss: 3419958272.0000\n",
      "Epoch 9/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4730824192.0000 - val_loss: 3415884032.0000\n",
      "Epoch 10/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4696038912.0000 - val_loss: 3372812800.0000\n",
      "Epoch 11/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4671981568.0000 - val_loss: 3358086400.0000\n",
      "Epoch 12/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4648229888.0000 - val_loss: 3360166656.0000\n",
      "Epoch 13/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4619939328.0000 - val_loss: 3334076672.0000\n",
      "Epoch 14/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4600059392.0000 - val_loss: 3344979456.0000\n",
      "Epoch 15/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4584799232.0000 - val_loss: 3340727552.0000\n",
      "Epoch 16/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4557460480.0000 - val_loss: 3304061184.0000\n",
      "Epoch 17/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4537777152.0000 - val_loss: 3295948032.0000\n",
      "Epoch 18/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4517513216.0000 - val_loss: 3293925376.0000\n",
      "Epoch 19/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4506211840.0000 - val_loss: 3269117440.0000\n",
      "Epoch 20/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4483548160.0000 - val_loss: 3269689344.0000\n",
      "Epoch 21/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4481189376.0000 - val_loss: 3253578240.0000\n",
      "Epoch 22/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4457665536.0000 - val_loss: 3266695168.0000\n",
      "Epoch 23/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4451306496.0000 - val_loss: 3268707328.0000\n",
      "Epoch 24/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4442026496.0000 - val_loss: 3242062592.0000\n",
      "Epoch 25/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4428067840.0000 - val_loss: 3226589184.0000\n",
      "Epoch 26/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4414135296.0000 - val_loss: 3230350336.0000\n",
      "Epoch 27/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4409398272.0000 - val_loss: 3288503040.0000\n",
      "Epoch 28/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4399259136.0000 - val_loss: 3233790720.0000\n",
      "Epoch 29/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4394119168.0000 - val_loss: 3231480832.0000\n",
      "Epoch 30/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4382822400.0000 - val_loss: 3209844480.0000\n",
      "Epoch 31/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4380759552.0000 - val_loss: 3199995136.0000\n",
      "Epoch 32/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4373288960.0000 - val_loss: 3202864896.0000\n",
      "Epoch 33/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4358677504.0000 - val_loss: 3206036480.0000\n",
      "Epoch 34/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4357218816.0000 - val_loss: 3229504256.0000\n",
      "Epoch 35/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4347536896.0000 - val_loss: 3185088768.0000\n",
      "Epoch 36/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4345989632.0000 - val_loss: 3182427904.0000\n",
      "Epoch 37/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4339505152.0000 - val_loss: 3243105792.0000\n",
      "Epoch 38/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4332794368.0000 - val_loss: 3250328576.0000\n",
      "Epoch 39/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4328637952.0000 - val_loss: 3180302336.0000\n",
      "Epoch 40/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4330706944.0000 - val_loss: 3174369792.0000\n",
      "Epoch 41/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4316313600.0000 - val_loss: 3181927936.0000\n",
      "Epoch 42/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4320088576.0000 - val_loss: 3163049472.0000\n",
      "Epoch 43/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4308539392.0000 - val_loss: 3162198784.0000\n",
      "Epoch 44/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4310870528.0000 - val_loss: 3281296640.0000\n",
      "Epoch 45/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4304769536.0000 - val_loss: 3216523776.0000\n",
      "Epoch 46/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4301348352.0000 - val_loss: 3181032704.0000\n",
      "Epoch 47/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4300728832.0000 - val_loss: 3185444608.0000\n",
      "Epoch 48/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4299437568.0000 - val_loss: 3180463872.0000\n",
      "Epoch 49/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4291765248.0000 - val_loss: 3162329856.0000\n",
      "Epoch 50/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4290965760.0000 - val_loss: 3154852864.0000\n",
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_162 (Dense)           (None, 16)                224       \n",
      "                                                                 \n",
      " dense_163 (Dense)           (None, 16)                272       \n",
      "                                                                 \n",
      " dense_164 (Dense)           (None, 16)                272       \n",
      "                                                                 \n",
      " dense_165 (Dense)           (None, 16)                272       \n",
      "                                                                 \n",
      " dense_166 (Dense)           (None, 16)                272       \n",
      "                                                                 \n",
      " dense_167 (Dense)           (None, 16)                272       \n",
      "                                                                 \n",
      " dense_168 (Dense)           (None, 16)                272       \n",
      "                                                                 \n",
      " dense_169 (Dense)           (None, 16)                272       \n",
      "                                                                 \n",
      " dense_170 (Dense)           (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,145\n",
      "Trainable params: 2,145\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "549/549 [==============================] - 2s 2ms/step - loss: 26875899904.0000 - val_loss: 5141181952.0000\n",
      "Epoch 2/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 5841213440.0000 - val_loss: 3982724608.0000\n",
      "Epoch 3/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 5041328128.0000 - val_loss: 3479810304.0000\n",
      "Epoch 4/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4810446848.0000 - val_loss: 3449195264.0000\n",
      "Epoch 5/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4707885568.0000 - val_loss: 3419770880.0000\n",
      "Epoch 6/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4626913792.0000 - val_loss: 3396954880.0000\n",
      "Epoch 7/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4566791168.0000 - val_loss: 3416210944.0000\n",
      "Epoch 8/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4508594688.0000 - val_loss: 3316370432.0000\n",
      "Epoch 9/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4450602496.0000 - val_loss: 3315778560.0000\n",
      "Epoch 10/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4393016832.0000 - val_loss: 3302317824.0000\n",
      "Epoch 11/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4340517376.0000 - val_loss: 3313081088.0000\n",
      "Epoch 12/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4288067072.0000 - val_loss: 3317348096.0000\n",
      "Epoch 13/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4234793984.0000 - val_loss: 3297084416.0000\n",
      "Epoch 14/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4185274624.0000 - val_loss: 3265480960.0000\n",
      "Epoch 15/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4160465152.0000 - val_loss: 3328824064.0000\n",
      "Epoch 16/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4117654272.0000 - val_loss: 3269487104.0000\n",
      "Epoch 17/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4075462144.0000 - val_loss: 3279975424.0000\n",
      "Epoch 18/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4046953472.0000 - val_loss: 3205588992.0000\n",
      "Epoch 19/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4028045056.0000 - val_loss: 3246628352.0000\n",
      "Epoch 20/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3991473408.0000 - val_loss: 3183381248.0000\n",
      "Epoch 21/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3977503488.0000 - val_loss: 3217093376.0000\n",
      "Epoch 22/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3941404160.0000 - val_loss: 3203981312.0000\n",
      "Epoch 23/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3918422784.0000 - val_loss: 3282459904.0000\n",
      "Epoch 24/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3901683712.0000 - val_loss: 3196720128.0000\n",
      "Epoch 25/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3887327744.0000 - val_loss: 3182959104.0000\n",
      "Epoch 26/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3850983680.0000 - val_loss: 3280714496.0000\n",
      "Epoch 27/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3849476096.0000 - val_loss: 3251817472.0000\n",
      "Epoch 28/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3821898752.0000 - val_loss: 3201951232.0000\n",
      "Epoch 29/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3809279232.0000 - val_loss: 3166768896.0000\n",
      "Epoch 30/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3788013824.0000 - val_loss: 3218478336.0000\n",
      "Epoch 31/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3769314816.0000 - val_loss: 3261477120.0000\n",
      "Epoch 32/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3763343104.0000 - val_loss: 3226172928.0000\n",
      "Epoch 33/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3730208768.0000 - val_loss: 3250750464.0000\n",
      "Epoch 34/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3728017920.0000 - val_loss: 3250260480.0000\n",
      "Epoch 35/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3700910848.0000 - val_loss: 3228539136.0000\n",
      "Epoch 36/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3701473792.0000 - val_loss: 3291904768.0000\n",
      "Epoch 37/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3671508992.0000 - val_loss: 3368088832.0000\n",
      "Epoch 38/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3658085376.0000 - val_loss: 3327229184.0000\n",
      "Epoch 39/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3639806976.0000 - val_loss: 3311748096.0000\n",
      "Epoch 39: early stopping\n",
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_171 (Dense)           (None, 32)                448       \n",
      "                                                                 \n",
      " dense_172 (Dense)           (None, 32)                1056      \n",
      "                                                                 \n",
      " dense_173 (Dense)           (None, 32)                1056      \n",
      "                                                                 \n",
      " dense_174 (Dense)           (None, 32)                1056      \n",
      "                                                                 \n",
      " dense_175 (Dense)           (None, 32)                1056      \n",
      "                                                                 \n",
      " dense_176 (Dense)           (None, 32)                1056      \n",
      "                                                                 \n",
      " dense_177 (Dense)           (None, 32)                1056      \n",
      "                                                                 \n",
      " dense_178 (Dense)           (None, 32)                1056      \n",
      "                                                                 \n",
      " dense_179 (Dense)           (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7,873\n",
      "Trainable params: 7,873\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "549/549 [==============================] - 2s 2ms/step - loss: 17656807424.0000 - val_loss: 3780540160.0000\n",
      "Epoch 2/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4878801920.0000 - val_loss: 3468644352.0000\n",
      "Epoch 3/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4509742592.0000 - val_loss: 3347891712.0000\n",
      "Epoch 4/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4331911168.0000 - val_loss: 3353496832.0000\n",
      "Epoch 5/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4210042112.0000 - val_loss: 3309776640.0000\n",
      "Epoch 6/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4076973056.0000 - val_loss: 3308523520.0000\n",
      "Epoch 7/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3986354944.0000 - val_loss: 3382655488.0000\n",
      "Epoch 8/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3875872256.0000 - val_loss: 3388201984.0000\n",
      "Epoch 9/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3807739904.0000 - val_loss: 3395391744.0000\n",
      "Epoch 10/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3740308480.0000 - val_loss: 3822409728.0000\n",
      "Epoch 11/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3688219904.0000 - val_loss: 3610011904.0000\n",
      "Epoch 12/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3614631936.0000 - val_loss: 3809154816.0000\n",
      "Epoch 13/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3568378624.0000 - val_loss: 3499084288.0000\n",
      "Epoch 14/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3530524672.0000 - val_loss: 3471827968.0000\n",
      "Epoch 15/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3497516800.0000 - val_loss: 3599022336.0000\n",
      "Epoch 16/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3456282880.0000 - val_loss: 3618041088.0000\n",
      "Epoch 16: early stopping\n",
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_180 (Dense)           (None, 64)                896       \n",
      "                                                                 \n",
      " dense_181 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_182 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_183 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_184 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_185 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_186 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_187 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_188 (Dense)           (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 30,081\n",
      "Trainable params: 30,081\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "549/549 [==============================] - 3s 3ms/step - loss: 13466230784.0000 - val_loss: 3674004480.0000\n",
      "Epoch 2/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4636524544.0000 - val_loss: 3320839424.0000\n",
      "Epoch 3/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4275316992.0000 - val_loss: 3223717632.0000\n",
      "Epoch 4/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4006455040.0000 - val_loss: 3423435264.0000\n",
      "Epoch 5/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3815858944.0000 - val_loss: 3323369472.0000\n",
      "Epoch 6/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3674056192.0000 - val_loss: 3774620160.0000\n",
      "Epoch 7/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3610201600.0000 - val_loss: 3627929088.0000\n",
      "Epoch 8/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3520864256.0000 - val_loss: 3340008192.0000\n",
      "Epoch 9/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3464417280.0000 - val_loss: 3513364992.0000\n",
      "Epoch 10/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3453154304.0000 - val_loss: 3615910144.0000\n",
      "Epoch 11/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3404279552.0000 - val_loss: 3314003200.0000\n",
      "Epoch 12/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3358004992.0000 - val_loss: 3367447296.0000\n",
      "Epoch 13/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3316681728.0000 - val_loss: 3256163072.0000\n",
      "Epoch 13: early stopping\n",
      "Model: \"sequential_15\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_189 (Dense)           (None, 128)               1792      \n",
      "                                                                 \n",
      " dense_190 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_191 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_192 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_193 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_194 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_195 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_196 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_197 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 117,505\n",
      "Trainable params: 117,505\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "549/549 [==============================] - 3s 3ms/step - loss: 10695294976.0000 - val_loss: 3498667008.0000\n",
      "Epoch 2/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 4496153600.0000 - val_loss: 3741141760.0000\n",
      "Epoch 3/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 4117288448.0000 - val_loss: 3352175872.0000\n",
      "Epoch 4/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 3825767168.0000 - val_loss: 3939502848.0000\n",
      "Epoch 5/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 3658061056.0000 - val_loss: 3355994880.0000\n",
      "Epoch 6/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 3530229504.0000 - val_loss: 4103645440.0000\n",
      "Epoch 7/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 3460742144.0000 - val_loss: 3886993664.0000\n",
      "Epoch 8/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 3411697152.0000 - val_loss: 3296963328.0000\n",
      "Epoch 9/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 3349294592.0000 - val_loss: 3426554112.0000\n",
      "Epoch 10/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 3308016896.0000 - val_loss: 3341678592.0000\n",
      "Epoch 11/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 3241617920.0000 - val_loss: 3612321024.0000\n",
      "Epoch 12/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 3210162944.0000 - val_loss: 3387741184.0000\n",
      "Epoch 13/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 3151380992.0000 - val_loss: 3328826880.0000\n",
      "Epoch 14/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 3136945152.0000 - val_loss: 3344716288.0000\n",
      "Epoch 15/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 3075708416.0000 - val_loss: 3243800320.0000\n",
      "Epoch 16/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 3053760000.0000 - val_loss: 3677014016.0000\n",
      "Epoch 17/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 3039687680.0000 - val_loss: 3433235712.0000\n",
      "Epoch 18/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 2965028608.0000 - val_loss: 3373951488.0000\n",
      "Epoch 19/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 2983046400.0000 - val_loss: 3457824256.0000\n",
      "Epoch 20/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 2937512960.0000 - val_loss: 3202939136.0000\n",
      "Epoch 21/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 2964937216.0000 - val_loss: 3122019840.0000\n",
      "Epoch 22/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 2882596352.0000 - val_loss: 3238280704.0000\n",
      "Epoch 23/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 2828760832.0000 - val_loss: 3327670272.0000\n",
      "Epoch 24/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 2873596672.0000 - val_loss: 3466420992.0000\n",
      "Epoch 25/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 2816280320.0000 - val_loss: 3094044416.0000\n",
      "Epoch 26/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 2810995200.0000 - val_loss: 3358134016.0000\n",
      "Epoch 27/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 2777357824.0000 - val_loss: 3847250432.0000\n",
      "Epoch 28/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 2788540160.0000 - val_loss: 3656093696.0000\n",
      "Epoch 29/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 2725755904.0000 - val_loss: 3800841216.0000\n",
      "Epoch 30/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 2747350528.0000 - val_loss: 3246147328.0000\n",
      "Epoch 31/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 2691691264.0000 - val_loss: 3548620032.0000\n",
      "Epoch 32/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 2643769088.0000 - val_loss: 3051118336.0000\n",
      "Epoch 33/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 2648049152.0000 - val_loss: 3287923968.0000\n",
      "Epoch 34/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 2628929280.0000 - val_loss: 3499586048.0000\n",
      "Epoch 35/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 2611470336.0000 - val_loss: 3315650816.0000\n",
      "Epoch 36/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 2648683008.0000 - val_loss: 3245439232.0000\n",
      "Epoch 37/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 2583341568.0000 - val_loss: 4215323392.0000\n",
      "Epoch 38/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 2595076352.0000 - val_loss: 3289260544.0000\n",
      "Epoch 39/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 2563378944.0000 - val_loss: 3722235392.0000\n",
      "Epoch 40/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 2595223296.0000 - val_loss: 3550194432.0000\n",
      "Epoch 41/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 2521016576.0000 - val_loss: 3653093888.0000\n",
      "Epoch 42/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 2520552704.0000 - val_loss: 4529293824.0000\n",
      "Epoch 42: early stopping\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>configuracion</th>\n",
       "      <th>error_test</th>\n",
       "      <th>error_train</th>\n",
       "      <th>segundos_entrenamiento</th>\n",
       "      <th>epoca</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ANN_capas=[8; 8; 8; 8; 8; 8; 8; 8; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=10</td>\n",
       "      <td>56168.076912</td>\n",
       "      <td>65505.463589</td>\n",
       "      <td>52.175863</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ANN_capas=[10; 10; 10; 10; 10; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=10</td>\n",
       "      <td>56579.769105</td>\n",
       "      <td>65503.054219</td>\n",
       "      <td>46.783819</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ANN_capas=[64; 64; 64; 64; 64; 64; 64; 64; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=10</td>\n",
       "      <td>57062.799371</td>\n",
       "      <td>57590.639239</td>\n",
       "      <td>17.061472</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ANN_capas=[10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=10</td>\n",
       "      <td>57541.599283</td>\n",
       "      <td>63024.952519</td>\n",
       "      <td>25.163197</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ANN_capas=[16; 16; 16; 16; 16; 16; 16; 16; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=10</td>\n",
       "      <td>57547.789671</td>\n",
       "      <td>60330.812824</td>\n",
       "      <td>41.198634</td>\n",
       "      <td>39.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ANN_capas=[10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=10</td>\n",
       "      <td>59104.701674</td>\n",
       "      <td>62958.017758</td>\n",
       "      <td>24.140647</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ANN_capas=[10; 10; 1]_activaciones=['relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=10</td>\n",
       "      <td>59305.700232</td>\n",
       "      <td>70519.912422</td>\n",
       "      <td>45.313531</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ANN_capas=[10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=10</td>\n",
       "      <td>59431.114275</td>\n",
       "      <td>61390.786540</td>\n",
       "      <td>22.732815</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ANN_capas=[32; 32; 32; 32; 32; 32; 32; 32; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=10</td>\n",
       "      <td>60150.154514</td>\n",
       "      <td>58790.159721</td>\n",
       "      <td>18.325325</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ANN_capas=[128; 128; 128; 128; 128; 128; 128; 128; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=10</td>\n",
       "      <td>67300.028410</td>\n",
       "      <td>50205.106354</td>\n",
       "      <td>68.336977</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ANN_capas=[10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=10</td>\n",
       "      <td>120431.773615</td>\n",
       "      <td>115869.879020</td>\n",
       "      <td>47.140597</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ANN_capas=[4; 4; 4; 4; 4; 4; 4; 4; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=10</td>\n",
       "      <td>237580.241064</td>\n",
       "      <td>236710.114832</td>\n",
       "      <td>49.197681</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           configuracion  \\\n",
       "7                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   ANN_capas=[8; 8; 8; 8; 8; 8; 8; 8; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=10   \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               ANN_capas=[10; 10; 10; 10; 10; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=10   \n",
       "10                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          ANN_capas=[64; 64; 64; 64; 64; 64; 64; 64; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=10   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   ANN_capas=[10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=10   \n",
       "8                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           ANN_capas=[16; 16; 16; 16; 16; 16; 16; 16; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=10   \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                       ANN_capas=[10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=10   \n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   ANN_capas=[10; 10; 1]_activaciones=['relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=10   \n",
       "5                                                                                                                                                                                                                                                                                                                                                                           ANN_capas=[10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=10   \n",
       "9                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           ANN_capas=[32; 32; 32; 32; 32; 32; 32; 32; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=10   \n",
       "11                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  ANN_capas=[128; 128; 128; 128; 128; 128; 128; 128; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=10   \n",
       "2   ANN_capas=[10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=10   \n",
       "6                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   ANN_capas=[4; 4; 4; 4; 4; 4; 4; 4; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=10   \n",
       "\n",
       "       error_test    error_train  segundos_entrenamiento  epoca  \n",
       "7    56168.076912   65505.463589               52.175863   50.0  \n",
       "3    56579.769105   65503.054219               46.783819   50.0  \n",
       "10   57062.799371   57590.639239               17.061472   13.0  \n",
       "1    57541.599283   63024.952519               25.163197   20.0  \n",
       "8    57547.789671   60330.812824               41.198634   39.0  \n",
       "4    59104.701674   62958.017758               24.140647   17.0  \n",
       "0    59305.700232   70519.912422               45.313531   50.0  \n",
       "5    59431.114275   61390.786540               22.732815   14.0  \n",
       "9    60150.154514   58790.159721               18.325325   16.0  \n",
       "11   67300.028410   50205.106354               68.336977   42.0  \n",
       "2   120431.773615  115869.879020               47.140597   18.0  \n",
       "6   237580.241064  236710.114832               49.197681   50.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Experimentos para número de capas\n",
    "experimentoNumeroNeuronas = [4, 8, 16, 32, 64, 128]\n",
    "for experimento in experimentoNumeroNeuronas:\n",
    "    listaCapas = []\n",
    "    listaActivaciones = []\n",
    "    for iCapa in range(8):\n",
    "        listaCapas.append(experimento)\n",
    "        listaActivaciones.append('relu')\n",
    "    listaCapas.append(1)\n",
    "    listaActivaciones.append('linear')\n",
    "    redAnn, configuracion = construirANN(caracteristicasEstandarizadas.shape[1], listaCapas, listaActivaciones)\n",
    "    redAnn.summary()\n",
    "    entrenarANN(redAnn, configuracion, caracteristicasEstandarizadas, etiquetas, 'mean_squared_error', tamanioBatch=32, epocas=50)\n",
    "\n",
    "mostrarResultadosBitacora('ANN')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reduced-label",
   "metadata": {},
   "source": [
    "### Comentarios a número de neuronas por capa y más pruebas\n",
    "* Aunque el mejor error se presentó con un número de 8 neuronas por capa, se puede notar que el error de entrenamiento no es el mejor y tomó un tiempo mayor (52 segundos) que otros modelos con más neuronas.\n",
    "* Un modelo con 64 neuronas por capa redujo el error en los datos de entrenamiento bastante rápido (17 segundos) pero sufrió de un detenimiento temprano porque no mostró mejoría en el error de los datos de validación después de 10 iteraciones.\n",
    "* Los modelos de 16 y 32 neuronas por capa siguieron un patrón similar al modelo de 64 neuronas por capa. Se realizarán más pruebas sobre estos 3 modelos para sin detenimiento temprano para confirmar que el error en los datos de validación empeorará para los 3 modelos.\n",
    "* No se tomará en cuenta el modelo con 128 neuronas por capa pues presentó peor métrica en el error de los datos de validación e incrementa bastante el número de parámetros al modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "basic-hunter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_198 (Dense)           (None, 16)                224       \n",
      "                                                                 \n",
      " dense_199 (Dense)           (None, 16)                272       \n",
      "                                                                 \n",
      " dense_200 (Dense)           (None, 16)                272       \n",
      "                                                                 \n",
      " dense_201 (Dense)           (None, 16)                272       \n",
      "                                                                 \n",
      " dense_202 (Dense)           (None, 16)                272       \n",
      "                                                                 \n",
      " dense_203 (Dense)           (None, 16)                272       \n",
      "                                                                 \n",
      " dense_204 (Dense)           (None, 16)                272       \n",
      "                                                                 \n",
      " dense_205 (Dense)           (None, 16)                272       \n",
      "                                                                 \n",
      " dense_206 (Dense)           (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,145\n",
      "Trainable params: 2,145\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "549/549 [==============================] - 2s 2ms/step - loss: 26875899904.0000 - val_loss: 5141181952.0000\n",
      "Epoch 2/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 5841213440.0000 - val_loss: 3982724608.0000\n",
      "Epoch 3/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 5041328128.0000 - val_loss: 3479810304.0000\n",
      "Epoch 4/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4810446848.0000 - val_loss: 3449195264.0000\n",
      "Epoch 5/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4707885568.0000 - val_loss: 3419770880.0000\n",
      "Epoch 6/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4626913792.0000 - val_loss: 3396954880.0000\n",
      "Epoch 7/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4566791168.0000 - val_loss: 3416210944.0000\n",
      "Epoch 8/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4508594688.0000 - val_loss: 3316370432.0000\n",
      "Epoch 9/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4450602496.0000 - val_loss: 3315778560.0000\n",
      "Epoch 10/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4393016832.0000 - val_loss: 3302317824.0000\n",
      "Epoch 11/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4340517376.0000 - val_loss: 3313081088.0000\n",
      "Epoch 12/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4288067072.0000 - val_loss: 3317348096.0000\n",
      "Epoch 13/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4234793984.0000 - val_loss: 3297084416.0000\n",
      "Epoch 14/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4185274624.0000 - val_loss: 3265480960.0000\n",
      "Epoch 15/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4160465152.0000 - val_loss: 3328824064.0000\n",
      "Epoch 16/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4117654272.0000 - val_loss: 3269487104.0000\n",
      "Epoch 17/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4075462144.0000 - val_loss: 3279975424.0000\n",
      "Epoch 18/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4046953472.0000 - val_loss: 3205588992.0000\n",
      "Epoch 19/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4028045056.0000 - val_loss: 3246628352.0000\n",
      "Epoch 20/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3991473408.0000 - val_loss: 3183381248.0000\n",
      "Epoch 21/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3977503488.0000 - val_loss: 3217093376.0000\n",
      "Epoch 22/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3941404160.0000 - val_loss: 3203981312.0000\n",
      "Epoch 23/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3918422784.0000 - val_loss: 3282459904.0000\n",
      "Epoch 24/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3901683712.0000 - val_loss: 3196720128.0000\n",
      "Epoch 25/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3887327744.0000 - val_loss: 3182959104.0000\n",
      "Epoch 26/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3850983680.0000 - val_loss: 3280714496.0000\n",
      "Epoch 27/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3849476096.0000 - val_loss: 3251817472.0000\n",
      "Epoch 28/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3821898752.0000 - val_loss: 3201951232.0000\n",
      "Epoch 29/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3809279232.0000 - val_loss: 3166768896.0000\n",
      "Epoch 30/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3788013824.0000 - val_loss: 3218478336.0000\n",
      "Epoch 31/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3769314816.0000 - val_loss: 3261477120.0000\n",
      "Epoch 32/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3763343104.0000 - val_loss: 3226172928.0000\n",
      "Epoch 33/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3730208768.0000 - val_loss: 3250750464.0000\n",
      "Epoch 34/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3728017920.0000 - val_loss: 3250260480.0000\n",
      "Epoch 35/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3700910848.0000 - val_loss: 3228539136.0000\n",
      "Epoch 36/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3701473792.0000 - val_loss: 3291904768.0000\n",
      "Epoch 37/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3671508992.0000 - val_loss: 3368088832.0000\n",
      "Epoch 38/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3658085376.0000 - val_loss: 3327229184.0000\n",
      "Epoch 39/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3639806976.0000 - val_loss: 3311748096.0000\n",
      "Epoch 40/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3633302528.0000 - val_loss: 3356454912.0000\n",
      "Epoch 41/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3601811712.0000 - val_loss: 3366100224.0000\n",
      "Epoch 42/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3582265856.0000 - val_loss: 3403424256.0000\n",
      "Epoch 43/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3560363264.0000 - val_loss: 3379409152.0000\n",
      "Epoch 44/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3563725312.0000 - val_loss: 3366673664.0000\n",
      "Epoch 45/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3527253760.0000 - val_loss: 3399213568.0000\n",
      "Epoch 46/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3521502208.0000 - val_loss: 3424713216.0000\n",
      "Epoch 47/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3508763904.0000 - val_loss: 3383261696.0000\n",
      "Epoch 48/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3496144384.0000 - val_loss: 3398925568.0000\n",
      "Epoch 49/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3482188544.0000 - val_loss: 3453717760.0000\n",
      "Epoch 50/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3472405760.0000 - val_loss: 3485906176.0000\n",
      "Model: \"sequential_17\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_207 (Dense)           (None, 32)                448       \n",
      "                                                                 \n",
      " dense_208 (Dense)           (None, 32)                1056      \n",
      "                                                                 \n",
      " dense_209 (Dense)           (None, 32)                1056      \n",
      "                                                                 \n",
      " dense_210 (Dense)           (None, 32)                1056      \n",
      "                                                                 \n",
      " dense_211 (Dense)           (None, 32)                1056      \n",
      "                                                                 \n",
      " dense_212 (Dense)           (None, 32)                1056      \n",
      "                                                                 \n",
      " dense_213 (Dense)           (None, 32)                1056      \n",
      "                                                                 \n",
      " dense_214 (Dense)           (None, 32)                1056      \n",
      "                                                                 \n",
      " dense_215 (Dense)           (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7,873\n",
      "Trainable params: 7,873\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "549/549 [==============================] - 2s 2ms/step - loss: 17656807424.0000 - val_loss: 3780540160.0000\n",
      "Epoch 2/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4878801920.0000 - val_loss: 3468644352.0000\n",
      "Epoch 3/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4509742592.0000 - val_loss: 3347891712.0000\n",
      "Epoch 4/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4331911168.0000 - val_loss: 3353496832.0000\n",
      "Epoch 5/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4210042112.0000 - val_loss: 3309776640.0000\n",
      "Epoch 6/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4076973056.0000 - val_loss: 3308523520.0000\n",
      "Epoch 7/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3986354944.0000 - val_loss: 3382655488.0000\n",
      "Epoch 8/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3875872256.0000 - val_loss: 3388201984.0000\n",
      "Epoch 9/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3807739904.0000 - val_loss: 3395391744.0000\n",
      "Epoch 10/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3740308480.0000 - val_loss: 3822409728.0000\n",
      "Epoch 11/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3688219904.0000 - val_loss: 3610011904.0000\n",
      "Epoch 12/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3614631936.0000 - val_loss: 3809154816.0000\n",
      "Epoch 13/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3568378624.0000 - val_loss: 3499084288.0000\n",
      "Epoch 14/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3530524672.0000 - val_loss: 3471827968.0000\n",
      "Epoch 15/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3497516800.0000 - val_loss: 3599022336.0000\n",
      "Epoch 16/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3456282880.0000 - val_loss: 3618041088.0000\n",
      "Epoch 17/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3428663552.0000 - val_loss: 3517861632.0000\n",
      "Epoch 18/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3389469440.0000 - val_loss: 3593240320.0000\n",
      "Epoch 19/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3400797184.0000 - val_loss: 3739061760.0000\n",
      "Epoch 20/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3373650176.0000 - val_loss: 3659584000.0000\n",
      "Epoch 21/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3366333696.0000 - val_loss: 3519447552.0000\n",
      "Epoch 22/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3353736960.0000 - val_loss: 3496701440.0000\n",
      "Epoch 23/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3308365056.0000 - val_loss: 3546380800.0000\n",
      "Epoch 24/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3294183680.0000 - val_loss: 3662319104.0000\n",
      "Epoch 25/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3289493504.0000 - val_loss: 3645159168.0000\n",
      "Epoch 26/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3259715328.0000 - val_loss: 3956800256.0000\n",
      "Epoch 27/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3259443968.0000 - val_loss: 3918607104.0000\n",
      "Epoch 28/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3240388096.0000 - val_loss: 3588684544.0000\n",
      "Epoch 29/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3221451520.0000 - val_loss: 3575517440.0000\n",
      "Epoch 30/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3211251968.0000 - val_loss: 3512392704.0000\n",
      "Epoch 31/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3203049216.0000 - val_loss: 3504808960.0000\n",
      "Epoch 32/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3165931520.0000 - val_loss: 3619972864.0000\n",
      "Epoch 33/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3179497472.0000 - val_loss: 3835767296.0000\n",
      "Epoch 34/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3151318528.0000 - val_loss: 3605181440.0000\n",
      "Epoch 35/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3128901888.0000 - val_loss: 3399176192.0000\n",
      "Epoch 36/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3136679168.0000 - val_loss: 3374435840.0000\n",
      "Epoch 37/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3107745792.0000 - val_loss: 3675484160.0000\n",
      "Epoch 38/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3097679872.0000 - val_loss: 3584523264.0000\n",
      "Epoch 39/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3085948672.0000 - val_loss: 3610415104.0000\n",
      "Epoch 40/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3077681152.0000 - val_loss: 3445752832.0000\n",
      "Epoch 41/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3059904000.0000 - val_loss: 3419454464.0000\n",
      "Epoch 42/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3048457472.0000 - val_loss: 3738151424.0000\n",
      "Epoch 43/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3033584128.0000 - val_loss: 3613172736.0000\n",
      "Epoch 44/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3062212096.0000 - val_loss: 3426908160.0000\n",
      "Epoch 45/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3039725568.0000 - val_loss: 3406078208.0000\n",
      "Epoch 46/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3024472832.0000 - val_loss: 3533946368.0000\n",
      "Epoch 47/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3016544512.0000 - val_loss: 3448488704.0000\n",
      "Epoch 48/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3012895488.0000 - val_loss: 3695675392.0000\n",
      "Epoch 49/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2982238464.0000 - val_loss: 3796489216.0000\n",
      "Epoch 50/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3001521664.0000 - val_loss: 3357312000.0000\n",
      "Model: \"sequential_18\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_216 (Dense)           (None, 64)                896       \n",
      "                                                                 \n",
      " dense_217 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_218 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_219 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_220 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_221 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_222 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_223 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_224 (Dense)           (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 30,081\n",
      "Trainable params: 30,081\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "549/549 [==============================] - 3s 3ms/step - loss: 13466230784.0000 - val_loss: 3674004480.0000\n",
      "Epoch 2/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4636524544.0000 - val_loss: 3320839424.0000\n",
      "Epoch 3/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4275316992.0000 - val_loss: 3223717632.0000\n",
      "Epoch 4/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4006455040.0000 - val_loss: 3423435264.0000\n",
      "Epoch 5/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 3815858944.0000 - val_loss: 3323369472.0000\n",
      "Epoch 6/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3674056192.0000 - val_loss: 3774620160.0000\n",
      "Epoch 7/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3610201600.0000 - val_loss: 3627929088.0000\n",
      "Epoch 8/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3520864256.0000 - val_loss: 3340008192.0000\n",
      "Epoch 9/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3464417280.0000 - val_loss: 3513364992.0000\n",
      "Epoch 10/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 3453154304.0000 - val_loss: 3615910144.0000\n",
      "Epoch 11/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 3404279552.0000 - val_loss: 3314003200.0000\n",
      "Epoch 12/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3358004992.0000 - val_loss: 3367447296.0000\n",
      "Epoch 13/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3316681728.0000 - val_loss: 3256163072.0000\n",
      "Epoch 14/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3296029952.0000 - val_loss: 3229398016.0000\n",
      "Epoch 15/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3265763072.0000 - val_loss: 3205946368.0000\n",
      "Epoch 16/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3241182464.0000 - val_loss: 3133758208.0000\n",
      "Epoch 17/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3203079680.0000 - val_loss: 3192221184.0000\n",
      "Epoch 18/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3142255360.0000 - val_loss: 3473808640.0000\n",
      "Epoch 19/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3143247104.0000 - val_loss: 3434003456.0000\n",
      "Epoch 20/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3123024896.0000 - val_loss: 3220562432.0000\n",
      "Epoch 21/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3094937344.0000 - val_loss: 3141468928.0000\n",
      "Epoch 22/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3057360128.0000 - val_loss: 3049475328.0000\n",
      "Epoch 23/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3014991360.0000 - val_loss: 3094492416.0000\n",
      "Epoch 24/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3016388352.0000 - val_loss: 3349959424.0000\n",
      "Epoch 25/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2985040384.0000 - val_loss: 3034777856.0000\n",
      "Epoch 26/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2952465152.0000 - val_loss: 3285866752.0000\n",
      "Epoch 27/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2940406272.0000 - val_loss: 3569823744.0000\n",
      "Epoch 28/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2945169920.0000 - val_loss: 2960786688.0000\n",
      "Epoch 29/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2904704000.0000 - val_loss: 3190733824.0000\n",
      "Epoch 30/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2911163392.0000 - val_loss: 3008723200.0000\n",
      "Epoch 31/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2884058368.0000 - val_loss: 2979547904.0000\n",
      "Epoch 32/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2857717504.0000 - val_loss: 3005164032.0000\n",
      "Epoch 33/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2862246656.0000 - val_loss: 3188425984.0000\n",
      "Epoch 34/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2845900288.0000 - val_loss: 3227755776.0000\n",
      "Epoch 35/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2829608960.0000 - val_loss: 2858974208.0000\n",
      "Epoch 36/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2839750400.0000 - val_loss: 2980634880.0000\n",
      "Epoch 37/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2806204160.0000 - val_loss: 3460045312.0000\n",
      "Epoch 38/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2823948288.0000 - val_loss: 3001301504.0000\n",
      "Epoch 39/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2786333696.0000 - val_loss: 3102268416.0000\n",
      "Epoch 40/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2786310400.0000 - val_loss: 3079550976.0000\n",
      "Epoch 41/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2752143616.0000 - val_loss: 2938276352.0000\n",
      "Epoch 42/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2754473472.0000 - val_loss: 3089401600.0000\n",
      "Epoch 43/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2750727936.0000 - val_loss: 3138818816.0000\n",
      "Epoch 44/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2774377728.0000 - val_loss: 3289478400.0000\n",
      "Epoch 45/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2746329344.0000 - val_loss: 2892930048.0000\n",
      "Epoch 46/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2731797504.0000 - val_loss: 2964855296.0000\n",
      "Epoch 47/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2698282752.0000 - val_loss: 3058185216.0000\n",
      "Epoch 48/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2750285568.0000 - val_loss: 3011571968.0000\n",
      "Epoch 49/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2669492480.0000 - val_loss: 3296058368.0000\n",
      "Epoch 50/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2699718144.0000 - val_loss: 2941043200.0000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>configuracion</th>\n",
       "      <th>error_test</th>\n",
       "      <th>error_train</th>\n",
       "      <th>segundos_entrenamiento</th>\n",
       "      <th>epoca</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ANN_capas=[64; 64; 64; 64; 64; 64; 64; 64; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1</td>\n",
       "      <td>54231.385747</td>\n",
       "      <td>51958.811996</td>\n",
       "      <td>63.929625</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ANN_capas=[8; 8; 8; 8; 8; 8; 8; 8; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=10</td>\n",
       "      <td>56168.076912</td>\n",
       "      <td>65505.463589</td>\n",
       "      <td>52.175863</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ANN_capas=[10; 10; 10; 10; 10; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=10</td>\n",
       "      <td>56579.769105</td>\n",
       "      <td>65503.054219</td>\n",
       "      <td>46.783819</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ANN_capas=[64; 64; 64; 64; 64; 64; 64; 64; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=10</td>\n",
       "      <td>57062.799371</td>\n",
       "      <td>57590.639239</td>\n",
       "      <td>17.061472</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ANN_capas=[10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=10</td>\n",
       "      <td>57541.599283</td>\n",
       "      <td>63024.952519</td>\n",
       "      <td>25.163197</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ANN_capas=[16; 16; 16; 16; 16; 16; 16; 16; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=10</td>\n",
       "      <td>57547.789671</td>\n",
       "      <td>60330.812824</td>\n",
       "      <td>41.198634</td>\n",
       "      <td>39.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ANN_capas=[32; 32; 32; 32; 32; 32; 32; 32; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1</td>\n",
       "      <td>57942.316143</td>\n",
       "      <td>54786.144818</td>\n",
       "      <td>57.596981</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>ANN_capas=[16; 16; 16; 16; 16; 16; 16; 16; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1</td>\n",
       "      <td>59041.563123</td>\n",
       "      <td>58927.122448</td>\n",
       "      <td>52.700294</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ANN_capas=[10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=10</td>\n",
       "      <td>59104.701674</td>\n",
       "      <td>62958.017758</td>\n",
       "      <td>24.140647</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ANN_capas=[10; 10; 1]_activaciones=['relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=10</td>\n",
       "      <td>59305.700232</td>\n",
       "      <td>70519.912422</td>\n",
       "      <td>45.313531</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ANN_capas=[10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=10</td>\n",
       "      <td>59431.114275</td>\n",
       "      <td>61390.786540</td>\n",
       "      <td>22.732815</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ANN_capas=[32; 32; 32; 32; 32; 32; 32; 32; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=10</td>\n",
       "      <td>60150.154514</td>\n",
       "      <td>58790.159721</td>\n",
       "      <td>18.325325</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ANN_capas=[128; 128; 128; 128; 128; 128; 128; 128; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=10</td>\n",
       "      <td>67300.028410</td>\n",
       "      <td>50205.106354</td>\n",
       "      <td>68.336977</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ANN_capas=[10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=10</td>\n",
       "      <td>120431.773615</td>\n",
       "      <td>115869.879020</td>\n",
       "      <td>47.140597</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ANN_capas=[4; 4; 4; 4; 4; 4; 4; 4; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=10</td>\n",
       "      <td>237580.241064</td>\n",
       "      <td>236710.114832</td>\n",
       "      <td>49.197681</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           configuracion  \\\n",
       "14                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          ANN_capas=[64; 64; 64; 64; 64; 64; 64; 64; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1   \n",
       "7                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   ANN_capas=[8; 8; 8; 8; 8; 8; 8; 8; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=10   \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               ANN_capas=[10; 10; 10; 10; 10; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=10   \n",
       "10                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          ANN_capas=[64; 64; 64; 64; 64; 64; 64; 64; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=10   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   ANN_capas=[10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=10   \n",
       "8                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           ANN_capas=[16; 16; 16; 16; 16; 16; 16; 16; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=10   \n",
       "13                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          ANN_capas=[32; 32; 32; 32; 32; 32; 32; 32; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1   \n",
       "12                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          ANN_capas=[16; 16; 16; 16; 16; 16; 16; 16; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1   \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                       ANN_capas=[10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=10   \n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   ANN_capas=[10; 10; 1]_activaciones=['relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=10   \n",
       "5                                                                                                                                                                                                                                                                                                                                                                           ANN_capas=[10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=10   \n",
       "9                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           ANN_capas=[32; 32; 32; 32; 32; 32; 32; 32; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=10   \n",
       "11                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  ANN_capas=[128; 128; 128; 128; 128; 128; 128; 128; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=10   \n",
       "2   ANN_capas=[10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=10   \n",
       "6                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   ANN_capas=[4; 4; 4; 4; 4; 4; 4; 4; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=10   \n",
       "\n",
       "       error_test    error_train  segundos_entrenamiento  epoca  \n",
       "14   54231.385747   51958.811996               63.929625   50.0  \n",
       "7    56168.076912   65505.463589               52.175863   50.0  \n",
       "3    56579.769105   65503.054219               46.783819   50.0  \n",
       "10   57062.799371   57590.639239               17.061472   13.0  \n",
       "1    57541.599283   63024.952519               25.163197   20.0  \n",
       "8    57547.789671   60330.812824               41.198634   39.0  \n",
       "13   57942.316143   54786.144818               57.596981   50.0  \n",
       "12   59041.563123   58927.122448               52.700294   50.0  \n",
       "4    59104.701674   62958.017758               24.140647   17.0  \n",
       "0    59305.700232   70519.912422               45.313531   50.0  \n",
       "5    59431.114275   61390.786540               22.732815   14.0  \n",
       "9    60150.154514   58790.159721               18.325325   16.0  \n",
       "11   67300.028410   50205.106354               68.336977   42.0  \n",
       "2   120431.773615  115869.879020               47.140597   18.0  \n",
       "6   237580.241064  236710.114832               49.197681   50.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Experimentos para número de capas\n",
    "experimentoNumeroNeuronas = [16, 32, 64]\n",
    "for experimento in experimentoNumeroNeuronas:\n",
    "    listaCapas = []\n",
    "    listaActivaciones = []\n",
    "    for iCapa in range(8):\n",
    "        listaCapas.append(experimento)\n",
    "        listaActivaciones.append('relu')\n",
    "    listaCapas.append(1)\n",
    "    listaActivaciones.append('linear')\n",
    "    redAnn, configuracion = construirANN(caracteristicasEstandarizadas.shape[1], listaCapas, listaActivaciones)\n",
    "    redAnn.summary()\n",
    "    entrenarANN(redAnn, configuracion, caracteristicasEstandarizadas, etiquetas, 'mean_squared_error', tamanioBatch=32, \\\n",
    "                epocas=50, paciencia=-1)\n",
    "\n",
    "mostrarResultadosBitacora('ANN')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "egyptian-labor",
   "metadata": {},
   "source": [
    "### Pruebas entre número de neuronas entre 64 y 128 neuronas.\n",
    "* Mientras que con 16 y 32 neuronas por capa, el error en los datos de validación no mejoró a través de las épocas; el modelo con 64 neuronas por capa mostró mejor reducción de error en los datos de entrenamiento y la mejor métrica de error en los datos de validación. Se realizarán más pruebas con los siguientes números de neuronas: 50, 80, 95 y 110."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "sticky-wings",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_25\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_279 (Dense)           (None, 50)                700       \n",
      "                                                                 \n",
      " dense_280 (Dense)           (None, 50)                2550      \n",
      "                                                                 \n",
      " dense_281 (Dense)           (None, 50)                2550      \n",
      "                                                                 \n",
      " dense_282 (Dense)           (None, 50)                2550      \n",
      "                                                                 \n",
      " dense_283 (Dense)           (None, 50)                2550      \n",
      "                                                                 \n",
      " dense_284 (Dense)           (None, 50)                2550      \n",
      "                                                                 \n",
      " dense_285 (Dense)           (None, 50)                2550      \n",
      "                                                                 \n",
      " dense_286 (Dense)           (None, 50)                2550      \n",
      "                                                                 \n",
      " dense_287 (Dense)           (None, 1)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 18,601\n",
      "Trainable params: 18,601\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_26\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_288 (Dense)           (None, 80)                1120      \n",
      "                                                                 \n",
      " dense_289 (Dense)           (None, 80)                6480      \n",
      "                                                                 \n",
      " dense_290 (Dense)           (None, 80)                6480      \n",
      "                                                                 \n",
      " dense_291 (Dense)           (None, 80)                6480      \n",
      "                                                                 \n",
      " dense_292 (Dense)           (None, 80)                6480      \n",
      "                                                                 \n",
      " dense_293 (Dense)           (None, 80)                6480      \n",
      "                                                                 \n",
      " dense_294 (Dense)           (None, 80)                6480      \n",
      "                                                                 \n",
      " dense_295 (Dense)           (None, 80)                6480      \n",
      "                                                                 \n",
      " dense_296 (Dense)           (None, 1)                 81        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 46,561\n",
      "Trainable params: 46,561\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_27\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_297 (Dense)           (None, 95)                1330      \n",
      "                                                                 \n",
      " dense_298 (Dense)           (None, 95)                9120      \n",
      "                                                                 \n",
      " dense_299 (Dense)           (None, 95)                9120      \n",
      "                                                                 \n",
      " dense_300 (Dense)           (None, 95)                9120      \n",
      "                                                                 \n",
      " dense_301 (Dense)           (None, 95)                9120      \n",
      "                                                                 \n",
      " dense_302 (Dense)           (None, 95)                9120      \n",
      "                                                                 \n",
      " dense_303 (Dense)           (None, 95)                9120      \n",
      "                                                                 \n",
      " dense_304 (Dense)           (None, 95)                9120      \n",
      "                                                                 \n",
      " dense_305 (Dense)           (None, 1)                 96        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 65,266\n",
      "Trainable params: 65,266\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_28\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_306 (Dense)           (None, 110)               1540      \n",
      "                                                                 \n",
      " dense_307 (Dense)           (None, 110)               12210     \n",
      "                                                                 \n",
      " dense_308 (Dense)           (None, 110)               12210     \n",
      "                                                                 \n",
      " dense_309 (Dense)           (None, 110)               12210     \n",
      "                                                                 \n",
      " dense_310 (Dense)           (None, 110)               12210     \n",
      "                                                                 \n",
      " dense_311 (Dense)           (None, 110)               12210     \n",
      "                                                                 \n",
      " dense_312 (Dense)           (None, 110)               12210     \n",
      "                                                                 \n",
      " dense_313 (Dense)           (None, 110)               12210     \n",
      "                                                                 \n",
      " dense_314 (Dense)           (None, 1)                 111       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 87,121\n",
      "Trainable params: 87,121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>configuracion</th>\n",
       "      <th>error_test</th>\n",
       "      <th>error_train</th>\n",
       "      <th>segundos_entrenamiento</th>\n",
       "      <th>epoca</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ANN_capas=[64; 64; 64; 64; 64; 64; 64; 64; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1</td>\n",
       "      <td>54231.385747</td>\n",
       "      <td>51958.811996</td>\n",
       "      <td>63.929625</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>ANN_capas=[50; 50; 50; 50; 50; 50; 50; 50; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1</td>\n",
       "      <td>55458.095748</td>\n",
       "      <td>52839.030423</td>\n",
       "      <td>58.103801</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ANN_capas=[8; 8; 8; 8; 8; 8; 8; 8; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=10</td>\n",
       "      <td>56168.076912</td>\n",
       "      <td>65505.463589</td>\n",
       "      <td>52.175863</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ANN_capas=[80; 80; 80; 80; 80; 80; 80; 80; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1</td>\n",
       "      <td>56252.450400</td>\n",
       "      <td>50596.551345</td>\n",
       "      <td>68.058731</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ANN_capas=[10; 10; 10; 10; 10; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=10</td>\n",
       "      <td>56579.769105</td>\n",
       "      <td>65503.054219</td>\n",
       "      <td>46.783819</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ANN_capas=[64; 64; 64; 64; 64; 64; 64; 64; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=10</td>\n",
       "      <td>57062.799371</td>\n",
       "      <td>57590.639239</td>\n",
       "      <td>17.061472</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ANN_capas=[10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=10</td>\n",
       "      <td>57541.599283</td>\n",
       "      <td>63024.952519</td>\n",
       "      <td>25.163197</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ANN_capas=[16; 16; 16; 16; 16; 16; 16; 16; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=10</td>\n",
       "      <td>57547.789671</td>\n",
       "      <td>60330.812824</td>\n",
       "      <td>41.198634</td>\n",
       "      <td>39.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ANN_capas=[32; 32; 32; 32; 32; 32; 32; 32; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1</td>\n",
       "      <td>57942.316143</td>\n",
       "      <td>54786.144818</td>\n",
       "      <td>57.596981</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>ANN_capas=[110; 110; 110; 110; 110; 110; 110; 110; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1</td>\n",
       "      <td>58199.064735</td>\n",
       "      <td>49075.829000</td>\n",
       "      <td>73.730136</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>ANN_capas=[95; 95; 95; 95; 95; 95; 95; 95; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1</td>\n",
       "      <td>58588.336314</td>\n",
       "      <td>49883.643171</td>\n",
       "      <td>72.325416</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>ANN_capas=[16; 16; 16; 16; 16; 16; 16; 16; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1</td>\n",
       "      <td>59041.563123</td>\n",
       "      <td>58927.122448</td>\n",
       "      <td>52.700294</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ANN_capas=[10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=10</td>\n",
       "      <td>59104.701674</td>\n",
       "      <td>62958.017758</td>\n",
       "      <td>24.140647</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ANN_capas=[10; 10; 1]_activaciones=['relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=10</td>\n",
       "      <td>59305.700232</td>\n",
       "      <td>70519.912422</td>\n",
       "      <td>45.313531</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ANN_capas=[10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=10</td>\n",
       "      <td>59431.114275</td>\n",
       "      <td>61390.786540</td>\n",
       "      <td>22.732815</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                   configuracion  \\\n",
       "14                                                                                                                                                  ANN_capas=[64; 64; 64; 64; 64; 64; 64; 64; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1   \n",
       "18                                                                                                                                                  ANN_capas=[50; 50; 50; 50; 50; 50; 50; 50; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1   \n",
       "7                                                                                                                                                           ANN_capas=[8; 8; 8; 8; 8; 8; 8; 8; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=10   \n",
       "15                                                                                                                                                  ANN_capas=[80; 80; 80; 80; 80; 80; 80; 80; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1   \n",
       "3                                                                                                                                                                                       ANN_capas=[10; 10; 10; 10; 10; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=10   \n",
       "10                                                                                                                                                  ANN_capas=[64; 64; 64; 64; 64; 64; 64; 64; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=10   \n",
       "1                                                                                                                           ANN_capas=[10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=10   \n",
       "8                                                                                                                                                   ANN_capas=[16; 16; 16; 16; 16; 16; 16; 16; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=10   \n",
       "13                                                                                                                                                  ANN_capas=[32; 32; 32; 32; 32; 32; 32; 32; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1   \n",
       "17                                                                                                                                          ANN_capas=[110; 110; 110; 110; 110; 110; 110; 110; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1   \n",
       "16                                                                                                                                                  ANN_capas=[95; 95; 95; 95; 95; 95; 95; 95; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1   \n",
       "12                                                                                                                                                  ANN_capas=[16; 16; 16; 16; 16; 16; 16; 16; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1   \n",
       "4                                                               ANN_capas=[10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=10   \n",
       "0                                                                                                                                                                                                                           ANN_capas=[10; 10; 1]_activaciones=['relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=10   \n",
       "5   ANN_capas=[10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=10   \n",
       "\n",
       "      error_test   error_train  segundos_entrenamiento  epoca  \n",
       "14  54231.385747  51958.811996               63.929625   50.0  \n",
       "18  55458.095748  52839.030423               58.103801   50.0  \n",
       "7   56168.076912  65505.463589               52.175863   50.0  \n",
       "15  56252.450400  50596.551345               68.058731   50.0  \n",
       "3   56579.769105  65503.054219               46.783819   50.0  \n",
       "10  57062.799371  57590.639239               17.061472   13.0  \n",
       "1   57541.599283  63024.952519               25.163197   20.0  \n",
       "8   57547.789671  60330.812824               41.198634   39.0  \n",
       "13  57942.316143  54786.144818               57.596981   50.0  \n",
       "17  58199.064735  49075.829000               73.730136   50.0  \n",
       "16  58588.336314  49883.643171               72.325416   50.0  \n",
       "12  59041.563123  58927.122448               52.700294   50.0  \n",
       "4   59104.701674  62958.017758               24.140647   17.0  \n",
       "0   59305.700232  70519.912422               45.313531   50.0  \n",
       "5   59431.114275  61390.786540               22.732815   14.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Experimentos para número de capas\n",
    "experimentoNumeroNeuronas = [50, 80, 95, 110]\n",
    "for experimento in experimentoNumeroNeuronas:\n",
    "    listaCapas = []\n",
    "    listaActivaciones = []\n",
    "    for iCapa in range(8):\n",
    "        listaCapas.append(experimento)\n",
    "        listaActivaciones.append('relu')\n",
    "    listaCapas.append(1)\n",
    "    listaActivaciones.append('linear')\n",
    "    redAnn, configuracion = construirANN(caracteristicasEstandarizadas.shape[1], listaCapas, listaActivaciones)\n",
    "    redAnn.summary()\n",
    "    entrenarANN(redAnn, configuracion, caracteristicasEstandarizadas, etiquetas, 'mean_squared_error', tamanioBatch=32, \\\n",
    "                epocas=50, paciencia=-1)\n",
    "\n",
    "mostrarResultadosBitacora('ANN')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hazardous-skirt",
   "metadata": {},
   "source": [
    "### Comentarios sobre número de neuronas y pruebas sobre un número de neuronas variable entre capas\n",
    "* Se puede observar que un número entre 50 y 80 neuronas entre las capas ocultas parece mantener la propiedad de mejorar el error en los datos de entrenamiento sin empeorar mucho el error en los datos de validación.\n",
    "* Se realizarán pruebas con un número variable de neuronas en las capas ocultas y se observará si alguna configuración en particular mejora el rendimiento del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "hawaiian-madonna",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_35\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_369 (Dense)           (None, 80)                1120      \n",
      "                                                                 \n",
      " dense_370 (Dense)           (None, 80)                6480      \n",
      "                                                                 \n",
      " dense_371 (Dense)           (None, 64)                5184      \n",
      "                                                                 \n",
      " dense_372 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_373 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_374 (Dense)           (None, 50)                3250      \n",
      "                                                                 \n",
      " dense_375 (Dense)           (None, 50)                2550      \n",
      "                                                                 \n",
      " dense_376 (Dense)           (None, 50)                2550      \n",
      "                                                                 \n",
      " dense_377 (Dense)           (None, 1)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 29,505\n",
      "Trainable params: 29,505\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_36\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_378 (Dense)           (None, 50)                700       \n",
      "                                                                 \n",
      " dense_379 (Dense)           (None, 50)                2550      \n",
      "                                                                 \n",
      " dense_380 (Dense)           (None, 50)                2550      \n",
      "                                                                 \n",
      " dense_381 (Dense)           (None, 64)                3264      \n",
      "                                                                 \n",
      " dense_382 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_383 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_384 (Dense)           (None, 80)                5200      \n",
      "                                                                 \n",
      " dense_385 (Dense)           (None, 80)                6480      \n",
      "                                                                 \n",
      " dense_386 (Dense)           (None, 1)                 81        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 29,145\n",
      "Trainable params: 29,145\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_37\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_387 (Dense)           (None, 50)                700       \n",
      "                                                                 \n",
      " dense_388 (Dense)           (None, 50)                2550      \n",
      "                                                                 \n",
      " dense_389 (Dense)           (None, 64)                3264      \n",
      "                                                                 \n",
      " dense_390 (Dense)           (None, 80)                5200      \n",
      "                                                                 \n",
      " dense_391 (Dense)           (None, 80)                6480      \n",
      "                                                                 \n",
      " dense_392 (Dense)           (None, 64)                5184      \n",
      "                                                                 \n",
      " dense_393 (Dense)           (None, 50)                3250      \n",
      "                                                                 \n",
      " dense_394 (Dense)           (None, 50)                2550      \n",
      "                                                                 \n",
      " dense_395 (Dense)           (None, 1)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 29,229\n",
      "Trainable params: 29,229\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_38\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_396 (Dense)           (None, 80)                1120      \n",
      "                                                                 \n",
      " dense_397 (Dense)           (None, 80)                6480      \n",
      "                                                                 \n",
      " dense_398 (Dense)           (None, 64)                5184      \n",
      "                                                                 \n",
      " dense_399 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_400 (Dense)           (None, 50)                3250      \n",
      "                                                                 \n",
      " dense_401 (Dense)           (None, 50)                2550      \n",
      "                                                                 \n",
      " dense_402 (Dense)           (None, 32)                1632      \n",
      "                                                                 \n",
      " dense_403 (Dense)           (None, 32)                1056      \n",
      "                                                                 \n",
      " dense_404 (Dense)           (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 25,465\n",
      "Trainable params: 25,465\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_39\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_405 (Dense)           (None, 32)                448       \n",
      "                                                                 \n",
      " dense_406 (Dense)           (None, 32)                1056      \n",
      "                                                                 \n",
      " dense_407 (Dense)           (None, 50)                1650      \n",
      "                                                                 \n",
      " dense_408 (Dense)           (None, 50)                2550      \n",
      "                                                                 \n",
      " dense_409 (Dense)           (None, 64)                3264      \n",
      "                                                                 \n",
      " dense_410 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_411 (Dense)           (None, 80)                5200      \n",
      "                                                                 \n",
      " dense_412 (Dense)           (None, 80)                6480      \n",
      "                                                                 \n",
      " dense_413 (Dense)           (None, 1)                 81        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 24,889\n",
      "Trainable params: 24,889\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_40\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_414 (Dense)           (None, 32)                448       \n",
      "                                                                 \n",
      " dense_415 (Dense)           (None, 50)                1650      \n",
      "                                                                 \n",
      " dense_416 (Dense)           (None, 64)                3264      \n",
      "                                                                 \n",
      " dense_417 (Dense)           (None, 80)                5200      \n",
      "                                                                 \n",
      " dense_418 (Dense)           (None, 80)                6480      \n",
      "                                                                 \n",
      " dense_419 (Dense)           (None, 64)                5184      \n",
      "                                                                 \n",
      " dense_420 (Dense)           (None, 50)                3250      \n",
      "                                                                 \n",
      " dense_421 (Dense)           (None, 32)                1632      \n",
      "                                                                 \n",
      " dense_422 (Dense)           (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 27,141\n",
      "Trainable params: 27,141\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_41\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_423 (Dense)           (None, 16)                224       \n",
      "                                                                 \n",
      " dense_424 (Dense)           (None, 50)                850       \n",
      "                                                                 \n",
      " dense_425 (Dense)           (None, 64)                3264      \n",
      "                                                                 \n",
      " dense_426 (Dense)           (None, 80)                5200      \n",
      "                                                                 \n",
      " dense_427 (Dense)           (None, 64)                5184      \n",
      "                                                                 \n",
      " dense_428 (Dense)           (None, 50)                3250      \n",
      "                                                                 \n",
      " dense_429 (Dense)           (None, 32)                1632      \n",
      "                                                                 \n",
      " dense_430 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " dense_431 (Dense)           (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20,149\n",
      "Trainable params: 20,149\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "549/549 [==============================] - 2s 2ms/step - loss: 17227274240.0000 - val_loss: 3984934912.0000\n",
      "Epoch 2/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 5207210496.0000 - val_loss: 3524737024.0000\n",
      "Epoch 3/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4766981632.0000 - val_loss: 3423668480.0000\n",
      "Epoch 4/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4553783808.0000 - val_loss: 3375745536.0000\n",
      "Epoch 5/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4416187392.0000 - val_loss: 3409472512.0000\n",
      "Epoch 6/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4271697920.0000 - val_loss: 3446437632.0000\n",
      "Epoch 7/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4164544256.0000 - val_loss: 3428932864.0000\n",
      "Epoch 8/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4034557952.0000 - val_loss: 3509842688.0000\n",
      "Epoch 9/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3962666752.0000 - val_loss: 3675990016.0000\n",
      "Epoch 10/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3904839680.0000 - val_loss: 4225853440.0000\n",
      "Epoch 11/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3874876928.0000 - val_loss: 3711889920.0000\n",
      "Epoch 12/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3799297536.0000 - val_loss: 4026483968.0000\n",
      "Epoch 13/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3750989824.0000 - val_loss: 3595505152.0000\n",
      "Epoch 14/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3721683712.0000 - val_loss: 3612545280.0000\n",
      "Epoch 15/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3690523904.0000 - val_loss: 3906284544.0000\n",
      "Epoch 16/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3652892672.0000 - val_loss: 3662343680.0000\n",
      "Epoch 17/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3618571776.0000 - val_loss: 3456422400.0000\n",
      "Epoch 18/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3578870784.0000 - val_loss: 3489377792.0000\n",
      "Epoch 19/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3577389056.0000 - val_loss: 3610123520.0000\n",
      "Epoch 20/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3543272704.0000 - val_loss: 3508220672.0000\n",
      "Epoch 21/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3532046080.0000 - val_loss: 3492569856.0000\n",
      "Epoch 22/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 3501829888.0000 - val_loss: 3423911168.0000\n",
      "Epoch 23/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 3449435648.0000 - val_loss: 3423577856.0000\n",
      "Epoch 24/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 3442635008.0000 - val_loss: 3460314624.0000\n",
      "Epoch 25/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3422535168.0000 - val_loss: 3792946944.0000\n",
      "Epoch 26/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3385134848.0000 - val_loss: 3900949248.0000\n",
      "Epoch 27/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3379715584.0000 - val_loss: 3871820800.0000\n",
      "Epoch 28/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3356996864.0000 - val_loss: 3685372160.0000\n",
      "Epoch 29/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3327660032.0000 - val_loss: 3434110720.0000\n",
      "Epoch 30/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3317744384.0000 - val_loss: 3494550784.0000\n",
      "Epoch 31/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3299351296.0000 - val_loss: 3554453248.0000\n",
      "Epoch 32/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3252961024.0000 - val_loss: 3638518528.0000\n",
      "Epoch 33/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3257848576.0000 - val_loss: 3792788480.0000\n",
      "Epoch 34/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3243688192.0000 - val_loss: 3647754752.0000\n",
      "Epoch 35/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3219310848.0000 - val_loss: 3364521728.0000\n",
      "Epoch 36/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3219898368.0000 - val_loss: 3382773248.0000\n",
      "Epoch 37/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3181993472.0000 - val_loss: 3542821632.0000\n",
      "Epoch 38/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3166636032.0000 - val_loss: 3573241600.0000\n",
      "Epoch 39/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3163854848.0000 - val_loss: 3545633024.0000\n",
      "Epoch 40/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3140185600.0000 - val_loss: 3508405504.0000\n",
      "Epoch 41/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3126475520.0000 - val_loss: 3425421312.0000\n",
      "Epoch 42/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3108220928.0000 - val_loss: 3565524480.0000\n",
      "Epoch 43/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3085707008.0000 - val_loss: 3666181632.0000\n",
      "Epoch 44/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3090679296.0000 - val_loss: 3450041600.0000\n",
      "Epoch 45/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3073707264.0000 - val_loss: 3427807232.0000\n",
      "Epoch 46/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3050342144.0000 - val_loss: 3492844800.0000\n",
      "Epoch 47/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3027292160.0000 - val_loss: 3539506944.0000\n",
      "Epoch 48/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3039971840.0000 - val_loss: 3613839360.0000\n",
      "Epoch 49/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2987520000.0000 - val_loss: 3861469184.0000\n",
      "Epoch 50/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2999960832.0000 - val_loss: 3416955648.0000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>configuracion</th>\n",
       "      <th>error_test</th>\n",
       "      <th>error_train</th>\n",
       "      <th>segundos_entrenamiento</th>\n",
       "      <th>epoca</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ANN_capas=[64; 64; 64; 64; 64; 64; 64; 64; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1</td>\n",
       "      <td>54231.385747</td>\n",
       "      <td>51958.811996</td>\n",
       "      <td>63.929625</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>ANN_capas=[32; 50; 64; 80; 80; 64; 50; 32; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1</td>\n",
       "      <td>54464.082256</td>\n",
       "      <td>52447.311771</td>\n",
       "      <td>65.858972</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>ANN_capas=[50; 50; 50; 50; 50; 50; 50; 50; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1</td>\n",
       "      <td>55458.095748</td>\n",
       "      <td>52839.030423</td>\n",
       "      <td>58.103801</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>ANN_capas=[80; 80; 64; 64; 50; 50; 32; 32; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1</td>\n",
       "      <td>55472.535331</td>\n",
       "      <td>51841.128383</td>\n",
       "      <td>63.146074</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ANN_capas=[8; 8; 8; 8; 8; 8; 8; 8; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=10</td>\n",
       "      <td>56168.076912</td>\n",
       "      <td>65505.463589</td>\n",
       "      <td>52.175863</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ANN_capas=[80; 80; 80; 80; 80; 80; 80; 80; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1</td>\n",
       "      <td>56252.450400</td>\n",
       "      <td>50596.551345</td>\n",
       "      <td>68.058731</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ANN_capas=[10; 10; 10; 10; 10; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=10</td>\n",
       "      <td>56579.769105</td>\n",
       "      <td>65503.054219</td>\n",
       "      <td>46.783819</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>ANN_capas=[50; 50; 50; 64; 64; 64; 80; 80; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1</td>\n",
       "      <td>56641.712968</td>\n",
       "      <td>50598.160283</td>\n",
       "      <td>71.024920</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>ANN_capas=[50; 50; 64; 80; 80; 64; 50; 50; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1</td>\n",
       "      <td>57028.581185</td>\n",
       "      <td>51121.124244</td>\n",
       "      <td>66.832304</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ANN_capas=[64; 64; 64; 64; 64; 64; 64; 64; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=10</td>\n",
       "      <td>57062.799371</td>\n",
       "      <td>57590.639239</td>\n",
       "      <td>17.061472</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ANN_capas=[10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=10</td>\n",
       "      <td>57541.599283</td>\n",
       "      <td>63024.952519</td>\n",
       "      <td>25.163197</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ANN_capas=[16; 16; 16; 16; 16; 16; 16; 16; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=10</td>\n",
       "      <td>57547.789671</td>\n",
       "      <td>60330.812824</td>\n",
       "      <td>41.198634</td>\n",
       "      <td>39.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ANN_capas=[32; 32; 32; 32; 32; 32; 32; 32; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1</td>\n",
       "      <td>57942.316143</td>\n",
       "      <td>54786.144818</td>\n",
       "      <td>57.596981</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>ANN_capas=[32; 32; 50; 50; 64; 64; 80; 80; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1</td>\n",
       "      <td>58058.025595</td>\n",
       "      <td>51759.779907</td>\n",
       "      <td>62.018519</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>ANN_capas=[80; 80; 64; 64; 64; 50; 50; 50; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1</td>\n",
       "      <td>58100.410739</td>\n",
       "      <td>51156.151223</td>\n",
       "      <td>62.984438</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                           configuracion  \\\n",
       "14                          ANN_capas=[64; 64; 64; 64; 64; 64; 64; 64; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1   \n",
       "24                          ANN_capas=[32; 50; 64; 80; 80; 64; 50; 32; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1   \n",
       "18                          ANN_capas=[50; 50; 50; 50; 50; 50; 50; 50; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1   \n",
       "22                          ANN_capas=[80; 80; 64; 64; 50; 50; 32; 32; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1   \n",
       "7                                   ANN_capas=[8; 8; 8; 8; 8; 8; 8; 8; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=10   \n",
       "15                          ANN_capas=[80; 80; 80; 80; 80; 80; 80; 80; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1   \n",
       "3                                                               ANN_capas=[10; 10; 10; 10; 10; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=10   \n",
       "20                          ANN_capas=[50; 50; 50; 64; 64; 64; 80; 80; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1   \n",
       "21                          ANN_capas=[50; 50; 64; 80; 80; 64; 50; 50; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1   \n",
       "10                          ANN_capas=[64; 64; 64; 64; 64; 64; 64; 64; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=10   \n",
       "1   ANN_capas=[10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=10   \n",
       "8                           ANN_capas=[16; 16; 16; 16; 16; 16; 16; 16; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=10   \n",
       "13                          ANN_capas=[32; 32; 32; 32; 32; 32; 32; 32; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1   \n",
       "23                          ANN_capas=[32; 32; 50; 50; 64; 64; 80; 80; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1   \n",
       "19                          ANN_capas=[80; 80; 64; 64; 64; 50; 50; 50; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1   \n",
       "\n",
       "      error_test   error_train  segundos_entrenamiento  epoca  \n",
       "14  54231.385747  51958.811996               63.929625   50.0  \n",
       "24  54464.082256  52447.311771               65.858972   50.0  \n",
       "18  55458.095748  52839.030423               58.103801   50.0  \n",
       "22  55472.535331  51841.128383               63.146074   50.0  \n",
       "7   56168.076912  65505.463589               52.175863   50.0  \n",
       "15  56252.450400  50596.551345               68.058731   50.0  \n",
       "3   56579.769105  65503.054219               46.783819   50.0  \n",
       "20  56641.712968  50598.160283               71.024920   50.0  \n",
       "21  57028.581185  51121.124244               66.832304   50.0  \n",
       "10  57062.799371  57590.639239               17.061472   13.0  \n",
       "1   57541.599283  63024.952519               25.163197   20.0  \n",
       "8   57547.789671  60330.812824               41.198634   39.0  \n",
       "13  57942.316143  54786.144818               57.596981   50.0  \n",
       "23  58058.025595  51759.779907               62.018519   50.0  \n",
       "19  58100.410739  51156.151223               62.984438   50.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Experimentos para número de capas\n",
    "experimentosListasNumeroNeuronas = [[80, 80, 64, 64, 64, 50, 50, 50], [50, 50, 50, 64, 64, 64, 80, 80], \\\n",
    "                                   [50, 50, 64, 80, 80, 64, 50, 50], [80, 80, 64, 64, 50, 50, 32, 32], \\\n",
    "                                   [32, 32, 50, 50, 64, 64, 80, 80], [32, 50, 64, 80, 80, 64, 50, 32], \\\n",
    "                                   [16, 50, 64, 80, 64, 50, 32, 16]]\n",
    "for experimento in experimentosListasNumeroNeuronas:\n",
    "    listaCapas = experimento\n",
    "    listaActivaciones = []\n",
    "    for iCapa in range(8):\n",
    "        listaActivaciones.append('relu')\n",
    "    listaCapas.append(1)\n",
    "    listaActivaciones.append('linear')\n",
    "    redAnn, configuracion = construirANN(caracteristicasEstandarizadas.shape[1], listaCapas, listaActivaciones)\n",
    "    redAnn.summary()\n",
    "    entrenarANN(redAnn, configuracion, caracteristicasEstandarizadas, etiquetas, 'mean_squared_error', tamanioBatch=32, \\\n",
    "                epocas=50, paciencia=-1)\n",
    "\n",
    "mostrarResultadosBitacora('ANN')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "willing-traffic",
   "metadata": {},
   "source": [
    "### Consideraciones y conclusión al número de neuronas por capa\n",
    "Se utilizará un modelo con __64__ neuronas por capa ya que a pesar de aumentar bastante el número de parámetros del modelo, reduce bastante bien el error en los datos de entrenamiento en un buen tiempo sin empeorar mucho el error en los datos de validación.\n",
    "\n",
    "### Función de activación\n",
    "Definido el número de capas y de neuronas por capa, se verificarán las diferentes funciones de activación y se observará cuál se desempeña mejor:\n",
    "* relu\n",
    "* sigmoid\n",
    "* tanh\n",
    "* selu\n",
    "* elu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "strong-contamination",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_52\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_522 (Dense)           (None, 64)                896       \n",
      "                                                                 \n",
      " dense_523 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_524 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_525 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_526 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_527 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_528 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_529 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_530 (Dense)           (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 30,081\n",
      "Trainable params: 30,081\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_53\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_531 (Dense)           (None, 64)                896       \n",
      "                                                                 \n",
      " dense_532 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_533 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_534 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_535 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_536 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_537 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_538 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_539 (Dense)           (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 30,081\n",
      "Trainable params: 30,081\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 56037310464.0000 - val_loss: 56445706240.0000\n",
      "Epoch 2/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 56028749824.0000 - val_loss: 56437342208.0000\n",
      "Epoch 3/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 56020578304.0000 - val_loss: 56429031424.0000\n",
      "Epoch 4/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 56011681792.0000 - val_loss: 56419561472.0000\n",
      "Epoch 5/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 56002179072.0000 - val_loss: 56410198016.0000\n",
      "Epoch 6/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 55992582144.0000 - val_loss: 56400183296.0000\n",
      "Epoch 7/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 55982391296.0000 - val_loss: 56389971968.0000\n",
      "Epoch 8/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 55972524032.0000 - val_loss: 56379936768.0000\n",
      "Epoch 9/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 55962365952.0000 - val_loss: 56369917952.0000\n",
      "Epoch 10/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 55952322560.0000 - val_loss: 56359583744.0000\n",
      "Epoch 11/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 55942074368.0000 - val_loss: 56349450240.0000\n",
      "Epoch 12/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 55932014592.0000 - val_loss: 56339406848.0000\n",
      "Epoch 13/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 55922012160.0000 - val_loss: 56329424896.0000\n",
      "Epoch 14/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 55912091648.0000 - val_loss: 56319434752.0000\n",
      "Epoch 15/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 55902113792.0000 - val_loss: 56309452800.0000\n",
      "Epoch 16/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 55892197376.0000 - val_loss: 56299474944.0000\n",
      "Epoch 17/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 55882256384.0000 - val_loss: 56289521664.0000\n",
      "Epoch 18/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 55872323584.0000 - val_loss: 56279560192.0000\n",
      "Epoch 19/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 55862444032.0000 - val_loss: 56269606912.0000\n",
      "Epoch 20/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 55852515328.0000 - val_loss: 56259678208.0000\n",
      "Epoch 21/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 55842205696.0000 - val_loss: 56249077760.0000\n",
      "Epoch 22/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 55831863296.0000 - val_loss: 56238809088.0000\n",
      "Epoch 23/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 55821701120.0000 - val_loss: 56228622336.0000\n",
      "Epoch 24/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 55811526656.0000 - val_loss: 56218423296.0000\n",
      "Epoch 25/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 55801413632.0000 - val_loss: 56208252928.0000\n",
      "Epoch 26/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 55790776320.0000 - val_loss: 56196890624.0000\n",
      "Epoch 27/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 55774310400.0000 - val_loss: 56176427008.0000\n",
      "Epoch 28/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 55755915264.0000 - val_loss: 56159158272.0000\n",
      "Epoch 29/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 55739494400.0000 - val_loss: 56143454208.0000\n",
      "Epoch 30/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 55724138496.0000 - val_loss: 56128258048.0000\n",
      "Epoch 31/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 55709122560.0000 - val_loss: 56113344512.0000\n",
      "Epoch 32/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 55694336000.0000 - val_loss: 56098516992.0000\n",
      "Epoch 33/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 55679606784.0000 - val_loss: 56083787776.0000\n",
      "Epoch 34/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 55664934912.0000 - val_loss: 56069054464.0000\n",
      "Epoch 35/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 55650271232.0000 - val_loss: 56054378496.0000\n",
      "Epoch 36/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 55635664896.0000 - val_loss: 56039710720.0000\n",
      "Epoch 37/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 55621087232.0000 - val_loss: 56025047040.0000\n",
      "Epoch 38/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 55606472704.0000 - val_loss: 56010383360.0000\n",
      "Epoch 39/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 55591796736.0000 - val_loss: 55995715584.0000\n",
      "Epoch 40/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 55577210880.0000 - val_loss: 55981060096.0000\n",
      "Epoch 41/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 55562608640.0000 - val_loss: 55966425088.0000\n",
      "Epoch 42/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 55548039168.0000 - val_loss: 55951794176.0000\n",
      "Epoch 43/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 55533424640.0000 - val_loss: 55937134592.0000\n",
      "Epoch 44/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 55518842880.0000 - val_loss: 55922487296.0000\n",
      "Epoch 45/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 55504232448.0000 - val_loss: 55907844096.0000\n",
      "Epoch 46/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 55489646592.0000 - val_loss: 55893209088.0000\n",
      "Epoch 47/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 55475056640.0000 - val_loss: 55878569984.0000\n",
      "Epoch 48/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 55460507648.0000 - val_loss: 55863967744.0000\n",
      "Epoch 49/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 55445893120.0000 - val_loss: 55849320448.0000\n",
      "Epoch 50/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 55431344128.0000 - val_loss: 55834710016.0000\n",
      "Model: \"sequential_54\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_540 (Dense)           (None, 64)                896       \n",
      "                                                                 \n",
      " dense_541 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_542 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_543 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_544 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_545 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_546 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_547 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_548 (Dense)           (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 30,081\n",
      "Trainable params: 30,081\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "549/549 [==============================] - 2s 2ms/step - loss: 56031096832.0000 - val_loss: 56436105216.0000\n",
      "Epoch 2/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 56015986688.0000 - val_loss: 56421277696.0000\n",
      "Epoch 3/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 56001212416.0000 - val_loss: 56406515712.0000\n",
      "Epoch 4/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 55986532352.0000 - val_loss: 56391811072.0000\n",
      "Epoch 5/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 55971799040.0000 - val_loss: 56377061376.0000\n",
      "Epoch 6/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 55957086208.0000 - val_loss: 56362348544.0000\n",
      "Epoch 7/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 55942459392.0000 - val_loss: 56347639808.0000\n",
      "Epoch 8/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 55927758848.0000 - val_loss: 56332963840.0000\n",
      "Epoch 9/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 55913156608.0000 - val_loss: 56318267392.0000\n",
      "Epoch 10/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 55898443776.0000 - val_loss: 56303566848.0000\n",
      "Epoch 11/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 55883841536.0000 - val_loss: 56288899072.0000\n",
      "Epoch 12/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 55869140992.0000 - val_loss: 56274194432.0000\n",
      "Epoch 13/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 55854505984.0000 - val_loss: 56259526656.0000\n",
      "Epoch 14/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 55839862784.0000 - val_loss: 56244846592.0000\n",
      "Epoch 15/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 55825207296.0000 - val_loss: 56230166528.0000\n",
      "Epoch 16/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 55810547712.0000 - val_loss: 56215502848.0000\n",
      "Epoch 17/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 55795929088.0000 - val_loss: 56200806400.0000\n",
      "Epoch 18/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 55781302272.0000 - val_loss: 56186130432.0000\n",
      "Epoch 19/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 55766679552.0000 - val_loss: 56171466752.0000\n",
      "Epoch 20/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 55751983104.0000 - val_loss: 56156827648.0000\n",
      "Epoch 21/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 55737364480.0000 - val_loss: 56142139392.0000\n",
      "Epoch 22/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 55722766336.0000 - val_loss: 56127504384.0000\n",
      "Epoch 23/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 55708176384.0000 - val_loss: 56112844800.0000\n",
      "Epoch 24/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 55693553664.0000 - val_loss: 56098172928.0000\n",
      "Epoch 25/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 55678922752.0000 - val_loss: 56083537920.0000\n",
      "Epoch 26/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 55664287744.0000 - val_loss: 56068874240.0000\n",
      "Epoch 27/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 55649648640.0000 - val_loss: 56054251520.0000\n",
      "Epoch 28/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 55635070976.0000 - val_loss: 56039596032.0000\n",
      "Epoch 29/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 55620419584.0000 - val_loss: 56024961024.0000\n",
      "Epoch 30/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 55605854208.0000 - val_loss: 56010301440.0000\n",
      "Epoch 31/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 55591223296.0000 - val_loss: 55995654144.0000\n",
      "Epoch 32/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 55576645632.0000 - val_loss: 55981027328.0000\n",
      "Epoch 33/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 55562031104.0000 - val_loss: 55966420992.0000\n",
      "Epoch 34/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 55547424768.0000 - val_loss: 55951785984.0000\n",
      "Epoch 35/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 55532847104.0000 - val_loss: 55937163264.0000\n",
      "Epoch 36/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 55518253056.0000 - val_loss: 55922524160.0000\n",
      "Epoch 37/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 55503654912.0000 - val_loss: 55907913728.0000\n",
      "Epoch 38/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 55489064960.0000 - val_loss: 55893295104.0000\n",
      "Epoch 39/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 55474495488.0000 - val_loss: 55878660096.0000\n",
      "Epoch 40/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 55459950592.0000 - val_loss: 55864057856.0000\n",
      "Epoch 41/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 55445344256.0000 - val_loss: 55849439232.0000\n",
      "Epoch 42/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 55430787072.0000 - val_loss: 55834849280.0000\n",
      "Epoch 43/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 55416180736.0000 - val_loss: 55820242944.0000\n",
      "Epoch 44/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 55401611264.0000 - val_loss: 55805648896.0000\n",
      "Epoch 45/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 55387037696.0000 - val_loss: 55791042560.0000\n",
      "Epoch 46/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 55372460032.0000 - val_loss: 55776452608.0000\n",
      "Epoch 47/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 55357902848.0000 - val_loss: 55761825792.0000\n",
      "Epoch 48/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 55343357952.0000 - val_loss: 55747264512.0000\n",
      "Epoch 49/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 55328808960.0000 - val_loss: 55732674560.0000\n",
      "Epoch 50/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 55314239488.0000 - val_loss: 55718096896.0000\n",
      "Model: \"sequential_55\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_549 (Dense)           (None, 64)                896       \n",
      "                                                                 \n",
      " dense_550 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_551 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_552 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_553 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_554 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_555 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_556 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_557 (Dense)           (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 30,081\n",
      "Trainable params: 30,081\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 12741874688.0000 - val_loss: 3780229376.0000\n",
      "Epoch 2/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4504688128.0000 - val_loss: 3407479040.0000\n",
      "Epoch 3/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4094493184.0000 - val_loss: 3296311552.0000\n",
      "Epoch 4/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3852285696.0000 - val_loss: 3380169472.0000\n",
      "Epoch 5/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3714493184.0000 - val_loss: 3218560256.0000\n",
      "Epoch 6/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3585540608.0000 - val_loss: 3335019008.0000\n",
      "Epoch 7/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3559949056.0000 - val_loss: 3330361088.0000\n",
      "Epoch 8/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3449091584.0000 - val_loss: 3157209088.0000\n",
      "Epoch 9/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3401786112.0000 - val_loss: 3273320960.0000\n",
      "Epoch 10/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3349716992.0000 - val_loss: 3583279104.0000\n",
      "Epoch 11/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3307800064.0000 - val_loss: 3129857536.0000\n",
      "Epoch 12/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3249327104.0000 - val_loss: 3219651328.0000\n",
      "Epoch 13/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3194759680.0000 - val_loss: 3277558272.0000\n",
      "Epoch 14/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3161031168.0000 - val_loss: 3144694784.0000\n",
      "Epoch 15/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3129569024.0000 - val_loss: 3225590016.0000\n",
      "Epoch 16/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3088320256.0000 - val_loss: 3199620352.0000\n",
      "Epoch 17/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3059211008.0000 - val_loss: 3189617408.0000\n",
      "Epoch 18/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3000550656.0000 - val_loss: 3204198656.0000\n",
      "Epoch 19/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3015418112.0000 - val_loss: 3252421376.0000\n",
      "Epoch 20/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2969876992.0000 - val_loss: 3176429568.0000\n",
      "Epoch 21/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2942315776.0000 - val_loss: 3124748800.0000\n",
      "Epoch 22/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2921080320.0000 - val_loss: 3131719936.0000\n",
      "Epoch 23/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2876646144.0000 - val_loss: 3579268864.0000\n",
      "Epoch 24/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2861697024.0000 - val_loss: 3247670528.0000\n",
      "Epoch 25/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2839105024.0000 - val_loss: 3075045376.0000\n",
      "Epoch 26/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2797335808.0000 - val_loss: 3319880448.0000\n",
      "Epoch 27/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2787174400.0000 - val_loss: 3388887808.0000\n",
      "Epoch 28/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2790097920.0000 - val_loss: 3072274176.0000\n",
      "Epoch 29/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2747391488.0000 - val_loss: 3162505216.0000\n",
      "Epoch 30/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2752161536.0000 - val_loss: 3073205504.0000\n",
      "Epoch 31/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2737645824.0000 - val_loss: 2954880512.0000\n",
      "Epoch 32/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2690049024.0000 - val_loss: 2983811072.0000\n",
      "Epoch 33/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2711879680.0000 - val_loss: 3058646016.0000\n",
      "Epoch 34/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2670162944.0000 - val_loss: 3107274240.0000\n",
      "Epoch 35/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2649611264.0000 - val_loss: 3159055616.0000\n",
      "Epoch 36/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 2672127232.0000 - val_loss: 3067029504.0000\n",
      "Epoch 37/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2628544512.0000 - val_loss: 3350292736.0000\n",
      "Epoch 38/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2616780800.0000 - val_loss: 3278231552.0000\n",
      "Epoch 39/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2616956160.0000 - val_loss: 3373328640.0000\n",
      "Epoch 40/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2607784960.0000 - val_loss: 3111142912.0000\n",
      "Epoch 41/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2569253120.0000 - val_loss: 3270034944.0000\n",
      "Epoch 42/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2585474816.0000 - val_loss: 3355866880.0000\n",
      "Epoch 43/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2551092480.0000 - val_loss: 3119264000.0000\n",
      "Epoch 44/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2574083072.0000 - val_loss: 3132393984.0000\n",
      "Epoch 45/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 2559760384.0000 - val_loss: 3101519360.0000\n",
      "Epoch 46/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 2542678784.0000 - val_loss: 3109432832.0000\n",
      "Epoch 47/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2527802368.0000 - val_loss: 3391398144.0000\n",
      "Epoch 48/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2539907584.0000 - val_loss: 3329610496.0000\n",
      "Epoch 49/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 2489826816.0000 - val_loss: 3102507520.0000\n",
      "Epoch 50/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2508555008.0000 - val_loss: 3028983808.0000\n",
      "Model: \"sequential_56\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_558 (Dense)           (None, 64)                896       \n",
      "                                                                 \n",
      " dense_559 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_560 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_561 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_562 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_563 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_564 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_565 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_566 (Dense)           (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 30,081\n",
      "Trainable params: 30,081\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 12557889536.0000 - val_loss: 3697672448.0000\n",
      "Epoch 2/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4528478208.0000 - val_loss: 3389339136.0000\n",
      "Epoch 3/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4202723584.0000 - val_loss: 3308553216.0000\n",
      "Epoch 4/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3957412608.0000 - val_loss: 3379048960.0000\n",
      "Epoch 5/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3825446144.0000 - val_loss: 3280989184.0000\n",
      "Epoch 6/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3713457664.0000 - val_loss: 3574827264.0000\n",
      "Epoch 7/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3695789824.0000 - val_loss: 3529154304.0000\n",
      "Epoch 8/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3623845888.0000 - val_loss: 3312336384.0000\n",
      "Epoch 9/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 3575429632.0000 - val_loss: 3480711424.0000\n",
      "Epoch 10/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3550134272.0000 - val_loss: 3778212096.0000\n",
      "Epoch 11/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3520719872.0000 - val_loss: 3481009920.0000\n",
      "Epoch 12/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3482146304.0000 - val_loss: 3619852288.0000\n",
      "Epoch 13/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3413114624.0000 - val_loss: 3480320000.0000\n",
      "Epoch 14/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3393126656.0000 - val_loss: 3459404032.0000\n",
      "Epoch 15/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3349471744.0000 - val_loss: 3527977728.0000\n",
      "Epoch 16/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3308579072.0000 - val_loss: 3463248128.0000\n",
      "Epoch 17/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3265932288.0000 - val_loss: 3478746368.0000\n",
      "Epoch 18/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 3211909632.0000 - val_loss: 3580603648.0000\n",
      "Epoch 19/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 3202082560.0000 - val_loss: 3527870208.0000\n",
      "Epoch 20/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 3170595072.0000 - val_loss: 3410961152.0000\n",
      "Epoch 21/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3147824128.0000 - val_loss: 3269108736.0000\n",
      "Epoch 22/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 3124166912.0000 - val_loss: 3204875264.0000\n",
      "Epoch 23/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 3070687232.0000 - val_loss: 3542271232.0000\n",
      "Epoch 24/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3052813312.0000 - val_loss: 3269476864.0000\n",
      "Epoch 25/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3030784256.0000 - val_loss: 3222132224.0000\n",
      "Epoch 26/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2981434112.0000 - val_loss: 3330184960.0000\n",
      "Epoch 27/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 2959887616.0000 - val_loss: 3408983552.0000\n",
      "Epoch 28/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 2964387072.0000 - val_loss: 3100001792.0000\n",
      "Epoch 29/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 2920799744.0000 - val_loss: 3083581952.0000\n",
      "Epoch 30/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 2910818560.0000 - val_loss: 3068209152.0000\n",
      "Epoch 31/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 2892419072.0000 - val_loss: 2887830784.0000\n",
      "Epoch 32/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2840213504.0000 - val_loss: 2969822208.0000\n",
      "Epoch 33/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2871948800.0000 - val_loss: 3027530240.0000\n",
      "Epoch 34/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2826061312.0000 - val_loss: 3088948736.0000\n",
      "Epoch 35/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2805245440.0000 - val_loss: 2921540608.0000\n",
      "Epoch 36/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2823396352.0000 - val_loss: 2935742976.0000\n",
      "Epoch 37/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2776606464.0000 - val_loss: 3234834944.0000\n",
      "Epoch 38/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2771848448.0000 - val_loss: 3037154304.0000\n",
      "Epoch 39/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2761400064.0000 - val_loss: 3040059904.0000\n",
      "Epoch 40/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2755213056.0000 - val_loss: 2953137408.0000\n",
      "Epoch 41/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2722328576.0000 - val_loss: 2941158400.0000\n",
      "Epoch 42/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 2717822208.0000 - val_loss: 3119179776.0000\n",
      "Epoch 43/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2711713280.0000 - val_loss: 2889962496.0000\n",
      "Epoch 44/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2708334336.0000 - val_loss: 2967217152.0000\n",
      "Epoch 45/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2689121536.0000 - val_loss: 2965677312.0000\n",
      "Epoch 46/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2675070208.0000 - val_loss: 2961047808.0000\n",
      "Epoch 47/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2660881152.0000 - val_loss: 3022682112.0000\n",
      "Epoch 48/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2658064128.0000 - val_loss: 3161844992.0000\n",
      "Epoch 49/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2619944192.0000 - val_loss: 2986365440.0000\n",
      "Epoch 50/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2642044928.0000 - val_loss: 2849054208.0000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>configuracion</th>\n",
       "      <th>error_test</th>\n",
       "      <th>error_train</th>\n",
       "      <th>segundos_entrenamiento</th>\n",
       "      <th>epoca</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>ANN_capas=[64; 64; 64; 64; 64; 64; 64; 64; 1]_activaciones=['elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1</td>\n",
       "      <td>53376.532371</td>\n",
       "      <td>51400.826141</td>\n",
       "      <td>67.506442</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ANN_capas=[64; 64; 64; 64; 64; 64; 64; 64; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1</td>\n",
       "      <td>54231.385747</td>\n",
       "      <td>51958.811996</td>\n",
       "      <td>63.929625</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>ANN_capas=[32; 50; 64; 80; 80; 64; 50; 32; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1</td>\n",
       "      <td>54464.082256</td>\n",
       "      <td>52447.311771</td>\n",
       "      <td>65.858972</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>ANN_capas=[64; 64; 64; 64; 64; 64; 64; 64; 1]_activaciones=['selu'; 'selu'; 'selu'; 'selu'; 'selu'; 'selu'; 'selu'; 'selu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1</td>\n",
       "      <td>55036.204520</td>\n",
       "      <td>50085.477017</td>\n",
       "      <td>63.393345</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>ANN_capas=[50; 50; 50; 50; 50; 50; 50; 50; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1</td>\n",
       "      <td>55458.095748</td>\n",
       "      <td>52839.030423</td>\n",
       "      <td>58.103801</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>ANN_capas=[80; 80; 64; 64; 50; 50; 32; 32; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1</td>\n",
       "      <td>55472.535331</td>\n",
       "      <td>51841.128383</td>\n",
       "      <td>63.146074</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ANN_capas=[8; 8; 8; 8; 8; 8; 8; 8; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=10</td>\n",
       "      <td>56168.076912</td>\n",
       "      <td>65505.463589</td>\n",
       "      <td>52.175863</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ANN_capas=[80; 80; 80; 80; 80; 80; 80; 80; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1</td>\n",
       "      <td>56252.450400</td>\n",
       "      <td>50596.551345</td>\n",
       "      <td>68.058731</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ANN_capas=[10; 10; 10; 10; 10; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=10</td>\n",
       "      <td>56579.769105</td>\n",
       "      <td>65503.054219</td>\n",
       "      <td>46.783819</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>ANN_capas=[50; 50; 50; 64; 64; 64; 80; 80; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1</td>\n",
       "      <td>56641.712968</td>\n",
       "      <td>50598.160283</td>\n",
       "      <td>71.024920</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>ANN_capas=[50; 50; 64; 80; 80; 64; 50; 50; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1</td>\n",
       "      <td>57028.581185</td>\n",
       "      <td>51121.124244</td>\n",
       "      <td>66.832304</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ANN_capas=[64; 64; 64; 64; 64; 64; 64; 64; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=10</td>\n",
       "      <td>57062.799371</td>\n",
       "      <td>57590.639239</td>\n",
       "      <td>17.061472</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>ANN_capas=[64; 64; 64; 64; 64; 64; 64; 64; 1]_activaciones=['selu'; 'selu'; 'selu'; 'selu'; 'selu'; 'selu'; 'selu'; 'selu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=10</td>\n",
       "      <td>57184.219362</td>\n",
       "      <td>50687.800505</td>\n",
       "      <td>54.517353</td>\n",
       "      <td>41.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ANN_capas=[10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=10</td>\n",
       "      <td>57541.599283</td>\n",
       "      <td>63024.952519</td>\n",
       "      <td>25.163197</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ANN_capas=[16; 16; 16; 16; 16; 16; 16; 16; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=10</td>\n",
       "      <td>57547.789671</td>\n",
       "      <td>60330.812824</td>\n",
       "      <td>41.198634</td>\n",
       "      <td>39.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                           configuracion  \\\n",
       "33                                  ANN_capas=[64; 64; 64; 64; 64; 64; 64; 64; 1]_activaciones=['elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1   \n",
       "14                          ANN_capas=[64; 64; 64; 64; 64; 64; 64; 64; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1   \n",
       "24                          ANN_capas=[32; 50; 64; 80; 80; 64; 50; 32; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1   \n",
       "32                          ANN_capas=[64; 64; 64; 64; 64; 64; 64; 64; 1]_activaciones=['selu'; 'selu'; 'selu'; 'selu'; 'selu'; 'selu'; 'selu'; 'selu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1   \n",
       "18                          ANN_capas=[50; 50; 50; 50; 50; 50; 50; 50; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1   \n",
       "22                          ANN_capas=[80; 80; 64; 64; 50; 50; 32; 32; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1   \n",
       "7                                   ANN_capas=[8; 8; 8; 8; 8; 8; 8; 8; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=10   \n",
       "15                          ANN_capas=[80; 80; 80; 80; 80; 80; 80; 80; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1   \n",
       "3                                                               ANN_capas=[10; 10; 10; 10; 10; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=10   \n",
       "20                          ANN_capas=[50; 50; 50; 64; 64; 64; 80; 80; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1   \n",
       "21                          ANN_capas=[50; 50; 64; 80; 80; 64; 50; 50; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1   \n",
       "10                          ANN_capas=[64; 64; 64; 64; 64; 64; 64; 64; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=10   \n",
       "28                          ANN_capas=[64; 64; 64; 64; 64; 64; 64; 64; 1]_activaciones=['selu'; 'selu'; 'selu'; 'selu'; 'selu'; 'selu'; 'selu'; 'selu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=10   \n",
       "1   ANN_capas=[10; 10; 10; 10; 10; 10; 10; 10; 10; 10; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=10   \n",
       "8                           ANN_capas=[16; 16; 16; 16; 16; 16; 16; 16; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=10   \n",
       "\n",
       "      error_test   error_train  segundos_entrenamiento  epoca  \n",
       "33  53376.532371  51400.826141               67.506442   50.0  \n",
       "14  54231.385747  51958.811996               63.929625   50.0  \n",
       "24  54464.082256  52447.311771               65.858972   50.0  \n",
       "32  55036.204520  50085.477017               63.393345   50.0  \n",
       "18  55458.095748  52839.030423               58.103801   50.0  \n",
       "22  55472.535331  51841.128383               63.146074   50.0  \n",
       "7   56168.076912  65505.463589               52.175863   50.0  \n",
       "15  56252.450400  50596.551345               68.058731   50.0  \n",
       "3   56579.769105  65503.054219               46.783819   50.0  \n",
       "20  56641.712968  50598.160283               71.024920   50.0  \n",
       "21  57028.581185  51121.124244               66.832304   50.0  \n",
       "10  57062.799371  57590.639239               17.061472   13.0  \n",
       "28  57184.219362  50687.800505               54.517353   41.0  \n",
       "1   57541.599283  63024.952519               25.163197   20.0  \n",
       "8   57547.789671  60330.812824               41.198634   39.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Experimentos para número de capas\n",
    "experimentoActivaciones = ['relu', 'sigmoid', 'tanh', 'selu', 'elu']\n",
    "listaCapas = [64, 64, 64, 64, 64, 64, 64, 64, 1]\n",
    "for experimento in experimentoActivaciones:\n",
    "    listaActivaciones = []\n",
    "    for iCapa in range(8):\n",
    "        listaActivaciones.append(experimento)\n",
    "    listaActivaciones.append('linear')\n",
    "    redAnn, configuracion = construirANN(caracteristicasEstandarizadas.shape[1], listaCapas, listaActivaciones)\n",
    "    redAnn.summary()\n",
    "    entrenarANN(redAnn, configuracion, caracteristicasEstandarizadas, etiquetas, 'mean_squared_error', \\\n",
    "                tamanioBatch=32, epocas=50,paciencia = -1)\n",
    "\n",
    "mostrarResultadosBitacora('ANN')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "offensive-projector",
   "metadata": {},
   "source": [
    "De la bitácora se puede observar que el mejor error en los datos de entrenamiento (y de validación) se pudo obtener de la función de activación __ELU__ para las capas ocultas. Algunas opciones aceptables podrían ser __ReLU__ y __SELU__.\n",
    "\n",
    "### Optimizadores\n",
    "Se realizarán experimentos con diversos optimizadores para observar si alguno mejora el rendimiento para la presente arquitectura:\n",
    "* rmsprop\n",
    "* adam\n",
    "* adadelta\n",
    "* adagrad\n",
    "* adamax\n",
    "* nadam\n",
    "* ftrl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "pressed-porter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_57\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_567 (Dense)           (None, 64)                896       \n",
      "                                                                 \n",
      " dense_568 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_569 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_570 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_571 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_572 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_573 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_574 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_575 (Dense)           (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 30,081\n",
      "Trainable params: 30,081\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "549/549 [==============================] - 3s 2ms/step - loss: 10655859712.0000 - val_loss: 3744950272.0000\n",
      "Epoch 2/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4594848768.0000 - val_loss: 3704283136.0000\n",
      "Epoch 3/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4315152896.0000 - val_loss: 3300472320.0000\n",
      "Epoch 4/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4089346560.0000 - val_loss: 3773577472.0000\n",
      "Epoch 5/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3905446912.0000 - val_loss: 3434771200.0000\n",
      "Epoch 6/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3808332288.0000 - val_loss: 4194656512.0000\n",
      "Epoch 7/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3741962496.0000 - val_loss: 3988566784.0000\n",
      "Epoch 8/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3697217280.0000 - val_loss: 3509794304.0000\n",
      "Epoch 9/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3660515328.0000 - val_loss: 3364419072.0000\n",
      "Epoch 10/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3625526528.0000 - val_loss: 3868811008.0000\n",
      "Epoch 11/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3580143104.0000 - val_loss: 3665424384.0000\n",
      "Epoch 12/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3516136192.0000 - val_loss: 4066867456.0000\n",
      "Epoch 13/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3462324224.0000 - val_loss: 3539903744.0000\n",
      "Epoch 14/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3454600704.0000 - val_loss: 3510826240.0000\n",
      "Epoch 15/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3401757696.0000 - val_loss: 3247070720.0000\n",
      "Epoch 16/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3346445312.0000 - val_loss: 3179930624.0000\n",
      "Epoch 17/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3309834752.0000 - val_loss: 3175496704.0000\n",
      "Epoch 18/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3279591936.0000 - val_loss: 3352059136.0000\n",
      "Epoch 19/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3244284160.0000 - val_loss: 4212869888.0000\n",
      "Epoch 20/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3228049152.0000 - val_loss: 3147973120.0000\n",
      "Epoch 21/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3185406720.0000 - val_loss: 3566774272.0000\n",
      "Epoch 22/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3168630784.0000 - val_loss: 3234165760.0000\n",
      "Epoch 23/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3150871552.0000 - val_loss: 3143156224.0000\n",
      "Epoch 24/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3116624896.0000 - val_loss: 3322449920.0000\n",
      "Epoch 25/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3091500288.0000 - val_loss: 3456096768.0000\n",
      "Epoch 26/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3081974528.0000 - val_loss: 3492883456.0000\n",
      "Epoch 27/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3038679040.0000 - val_loss: 3537767424.0000\n",
      "Epoch 28/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3030346496.0000 - val_loss: 3414639872.0000\n",
      "Epoch 29/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 3013444352.0000 - val_loss: 3131175168.0000\n",
      "Epoch 30/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 2982838528.0000 - val_loss: 3165581056.0000\n",
      "Epoch 31/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 2962422528.0000 - val_loss: 3038204672.0000\n",
      "Epoch 32/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2943335680.0000 - val_loss: 3242843904.0000\n",
      "Epoch 33/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2928169216.0000 - val_loss: 3220146432.0000\n",
      "Epoch 34/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2930771712.0000 - val_loss: 3754884864.0000\n",
      "Epoch 35/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2896329728.0000 - val_loss: 3293973248.0000\n",
      "Epoch 36/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 2875110656.0000 - val_loss: 3184331264.0000\n",
      "Epoch 37/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 2852195584.0000 - val_loss: 3169850368.0000\n",
      "Epoch 38/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2823440384.0000 - val_loss: 3372695296.0000\n",
      "Epoch 39/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2827442432.0000 - val_loss: 3218726400.0000\n",
      "Epoch 40/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2800361472.0000 - val_loss: 3447353856.0000\n",
      "Epoch 41/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2805331200.0000 - val_loss: 3131229184.0000\n",
      "Epoch 42/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 2780667392.0000 - val_loss: 3703138304.0000\n",
      "Epoch 43/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2768731904.0000 - val_loss: 3013135360.0000\n",
      "Epoch 44/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2750973184.0000 - val_loss: 3659642624.0000\n",
      "Epoch 45/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2731769088.0000 - val_loss: 3354081024.0000\n",
      "Epoch 46/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2724422912.0000 - val_loss: 3321673728.0000\n",
      "Epoch 47/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2719920384.0000 - val_loss: 3046321152.0000\n",
      "Epoch 48/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2693334528.0000 - val_loss: 3213130496.0000\n",
      "Epoch 49/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2702382080.0000 - val_loss: 3193263872.0000\n",
      "Epoch 50/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2684910848.0000 - val_loss: 3107293440.0000\n",
      "Model: \"sequential_58\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_576 (Dense)           (None, 64)                896       \n",
      "                                                                 \n",
      " dense_577 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_578 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_579 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_580 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_581 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_582 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_583 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_584 (Dense)           (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 30,081\n",
      "Trainable params: 30,081\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_59\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_585 (Dense)           (None, 64)                896       \n",
      "                                                                 \n",
      " dense_586 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_587 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_588 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_589 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_590 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_591 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_592 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_593 (Dense)           (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 30,081\n",
      "Trainable params: 30,081\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "549/549 [==============================] - 2s 2ms/step - loss: 56042905600.0000 - val_loss: 56455720960.0000\n",
      "Epoch 2/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 56042868736.0000 - val_loss: 56455639040.0000\n",
      "Epoch 3/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 56042762240.0000 - val_loss: 56455467008.0000\n",
      "Epoch 4/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 56042467328.0000 - val_loss: 56455077888.0000\n",
      "Epoch 5/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 56041730048.0000 - val_loss: 56453799936.0000\n",
      "Epoch 6/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 56038518784.0000 - val_loss: 56445992960.0000\n",
      "Epoch 7/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 30173054976.0000 - val_loss: 8195846144.0000\n",
      "Epoch 8/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 5068993536.0000 - val_loss: 3738331904.0000\n",
      "Epoch 9/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4764577792.0000 - val_loss: 3559925760.0000\n",
      "Epoch 10/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4699805184.0000 - val_loss: 3546330368.0000\n",
      "Epoch 11/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4677696000.0000 - val_loss: 3534679808.0000\n",
      "Epoch 12/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4670467584.0000 - val_loss: 3537881856.0000\n",
      "Epoch 13/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4667760640.0000 - val_loss: 3537816576.0000\n",
      "Epoch 14/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4666058752.0000 - val_loss: 3536958720.0000\n",
      "Epoch 15/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4664807936.0000 - val_loss: 3536221184.0000\n",
      "Epoch 16/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4663701504.0000 - val_loss: 3535806720.0000\n",
      "Epoch 17/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4662591488.0000 - val_loss: 3535321088.0000\n",
      "Epoch 18/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4661480960.0000 - val_loss: 3534790912.0000\n",
      "Epoch 19/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4660378624.0000 - val_loss: 3534295808.0000\n",
      "Epoch 20/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4659277824.0000 - val_loss: 3534103552.0000\n",
      "Epoch 21/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4658183168.0000 - val_loss: 3533646336.0000\n",
      "Epoch 22/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4657105408.0000 - val_loss: 3533150976.0000\n",
      "Epoch 23/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4656066048.0000 - val_loss: 3532652544.0000\n",
      "Epoch 24/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4655013888.0000 - val_loss: 3532482304.0000\n",
      "Epoch 25/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4653923328.0000 - val_loss: 3532250112.0000\n",
      "Epoch 26/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4652869120.0000 - val_loss: 3532043264.0000\n",
      "Epoch 27/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4651807232.0000 - val_loss: 3531627776.0000\n",
      "Epoch 28/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4650751488.0000 - val_loss: 3531356160.0000\n",
      "Epoch 29/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4649656832.0000 - val_loss: 3530989824.0000\n",
      "Epoch 30/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4648589312.0000 - val_loss: 3530753280.0000\n",
      "Epoch 31/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4647509504.0000 - val_loss: 3530334464.0000\n",
      "Epoch 32/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4646445568.0000 - val_loss: 3529799424.0000\n",
      "Epoch 33/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4645417472.0000 - val_loss: 3529438464.0000\n",
      "Epoch 34/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 4644398592.0000 - val_loss: 3528969728.0000\n",
      "Epoch 35/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 4643344896.0000 - val_loss: 3528683520.0000\n",
      "Epoch 36/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 4642297856.0000 - val_loss: 3528354816.0000\n",
      "Epoch 37/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 4641260032.0000 - val_loss: 3527976704.0000\n",
      "Epoch 38/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4640222720.0000 - val_loss: 3527725568.0000\n",
      "Epoch 39/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4639167488.0000 - val_loss: 3527289856.0000\n",
      "Epoch 40/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4638120448.0000 - val_loss: 3526905600.0000\n",
      "Epoch 41/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4637124608.0000 - val_loss: 3526522368.0000\n",
      "Epoch 42/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 4636102144.0000 - val_loss: 3526314240.0000\n",
      "Epoch 43/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 4635039232.0000 - val_loss: 3526074624.0000\n",
      "Epoch 44/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 4634033664.0000 - val_loss: 3525988864.0000\n",
      "Epoch 45/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4632978944.0000 - val_loss: 3525512960.0000\n",
      "Epoch 46/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4631960576.0000 - val_loss: 3525240832.0000\n",
      "Epoch 47/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 4630926336.0000 - val_loss: 3525000960.0000\n",
      "Epoch 48/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 4629919744.0000 - val_loss: 3524697088.0000\n",
      "Epoch 49/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 4628861952.0000 - val_loss: 3524373248.0000\n",
      "Epoch 50/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4627847168.0000 - val_loss: 3523997696.0000\n",
      "Model: \"sequential_60\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_594 (Dense)           (None, 64)                896       \n",
      "                                                                 \n",
      " dense_595 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_596 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_597 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_598 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_599 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_600 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_601 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_602 (Dense)           (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 30,081\n",
      "Trainable params: 30,081\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "549/549 [==============================] - 2s 2ms/step - loss: 51348471808.0000 - val_loss: 36740280320.0000\n",
      "Epoch 2/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 13401932800.0000 - val_loss: 6654621696.0000\n",
      "Epoch 3/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 7155329536.0000 - val_loss: 5151243776.0000\n",
      "Epoch 4/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 6443083776.0000 - val_loss: 4609773056.0000\n",
      "Epoch 5/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 6088878080.0000 - val_loss: 4364637696.0000\n",
      "Epoch 6/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 5868245504.0000 - val_loss: 4191639808.0000\n",
      "Epoch 7/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 5713014784.0000 - val_loss: 4081099008.0000\n",
      "Epoch 8/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 5595073536.0000 - val_loss: 3999419904.0000\n",
      "Epoch 9/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 5500678656.0000 - val_loss: 3937712128.0000\n",
      "Epoch 10/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 5423351808.0000 - val_loss: 3895173632.0000\n",
      "Epoch 11/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 5359632896.0000 - val_loss: 3853718784.0000\n",
      "Epoch 12/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 5304550400.0000 - val_loss: 3821890816.0000\n",
      "Epoch 13/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 5257980416.0000 - val_loss: 3795334144.0000\n",
      "Epoch 14/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 5216666624.0000 - val_loss: 3774359040.0000\n",
      "Epoch 15/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 5180517376.0000 - val_loss: 3756622336.0000\n",
      "Epoch 16/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 5148404224.0000 - val_loss: 3740178432.0000\n",
      "Epoch 17/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 5119680000.0000 - val_loss: 3726057472.0000\n",
      "Epoch 18/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 5093942272.0000 - val_loss: 3714423040.0000\n",
      "Epoch 19/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 5070605824.0000 - val_loss: 3703590400.0000\n",
      "Epoch 20/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 5048811520.0000 - val_loss: 3696139008.0000\n",
      "Epoch 21/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 5029545472.0000 - val_loss: 3685876736.0000\n",
      "Epoch 22/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 5011602432.0000 - val_loss: 3678197760.0000\n",
      "Epoch 23/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4995043328.0000 - val_loss: 3671146752.0000\n",
      "Epoch 24/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4979206656.0000 - val_loss: 3667926528.0000\n",
      "Epoch 25/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4964995584.0000 - val_loss: 3660316416.0000\n",
      "Epoch 26/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4951480320.0000 - val_loss: 3655584512.0000\n",
      "Epoch 27/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4938940416.0000 - val_loss: 3651106816.0000\n",
      "Epoch 28/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4927443456.0000 - val_loss: 3648182784.0000\n",
      "Epoch 29/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4916246528.0000 - val_loss: 3643881472.0000\n",
      "Epoch 30/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4905742848.0000 - val_loss: 3641836288.0000\n",
      "Epoch 31/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4896202752.0000 - val_loss: 3637081600.0000\n",
      "Epoch 32/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4886725120.0000 - val_loss: 3633492992.0000\n",
      "Epoch 33/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4877921792.0000 - val_loss: 3630883072.0000\n",
      "Epoch 34/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4869442560.0000 - val_loss: 3628110592.0000\n",
      "Epoch 35/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4861084160.0000 - val_loss: 3625440768.0000\n",
      "Epoch 36/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4853543936.0000 - val_loss: 3623358464.0000\n",
      "Epoch 37/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4846104064.0000 - val_loss: 3621468928.0000\n",
      "Epoch 38/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4838625792.0000 - val_loss: 3620045056.0000\n",
      "Epoch 39/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4831672832.0000 - val_loss: 3617462272.0000\n",
      "Epoch 40/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4825268224.0000 - val_loss: 3615532288.0000\n",
      "Epoch 41/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4818902528.0000 - val_loss: 3614053376.0000\n",
      "Epoch 42/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4813345280.0000 - val_loss: 3612808448.0000\n",
      "Epoch 43/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4806385152.0000 - val_loss: 3611320576.0000\n",
      "Epoch 44/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4801458176.0000 - val_loss: 3611365120.0000\n",
      "Epoch 45/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4795867136.0000 - val_loss: 3608678656.0000\n",
      "Epoch 46/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4790880256.0000 - val_loss: 3607914240.0000\n",
      "Epoch 47/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4785566208.0000 - val_loss: 3607222272.0000\n",
      "Epoch 48/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4781088256.0000 - val_loss: 3605349120.0000\n",
      "Epoch 49/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4776104960.0000 - val_loss: 3603517184.0000\n",
      "Epoch 50/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4771526144.0000 - val_loss: 3602732288.0000\n",
      "Model: \"sequential_61\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_603 (Dense)           (None, 64)                896       \n",
      "                                                                 \n",
      " dense_604 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_605 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_606 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_607 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_608 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_609 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_610 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_611 (Dense)           (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 30,081\n",
      "Trainable params: 30,081\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 22841044992.0000 - val_loss: 3962066432.0000\n",
      "Epoch 2/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 5202470400.0000 - val_loss: 3682723328.0000\n",
      "Epoch 3/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4804115968.0000 - val_loss: 3524127232.0000\n",
      "Epoch 4/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4596040704.0000 - val_loss: 3532481792.0000\n",
      "Epoch 5/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4460059648.0000 - val_loss: 3413856512.0000\n",
      "Epoch 6/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4335972864.0000 - val_loss: 3358727168.0000\n",
      "Epoch 7/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4235936768.0000 - val_loss: 3382706944.0000\n",
      "Epoch 8/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4125127680.0000 - val_loss: 3354711296.0000\n",
      "Epoch 9/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4052013312.0000 - val_loss: 3298576640.0000\n",
      "Epoch 10/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3972641024.0000 - val_loss: 3355416832.0000\n",
      "Epoch 11/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3909801216.0000 - val_loss: 3371225344.0000\n",
      "Epoch 12/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3856502528.0000 - val_loss: 3597105152.0000\n",
      "Epoch 13/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3798939904.0000 - val_loss: 3436463616.0000\n",
      "Epoch 14/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3742665984.0000 - val_loss: 3415991808.0000\n",
      "Epoch 15/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3706686208.0000 - val_loss: 3633378560.0000\n",
      "Epoch 16/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3671726848.0000 - val_loss: 3451132160.0000\n",
      "Epoch 17/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3636051968.0000 - val_loss: 3405520896.0000\n",
      "Epoch 18/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3617817856.0000 - val_loss: 3457369088.0000\n",
      "Epoch 19/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3608111872.0000 - val_loss: 3544063488.0000\n",
      "Epoch 20/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3581735936.0000 - val_loss: 3493009920.0000\n",
      "Epoch 21/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3580194560.0000 - val_loss: 3442184448.0000\n",
      "Epoch 22/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3560038656.0000 - val_loss: 3390999040.0000\n",
      "Epoch 23/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3545855232.0000 - val_loss: 3402102784.0000\n",
      "Epoch 24/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3522933248.0000 - val_loss: 3468940288.0000\n",
      "Epoch 25/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3519924736.0000 - val_loss: 3436029184.0000\n",
      "Epoch 26/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3495542528.0000 - val_loss: 3606412544.0000\n",
      "Epoch 27/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3490671616.0000 - val_loss: 3560010240.0000\n",
      "Epoch 28/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3471481344.0000 - val_loss: 3414551296.0000\n",
      "Epoch 29/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3462068480.0000 - val_loss: 3449183488.0000\n",
      "Epoch 30/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3439513856.0000 - val_loss: 3508595200.0000\n",
      "Epoch 31/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3423584000.0000 - val_loss: 3556885248.0000\n",
      "Epoch 32/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3415113472.0000 - val_loss: 3437668608.0000\n",
      "Epoch 33/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3387644160.0000 - val_loss: 3469625600.0000\n",
      "Epoch 34/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3375696128.0000 - val_loss: 3531998208.0000\n",
      "Epoch 35/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3361850112.0000 - val_loss: 3334164736.0000\n",
      "Epoch 36/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3344826624.0000 - val_loss: 3388896256.0000\n",
      "Epoch 37/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3324901376.0000 - val_loss: 3499004672.0000\n",
      "Epoch 38/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3303351808.0000 - val_loss: 3623230976.0000\n",
      "Epoch 39/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3297822464.0000 - val_loss: 3484940288.0000\n",
      "Epoch 40/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3294297600.0000 - val_loss: 3480820224.0000\n",
      "Epoch 41/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 3263040512.0000 - val_loss: 3687623680.0000\n",
      "Epoch 42/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3248907008.0000 - val_loss: 3592409600.0000\n",
      "Epoch 43/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3224564480.0000 - val_loss: 3432748032.0000\n",
      "Epoch 44/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3229576704.0000 - val_loss: 3382263808.0000\n",
      "Epoch 45/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3206331904.0000 - val_loss: 3386947584.0000\n",
      "Epoch 46/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3190125568.0000 - val_loss: 3409772800.0000\n",
      "Epoch 47/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3180786432.0000 - val_loss: 3485612288.0000\n",
      "Epoch 48/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3171032576.0000 - val_loss: 3639683584.0000\n",
      "Epoch 49/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3144804096.0000 - val_loss: 3573412096.0000\n",
      "Epoch 50/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3141299456.0000 - val_loss: 3401452544.0000\n",
      "Model: \"sequential_62\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_612 (Dense)           (None, 64)                896       \n",
      "                                                                 \n",
      " dense_613 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_614 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_615 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_616 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_617 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_618 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_619 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_620 (Dense)           (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 30,081\n",
      "Trainable params: 30,081\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "549/549 [==============================] - 3s 3ms/step - loss: 10041359360.0000 - val_loss: 3691707904.0000\n",
      "Epoch 2/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4450145792.0000 - val_loss: 3617383424.0000\n",
      "Epoch 3/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 4138824704.0000 - val_loss: 3280528384.0000\n",
      "Epoch 4/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 3925254912.0000 - val_loss: 3639842560.0000\n",
      "Epoch 5/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 3787634944.0000 - val_loss: 3422790144.0000\n",
      "Epoch 6/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 3695553024.0000 - val_loss: 3974474240.0000\n",
      "Epoch 7/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 3637082624.0000 - val_loss: 3878935040.0000\n",
      "Epoch 8/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 3595328000.0000 - val_loss: 3292664576.0000\n",
      "Epoch 9/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 3548980480.0000 - val_loss: 3422133504.0000\n",
      "Epoch 10/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 3512150528.0000 - val_loss: 3633873664.0000\n",
      "Epoch 11/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 3473527040.0000 - val_loss: 3534332160.0000\n",
      "Epoch 12/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 3422791936.0000 - val_loss: 3724002048.0000\n",
      "Epoch 13/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 3374277120.0000 - val_loss: 3406997760.0000\n",
      "Epoch 14/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 3339795456.0000 - val_loss: 3328639232.0000\n",
      "Epoch 15/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 3292215296.0000 - val_loss: 3263934976.0000\n",
      "Epoch 16/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 3256436224.0000 - val_loss: 3250393856.0000\n",
      "Epoch 17/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 3213624320.0000 - val_loss: 3200268544.0000\n",
      "Epoch 18/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 3172678912.0000 - val_loss: 3272309760.0000\n",
      "Epoch 19/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 3140705536.0000 - val_loss: 3304286976.0000\n",
      "Epoch 20/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 3119888896.0000 - val_loss: 3094903552.0000\n",
      "Epoch 21/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 3089758208.0000 - val_loss: 3211306240.0000\n",
      "Epoch 22/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 3062921216.0000 - val_loss: 3074884352.0000\n",
      "Epoch 23/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 3021790976.0000 - val_loss: 3245044224.0000\n",
      "Epoch 24/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 3001747200.0000 - val_loss: 3178798848.0000\n",
      "Epoch 25/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 2966049024.0000 - val_loss: 2978854656.0000\n",
      "Epoch 26/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 2941959168.0000 - val_loss: 3219295744.0000\n",
      "Epoch 27/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 2917092608.0000 - val_loss: 3170281216.0000\n",
      "Epoch 28/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 2908485120.0000 - val_loss: 3016108544.0000\n",
      "Epoch 29/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 2876748800.0000 - val_loss: 3047530752.0000\n",
      "Epoch 30/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 2863073792.0000 - val_loss: 3008665600.0000\n",
      "Epoch 31/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 2834122752.0000 - val_loss: 2899318272.0000\n",
      "Epoch 32/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 2806192896.0000 - val_loss: 3006611712.0000\n",
      "Epoch 33/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 2813660160.0000 - val_loss: 2994123776.0000\n",
      "Epoch 34/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 2794984192.0000 - val_loss: 3240228096.0000\n",
      "Epoch 35/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 2774392064.0000 - val_loss: 2984144384.0000\n",
      "Epoch 36/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 2776795648.0000 - val_loss: 2994574848.0000\n",
      "Epoch 37/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 2746198272.0000 - val_loss: 3207352576.0000\n",
      "Epoch 38/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 2752662784.0000 - val_loss: 3183761920.0000\n",
      "Epoch 39/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 2727149568.0000 - val_loss: 3058943488.0000\n",
      "Epoch 40/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 2726411520.0000 - val_loss: 3069351680.0000\n",
      "Epoch 41/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 2701861888.0000 - val_loss: 2978851328.0000\n",
      "Epoch 42/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 2695358208.0000 - val_loss: 3272517632.0000\n",
      "Epoch 43/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 2691995136.0000 - val_loss: 2913911040.0000\n",
      "Epoch 44/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 2683379200.0000 - val_loss: 3215192320.0000\n",
      "Epoch 45/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 2659263744.0000 - val_loss: 2962663168.0000\n",
      "Epoch 46/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 2654133760.0000 - val_loss: 3150053632.0000\n",
      "Epoch 47/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 2626907136.0000 - val_loss: 3228353792.0000\n",
      "Epoch 48/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 2626068224.0000 - val_loss: 3171187456.0000\n",
      "Epoch 49/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 2608709632.0000 - val_loss: 3080279040.0000\n",
      "Epoch 50/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 2609903360.0000 - val_loss: 3085649920.0000\n",
      "Model: \"sequential_63\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_621 (Dense)           (None, 64)                896       \n",
      "                                                                 \n",
      " dense_622 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_623 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_624 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_625 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_626 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_627 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_628 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_629 (Dense)           (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 30,081\n",
      "Trainable params: 30,081\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 51378651136.0000 - val_loss: 36849790976.0000\n",
      "Epoch 2/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 13444040704.0000 - val_loss: 6658885120.0000\n",
      "Epoch 3/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 7154435584.0000 - val_loss: 5151134208.0000\n",
      "Epoch 4/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 6440367104.0000 - val_loss: 4609375232.0000\n",
      "Epoch 5/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 6085601792.0000 - val_loss: 4364193792.0000\n",
      "Epoch 6/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 5864754176.0000 - val_loss: 4191311360.0000\n",
      "Epoch 7/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 5709409792.0000 - val_loss: 4080893184.0000\n",
      "Epoch 8/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 5591395328.0000 - val_loss: 3999320320.0000\n",
      "Epoch 9/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 5496936448.0000 - val_loss: 3937781248.0000\n",
      "Epoch 10/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 5419553792.0000 - val_loss: 3895169024.0000\n",
      "Epoch 11/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 5355785216.0000 - val_loss: 3853802240.0000\n",
      "Epoch 12/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 5300651520.0000 - val_loss: 3822024704.0000\n",
      "Epoch 13/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 5254045696.0000 - val_loss: 3795535104.0000\n",
      "Epoch 14/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 5212684288.0000 - val_loss: 3774608384.0000\n",
      "Epoch 15/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 5176498176.0000 - val_loss: 3756890880.0000\n",
      "Epoch 16/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 5144347136.0000 - val_loss: 3740460032.0000\n",
      "Epoch 17/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 5115582464.0000 - val_loss: 3726342144.0000\n",
      "Epoch 18/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 5089804800.0000 - val_loss: 3714733312.0000\n",
      "Epoch 19/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 5066424320.0000 - val_loss: 3703901696.0000\n",
      "Epoch 20/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 5044593664.0000 - val_loss: 3696500736.0000\n",
      "Epoch 21/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 5025293824.0000 - val_loss: 3686255360.0000\n",
      "Epoch 22/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 5007325184.0000 - val_loss: 3678570496.0000\n",
      "Epoch 23/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4990725632.0000 - val_loss: 3671497728.0000\n",
      "Epoch 24/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 4974854144.0000 - val_loss: 3668308736.0000\n",
      "Epoch 25/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 4960606720.0000 - val_loss: 3660679168.0000\n",
      "Epoch 26/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4947054592.0000 - val_loss: 3655957760.0000\n",
      "Epoch 27/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4934472192.0000 - val_loss: 3651649280.0000\n",
      "Epoch 28/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4922973184.0000 - val_loss: 3648701696.0000\n",
      "Epoch 29/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4911750656.0000 - val_loss: 3644527104.0000\n",
      "Epoch 30/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 4901218304.0000 - val_loss: 3642456064.0000\n",
      "Epoch 31/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4891650560.0000 - val_loss: 3637656576.0000\n",
      "Epoch 32/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4882082304.0000 - val_loss: 3633848576.0000\n",
      "Epoch 33/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4873169920.0000 - val_loss: 3631252480.0000\n",
      "Epoch 34/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4864668160.0000 - val_loss: 3628526080.0000\n",
      "Epoch 35/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4856259584.0000 - val_loss: 3625863680.0000\n",
      "Epoch 36/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4848679936.0000 - val_loss: 3623779328.0000\n",
      "Epoch 37/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4841211904.0000 - val_loss: 3621893632.0000\n",
      "Epoch 38/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4833711616.0000 - val_loss: 3620480256.0000\n",
      "Epoch 39/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4826735616.0000 - val_loss: 3617875456.0000\n",
      "Epoch 40/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 4820310016.0000 - val_loss: 3615941120.0000\n",
      "Epoch 41/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4813919744.0000 - val_loss: 3614463232.0000\n",
      "Epoch 42/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 4808349184.0000 - val_loss: 3613232128.0000\n",
      "Epoch 43/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4801364992.0000 - val_loss: 3611738368.0000\n",
      "Epoch 44/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 4796420608.0000 - val_loss: 3611804416.0000\n",
      "Epoch 45/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4790816256.0000 - val_loss: 3609107456.0000\n",
      "Epoch 46/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4785808384.0000 - val_loss: 3608353536.0000\n",
      "Epoch 47/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4780493824.0000 - val_loss: 3607694080.0000\n",
      "Epoch 48/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4775922688.0000 - val_loss: 3605631488.0000\n",
      "Epoch 49/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 4770804224.0000 - val_loss: 3603849472.0000\n",
      "Epoch 50/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 4766205440.0000 - val_loss: 3603090688.0000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>configuracion</th>\n",
       "      <th>error_test</th>\n",
       "      <th>error_train</th>\n",
       "      <th>segundos_entrenamiento</th>\n",
       "      <th>epoca</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>ANN_capas=[64; 64; 64; 64; 64; 64; 64; 64; 1]_activaciones=['elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1</td>\n",
       "      <td>53376.532371</td>\n",
       "      <td>51400.826141</td>\n",
       "      <td>67.506442</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ANN_capas=[64; 64; 64; 64; 64; 64; 64; 64; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1</td>\n",
       "      <td>54231.385747</td>\n",
       "      <td>51958.811996</td>\n",
       "      <td>63.929625</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>ANN_capas=[32; 50; 64; 80; 80; 64; 50; 32; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1</td>\n",
       "      <td>54464.082256</td>\n",
       "      <td>52447.311771</td>\n",
       "      <td>65.858972</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>ANN_capas=[64; 64; 64; 64; 64; 64; 64; 64; 1]_activaciones=['selu'; 'selu'; 'selu'; 'selu'; 'selu'; 'selu'; 'selu'; 'selu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1</td>\n",
       "      <td>55036.204520</td>\n",
       "      <td>50085.477017</td>\n",
       "      <td>63.393345</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>ANN_capas=[50; 50; 50; 50; 50; 50; 50; 50; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1</td>\n",
       "      <td>55458.095748</td>\n",
       "      <td>52839.030423</td>\n",
       "      <td>58.103801</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>ANN_capas=[80; 80; 64; 64; 50; 50; 32; 32; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1</td>\n",
       "      <td>55472.535331</td>\n",
       "      <td>51841.128383</td>\n",
       "      <td>63.146074</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>ANN_capas=[64; 64; 64; 64; 64; 64; 64; 64; 1]_activaciones=['elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=nadam_paciencia=-1</td>\n",
       "      <td>55548.626626</td>\n",
       "      <td>51087.213273</td>\n",
       "      <td>73.071667</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>ANN_capas=[64; 64; 64; 64; 64; 64; 64; 64; 1]_activaciones=['elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=rmsprop_paciencia=-1</td>\n",
       "      <td>55743.102174</td>\n",
       "      <td>51816.125367</td>\n",
       "      <td>60.515572</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ANN_capas=[8; 8; 8; 8; 8; 8; 8; 8; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=10</td>\n",
       "      <td>56168.076912</td>\n",
       "      <td>65505.463589</td>\n",
       "      <td>52.175863</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ANN_capas=[80; 80; 80; 80; 80; 80; 80; 80; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1</td>\n",
       "      <td>56252.450400</td>\n",
       "      <td>50596.551345</td>\n",
       "      <td>68.058731</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ANN_capas=[10; 10; 10; 10; 10; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=10</td>\n",
       "      <td>56579.769105</td>\n",
       "      <td>65503.054219</td>\n",
       "      <td>46.783819</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>ANN_capas=[50; 50; 50; 64; 64; 64; 80; 80; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1</td>\n",
       "      <td>56641.712968</td>\n",
       "      <td>50598.160283</td>\n",
       "      <td>71.024920</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>ANN_capas=[50; 50; 64; 80; 80; 64; 50; 50; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1</td>\n",
       "      <td>57028.581185</td>\n",
       "      <td>51121.124244</td>\n",
       "      <td>66.832304</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ANN_capas=[64; 64; 64; 64; 64; 64; 64; 64; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=10</td>\n",
       "      <td>57062.799371</td>\n",
       "      <td>57590.639239</td>\n",
       "      <td>17.061472</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>ANN_capas=[64; 64; 64; 64; 64; 64; 64; 64; 1]_activaciones=['selu'; 'selu'; 'selu'; 'selu'; 'selu'; 'selu'; 'selu'; 'selu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=10</td>\n",
       "      <td>57184.219362</td>\n",
       "      <td>50687.800505</td>\n",
       "      <td>54.517353</td>\n",
       "      <td>41.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                   configuracion  \\\n",
       "33          ANN_capas=[64; 64; 64; 64; 64; 64; 64; 64; 1]_activaciones=['elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1   \n",
       "14  ANN_capas=[64; 64; 64; 64; 64; 64; 64; 64; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1   \n",
       "24  ANN_capas=[32; 50; 64; 80; 80; 64; 50; 32; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1   \n",
       "32  ANN_capas=[64; 64; 64; 64; 64; 64; 64; 64; 1]_activaciones=['selu'; 'selu'; 'selu'; 'selu'; 'selu'; 'selu'; 'selu'; 'selu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1   \n",
       "18  ANN_capas=[50; 50; 50; 50; 50; 50; 50; 50; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1   \n",
       "22  ANN_capas=[80; 80; 64; 64; 50; 50; 32; 32; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1   \n",
       "38         ANN_capas=[64; 64; 64; 64; 64; 64; 64; 64; 1]_activaciones=['elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=nadam_paciencia=-1   \n",
       "34       ANN_capas=[64; 64; 64; 64; 64; 64; 64; 64; 1]_activaciones=['elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=rmsprop_paciencia=-1   \n",
       "7           ANN_capas=[8; 8; 8; 8; 8; 8; 8; 8; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=10   \n",
       "15  ANN_capas=[80; 80; 80; 80; 80; 80; 80; 80; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1   \n",
       "3                                       ANN_capas=[10; 10; 10; 10; 10; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=10   \n",
       "20  ANN_capas=[50; 50; 50; 64; 64; 64; 80; 80; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1   \n",
       "21  ANN_capas=[50; 50; 64; 80; 80; 64; 50; 50; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1   \n",
       "10  ANN_capas=[64; 64; 64; 64; 64; 64; 64; 64; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=10   \n",
       "28  ANN_capas=[64; 64; 64; 64; 64; 64; 64; 64; 1]_activaciones=['selu'; 'selu'; 'selu'; 'selu'; 'selu'; 'selu'; 'selu'; 'selu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=10   \n",
       "\n",
       "      error_test   error_train  segundos_entrenamiento  epoca  \n",
       "33  53376.532371  51400.826141               67.506442   50.0  \n",
       "14  54231.385747  51958.811996               63.929625   50.0  \n",
       "24  54464.082256  52447.311771               65.858972   50.0  \n",
       "32  55036.204520  50085.477017               63.393345   50.0  \n",
       "18  55458.095748  52839.030423               58.103801   50.0  \n",
       "22  55472.535331  51841.128383               63.146074   50.0  \n",
       "38  55548.626626  51087.213273               73.071667   50.0  \n",
       "34  55743.102174  51816.125367               60.515572   50.0  \n",
       "7   56168.076912  65505.463589               52.175863   50.0  \n",
       "15  56252.450400  50596.551345               68.058731   50.0  \n",
       "3   56579.769105  65503.054219               46.783819   50.0  \n",
       "20  56641.712968  50598.160283               71.024920   50.0  \n",
       "21  57028.581185  51121.124244               66.832304   50.0  \n",
       "10  57062.799371  57590.639239               17.061472   13.0  \n",
       "28  57184.219362  50687.800505               54.517353   41.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Experimentos para número de capas\n",
    "experimentoOptimizadores = ['rmsprop', 'adam', 'adadelta', 'adagrad', 'adamax', 'nadam', 'ftrl']\n",
    "listaCapas = [64, 64, 64, 64, 64, 64, 64, 64, 1]\n",
    "listaActivaciones = ['elu', 'elu', 'elu', 'elu', 'elu', 'elu', 'elu', 'elu', 'linear']\n",
    "for experimento in experimentoOptimizadores:\n",
    "    redAnn, configuracion = construirANN(caracteristicasEstandarizadas.shape[1], listaCapas, listaActivaciones)\n",
    "    redAnn.summary()\n",
    "    entrenarANN(redAnn, configuracion, caracteristicasEstandarizadas, etiquetas, 'mean_squared_error', \\\n",
    "                tamanioBatch=32, epocas=50, optimizador=experimento, paciencia = -1)\n",
    "\n",
    "mostrarResultadosBitacora('ANN')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "former-closer",
   "metadata": {},
   "source": [
    "De los experimentos anteriores se puede observar que el mejor optimizador para este modelo y conjunto de datos parece ser __adam__ y otras opciones aceptables podrían ser: __nadam__ y __rmsprop__.\n",
    "\n",
    "### Uso de DropOut y BatchNormalization\n",
    "Se aplicarán diversos porcentajes de DropOut en la red, así como la activación de BatchNormalization en las capas ocultas y se observará cómo se ve afectado el entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "automotive-growing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_65\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_636 (Dense)           (None, 64)                896       \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_637 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_638 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_639 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_640 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_641 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_642 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dropout_11 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_643 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dropout_12 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_644 (Dense)           (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 30,081\n",
      "Trainable params: 30,081\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "549/549 [==============================] - 3s 3ms/step - loss: 13866315776.0000 - val_loss: 3598697216.0000\n",
      "Epoch 2/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 5985163776.0000 - val_loss: 3548273408.0000\n",
      "Epoch 3/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 5680054272.0000 - val_loss: 3277081088.0000\n",
      "Epoch 4/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 5359664640.0000 - val_loss: 3430812416.0000\n",
      "Epoch 5/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 5189562368.0000 - val_loss: 3207537664.0000\n",
      "Epoch 6/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 4972484608.0000 - val_loss: 3399760896.0000\n",
      "Epoch 7/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 4968160768.0000 - val_loss: 3187587328.0000\n",
      "Epoch 8/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 4814064128.0000 - val_loss: 3225461760.0000\n",
      "Epoch 9/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 4743136768.0000 - val_loss: 3147112448.0000\n",
      "Epoch 10/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 4709399552.0000 - val_loss: 3264514048.0000\n",
      "Epoch 11/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 4645412352.0000 - val_loss: 3174764032.0000\n",
      "Epoch 12/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 4596303872.0000 - val_loss: 3535108608.0000\n",
      "Epoch 13/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 4550982656.0000 - val_loss: 3283822592.0000\n",
      "Epoch 14/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 4566759424.0000 - val_loss: 3498327040.0000\n",
      "Epoch 15/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 4476194304.0000 - val_loss: 3439671040.0000\n",
      "Epoch 16/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 4429152768.0000 - val_loss: 3418690048.0000\n",
      "Epoch 17/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 4397594112.0000 - val_loss: 3233667840.0000\n",
      "Epoch 18/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 4446788096.0000 - val_loss: 3220373248.0000\n",
      "Epoch 19/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 4353639424.0000 - val_loss: 3267404544.0000\n",
      "Epoch 20/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 4316556288.0000 - val_loss: 3184926208.0000\n",
      "Epoch 21/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 4280883456.0000 - val_loss: 3386475008.0000\n",
      "Epoch 22/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 4223600640.0000 - val_loss: 3349468160.0000\n",
      "Epoch 23/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 4203552768.0000 - val_loss: 3211429376.0000\n",
      "Epoch 24/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 4264657920.0000 - val_loss: 3238332672.0000\n",
      "Epoch 25/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 4104735744.0000 - val_loss: 3169861632.0000\n",
      "Epoch 26/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 4173066752.0000 - val_loss: 3281616896.0000\n",
      "Epoch 27/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 4179696128.0000 - val_loss: 3392594176.0000\n",
      "Epoch 28/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 4165076224.0000 - val_loss: 3197718272.0000\n",
      "Epoch 29/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 4121324800.0000 - val_loss: 3244433920.0000\n",
      "Epoch 30/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 4083316480.0000 - val_loss: 3322956032.0000\n",
      "Epoch 31/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 4043639552.0000 - val_loss: 3170209792.0000\n",
      "Epoch 32/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 4115595776.0000 - val_loss: 3317459200.0000\n",
      "Epoch 33/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 4038160384.0000 - val_loss: 3167447040.0000\n",
      "Epoch 34/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 4033796864.0000 - val_loss: 3303409664.0000\n",
      "Epoch 35/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 3979901440.0000 - val_loss: 3059019520.0000\n",
      "Epoch 36/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 3958365440.0000 - val_loss: 3043858176.0000\n",
      "Epoch 37/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 4004263936.0000 - val_loss: 3126676224.0000\n",
      "Epoch 38/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 3882783232.0000 - val_loss: 3339223040.0000\n",
      "Epoch 39/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 3881786368.0000 - val_loss: 3112808704.0000\n",
      "Epoch 40/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 3915778304.0000 - val_loss: 3109326848.0000\n",
      "Epoch 41/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 3878332928.0000 - val_loss: 3147437568.0000\n",
      "Epoch 42/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 3802725632.0000 - val_loss: 3107936256.0000\n",
      "Epoch 43/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 3861350400.0000 - val_loss: 3048447744.0000\n",
      "Epoch 44/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 3825131776.0000 - val_loss: 2991141632.0000\n",
      "Epoch 45/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 3841760768.0000 - val_loss: 3004108544.0000\n",
      "Epoch 46/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 3864610816.0000 - val_loss: 3151118336.0000\n",
      "Epoch 47/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 3824758528.0000 - val_loss: 3100405760.0000\n",
      "Epoch 48/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 3798004992.0000 - val_loss: 2995952640.0000\n",
      "Epoch 49/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 3802586368.0000 - val_loss: 3054851840.0000\n",
      "Epoch 50/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 3701260544.0000 - val_loss: 3154455552.0000\n",
      "Model: \"sequential_66\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_645 (Dense)           (None, 64)                896       \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 64)               256       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout_13 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_646 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 64)               256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_14 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_647 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 64)               256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_15 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_648 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 64)               256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_16 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_649 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 64)               256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_17 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_650 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 64)               256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_18 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_651 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (None, 64)               256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_19 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_652 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " batch_normalization_7 (Batc  (None, 64)               256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_20 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_653 (Dense)           (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 32,129\n",
      "Trainable params: 31,105\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "549/549 [==============================] - 6s 5ms/step - loss: 56029036544.0000 - val_loss: 56420188160.0000\n",
      "Epoch 2/50\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 55969685504.0000 - val_loss: 56336248832.0000\n",
      "Epoch 3/50\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 55862198272.0000 - val_loss: 56196009984.0000\n",
      "Epoch 4/50\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 55712493568.0000 - val_loss: 56054136832.0000\n",
      "Epoch 5/50\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 55524761600.0000 - val_loss: 55788470272.0000\n",
      "Epoch 6/50\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 55308226560.0000 - val_loss: 55570296832.0000\n",
      "Epoch 7/50\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 55060672512.0000 - val_loss: 55324659712.0000\n",
      "Epoch 8/50\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 54785327104.0000 - val_loss: 54995632128.0000\n",
      "Epoch 9/50\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 54484893696.0000 - val_loss: 54706737152.0000\n",
      "Epoch 10/50\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 54156922880.0000 - val_loss: 54318915584.0000\n",
      "Epoch 11/50\n",
      "549/549 [==============================] - 3s 6ms/step - loss: 53805633536.0000 - val_loss: 53987454976.0000\n",
      "Epoch 12/50\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 53431377920.0000 - val_loss: 53778432000.0000\n",
      "Epoch 13/50\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 53038354432.0000 - val_loss: 53168152576.0000\n",
      "Epoch 14/50\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 52617396224.0000 - val_loss: 52890329088.0000\n",
      "Epoch 15/50\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 52179623936.0000 - val_loss: 52658266112.0000\n",
      "Epoch 16/50\n",
      "549/549 [==============================] - 3s 6ms/step - loss: 51711205376.0000 - val_loss: 51948969984.0000\n",
      "Epoch 17/50\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 51241934848.0000 - val_loss: 51385757696.0000\n",
      "Epoch 18/50\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 50736758784.0000 - val_loss: 50704863232.0000\n",
      "Epoch 19/50\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 50207399936.0000 - val_loss: 50425663488.0000\n",
      "Epoch 20/50\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 49667796992.0000 - val_loss: 49827815424.0000\n",
      "Epoch 21/50\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 49110065152.0000 - val_loss: 49534046208.0000\n",
      "Epoch 22/50\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 48540852224.0000 - val_loss: 48508866560.0000\n",
      "Epoch 23/50\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 47954903040.0000 - val_loss: 47586398208.0000\n",
      "Epoch 24/50\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 47335235584.0000 - val_loss: 47969955840.0000\n",
      "Epoch 25/50\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 46708027392.0000 - val_loss: 46742880256.0000\n",
      "Epoch 26/50\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 46073462784.0000 - val_loss: 46250790912.0000\n",
      "Epoch 27/50\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 45418037248.0000 - val_loss: 46059089920.0000\n",
      "Epoch 28/50\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 44746600448.0000 - val_loss: 44556759040.0000\n",
      "Epoch 29/50\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 44060717056.0000 - val_loss: 43619491840.0000\n",
      "Epoch 30/50\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 43359817728.0000 - val_loss: 42877485056.0000\n",
      "Epoch 31/50\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 42657873920.0000 - val_loss: 42662166528.0000\n",
      "Epoch 32/50\n",
      "549/549 [==============================] - 3s 6ms/step - loss: 41932300288.0000 - val_loss: 42482290688.0000\n",
      "Epoch 33/50\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 41207656448.0000 - val_loss: 41536323584.0000\n",
      "Epoch 34/50\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 40470192128.0000 - val_loss: 40446308352.0000\n",
      "Epoch 35/50\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 39707074560.0000 - val_loss: 38902620160.0000\n",
      "Epoch 36/50\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 38997856256.0000 - val_loss: 39320248320.0000\n",
      "Epoch 37/50\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 38205001728.0000 - val_loss: 38026158080.0000\n",
      "Epoch 38/50\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 37409128448.0000 - val_loss: 37436968960.0000\n",
      "Epoch 39/50\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 36633104384.0000 - val_loss: 36654092288.0000\n",
      "Epoch 40/50\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 35887779840.0000 - val_loss: 36578881536.0000\n",
      "Epoch 41/50\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 35106201600.0000 - val_loss: 35238633472.0000\n",
      "Epoch 42/50\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 34282465280.0000 - val_loss: 35007746048.0000\n",
      "Epoch 43/50\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 33512898560.0000 - val_loss: 33154174976.0000\n",
      "Epoch 44/50\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 32710242304.0000 - val_loss: 33036480512.0000\n",
      "Epoch 45/50\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 31939229696.0000 - val_loss: 31562840064.0000\n",
      "Epoch 46/50\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 31099119616.0000 - val_loss: 32009910272.0000\n",
      "Epoch 47/50\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 30279737344.0000 - val_loss: 29573799936.0000\n",
      "Epoch 48/50\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 29512755200.0000 - val_loss: 29659373568.0000\n",
      "Epoch 49/50\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 28727603200.0000 - val_loss: 29971017728.0000\n",
      "Epoch 50/50\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 27937978368.0000 - val_loss: 28716763136.0000\n",
      "Model: \"sequential_67\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_654 (Dense)           (None, 64)                896       \n",
      "                                                                 \n",
      " dropout_21 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_655 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dropout_22 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_656 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dropout_23 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_657 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dropout_24 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_658 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dropout_25 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_659 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dropout_26 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_660 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dropout_27 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_661 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dropout_28 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_662 (Dense)           (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 30,081\n",
      "Trainable params: 30,081\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "549/549 [==============================] - 3s 3ms/step - loss: 15294058496.0000 - val_loss: 3757400320.0000\n",
      "Epoch 2/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 7522126848.0000 - val_loss: 4075792896.0000\n",
      "Epoch 3/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 7054252544.0000 - val_loss: 3646058496.0000\n",
      "Epoch 4/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 6632094720.0000 - val_loss: 3374814464.0000\n",
      "Epoch 5/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 6492118016.0000 - val_loss: 3306654208.0000\n",
      "Epoch 6/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 6279398912.0000 - val_loss: 3336237824.0000\n",
      "Epoch 7/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 6212322816.0000 - val_loss: 3331586816.0000\n",
      "Epoch 8/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 6015420928.0000 - val_loss: 3205966592.0000\n",
      "Epoch 9/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 5921759744.0000 - val_loss: 3254848000.0000\n",
      "Epoch 10/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 5786306560.0000 - val_loss: 3286002688.0000\n",
      "Epoch 11/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 5835604480.0000 - val_loss: 3184171008.0000\n",
      "Epoch 12/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 5692980224.0000 - val_loss: 3584136704.0000\n",
      "Epoch 13/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 5567056384.0000 - val_loss: 3340404224.0000\n",
      "Epoch 14/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 5576038400.0000 - val_loss: 3300938240.0000\n",
      "Epoch 15/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 5528997376.0000 - val_loss: 3315309824.0000\n",
      "Epoch 16/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 5534714368.0000 - val_loss: 3169018112.0000\n",
      "Epoch 17/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 5417660416.0000 - val_loss: 3165884928.0000\n",
      "Epoch 18/50\n",
      "549/549 [==============================] - 2s 4ms/step - loss: 5518080000.0000 - val_loss: 3426451712.0000\n",
      "Epoch 19/50\n",
      "549/549 [==============================] - 2s 4ms/step - loss: 5398420992.0000 - val_loss: 3295793920.0000\n",
      "Epoch 20/50\n",
      "549/549 [==============================] - 2s 4ms/step - loss: 5339459584.0000 - val_loss: 3109295104.0000\n",
      "Epoch 21/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 5358141952.0000 - val_loss: 3501750016.0000\n",
      "Epoch 22/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 5330812928.0000 - val_loss: 3278450432.0000\n",
      "Epoch 23/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 5163581440.0000 - val_loss: 3189415168.0000\n",
      "Epoch 24/50\n",
      "549/549 [==============================] - 2s 4ms/step - loss: 5227255808.0000 - val_loss: 3134130432.0000\n",
      "Epoch 25/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 5125853184.0000 - val_loss: 3177874432.0000\n",
      "Epoch 26/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 5152461824.0000 - val_loss: 3410560768.0000\n",
      "Epoch 27/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 5212403712.0000 - val_loss: 3436345088.0000\n",
      "Epoch 28/50\n",
      "549/549 [==============================] - 2s 4ms/step - loss: 5243743744.0000 - val_loss: 3144252416.0000\n",
      "Epoch 29/50\n",
      "549/549 [==============================] - 2s 4ms/step - loss: 5223454208.0000 - val_loss: 3605199104.0000\n",
      "Epoch 30/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 5173580288.0000 - val_loss: 3617662976.0000\n",
      "Epoch 31/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 5050308096.0000 - val_loss: 3365516800.0000\n",
      "Epoch 32/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 5138039808.0000 - val_loss: 3249125888.0000\n",
      "Epoch 33/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 5161776128.0000 - val_loss: 3474212608.0000\n",
      "Epoch 34/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 5134141440.0000 - val_loss: 3444114688.0000\n",
      "Epoch 35/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 5022756864.0000 - val_loss: 3197622016.0000\n",
      "Epoch 36/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 5124279808.0000 - val_loss: 3233613568.0000\n",
      "Epoch 37/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 5071300608.0000 - val_loss: 3333043712.0000\n",
      "Epoch 38/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 5065932288.0000 - val_loss: 3347156224.0000\n",
      "Epoch 39/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 4910136832.0000 - val_loss: 3287267840.0000\n",
      "Epoch 40/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 5028618240.0000 - val_loss: 3430185728.0000\n",
      "Epoch 41/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 4903808000.0000 - val_loss: 3204194560.0000\n",
      "Epoch 42/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 4977737216.0000 - val_loss: 3154682624.0000\n",
      "Epoch 43/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 4894367232.0000 - val_loss: 3286646272.0000\n",
      "Epoch 44/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 4965619200.0000 - val_loss: 3277881088.0000\n",
      "Epoch 45/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 4918970368.0000 - val_loss: 3398772224.0000\n",
      "Epoch 46/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 4942809088.0000 - val_loss: 3205773568.0000\n",
      "Epoch 47/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 4968671744.0000 - val_loss: 3372582912.0000\n",
      "Epoch 48/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 4953577472.0000 - val_loss: 3321367296.0000\n",
      "Epoch 49/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 4908343808.0000 - val_loss: 3494895104.0000\n",
      "Epoch 50/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 4768973824.0000 - val_loss: 3589048064.0000\n",
      "Model: \"sequential_68\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_663 (Dense)           (None, 64)                896       \n",
      "                                                                 \n",
      " batch_normalization_8 (Batc  (None, 64)               256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_29 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_664 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " batch_normalization_9 (Batc  (None, 64)               256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_30 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_665 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " batch_normalization_10 (Bat  (None, 64)               256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_31 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_666 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " batch_normalization_11 (Bat  (None, 64)               256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_32 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_667 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " batch_normalization_12 (Bat  (None, 64)               256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_33 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_668 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " batch_normalization_13 (Bat  (None, 64)               256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_34 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_669 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " batch_normalization_14 (Bat  (None, 64)               256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_35 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_670 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " batch_normalization_15 (Bat  (None, 64)               256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_36 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_671 (Dense)           (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 32,129\n",
      "Trainable params: 31,105\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "549/549 [==============================] - 6s 6ms/step - loss: 56029605888.0000 - val_loss: 56418578432.0000\n",
      "Epoch 2/50\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 55971090432.0000 - val_loss: 56327323648.0000\n",
      "Epoch 3/50\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 55865176064.0000 - val_loss: 56203538432.0000\n",
      "Epoch 4/50\n",
      "549/549 [==============================] - 3s 6ms/step - loss: 55718404096.0000 - val_loss: 56051838976.0000\n",
      "Epoch 5/50\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 55534026752.0000 - val_loss: 55814582272.0000\n",
      "Epoch 6/50\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 55322726400.0000 - val_loss: 55564382208.0000\n",
      "Epoch 7/50\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 55081091072.0000 - val_loss: 55300804608.0000\n",
      "Epoch 8/50\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 54811422720.0000 - val_loss: 54984507392.0000\n",
      "Epoch 9/50\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 54515957760.0000 - val_loss: 54639992832.0000\n",
      "Epoch 10/50\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 54196305920.0000 - val_loss: 54353920000.0000\n",
      "Epoch 11/50\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 53850849280.0000 - val_loss: 54059667456.0000\n",
      "Epoch 12/50\n",
      "549/549 [==============================] - 3s 6ms/step - loss: 53484900352.0000 - val_loss: 53709090816.0000\n",
      "Epoch 13/50\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 53097713664.0000 - val_loss: 53215821824.0000\n",
      "Epoch 14/50\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 52686413824.0000 - val_loss: 52869668864.0000\n",
      "Epoch 15/50\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 52255625216.0000 - val_loss: 52471218176.0000\n",
      "Epoch 16/50\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 51796267008.0000 - val_loss: 51794911232.0000\n",
      "Epoch 17/50\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 51338035200.0000 - val_loss: 51228680192.0000\n",
      "Epoch 18/50\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 50842869760.0000 - val_loss: 50834927616.0000\n",
      "Epoch 19/50\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 50326982656.0000 - val_loss: 50309386240.0000\n",
      "Epoch 20/50\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 49794281472.0000 - val_loss: 49848250368.0000\n",
      "Epoch 21/50\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 49253597184.0000 - val_loss: 49027252224.0000\n",
      "Epoch 22/50\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 48684531712.0000 - val_loss: 48306909184.0000\n",
      "Epoch 23/50\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 48109871104.0000 - val_loss: 47813709824.0000\n",
      "Epoch 24/50\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 47512174592.0000 - val_loss: 47719624704.0000\n",
      "Epoch 25/50\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 46904938496.0000 - val_loss: 46777409536.0000\n",
      "Epoch 26/50\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 46261137408.0000 - val_loss: 46176722944.0000\n",
      "Epoch 27/50\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 45632442368.0000 - val_loss: 45541949440.0000\n",
      "Epoch 28/50\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 44973027328.0000 - val_loss: 44637773824.0000\n",
      "Epoch 29/50\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 44301930496.0000 - val_loss: 43794960384.0000\n",
      "Epoch 30/50\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 43607732224.0000 - val_loss: 43705810944.0000\n",
      "Epoch 31/50\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 42909753344.0000 - val_loss: 42963546112.0000\n",
      "Epoch 32/50\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 42212671488.0000 - val_loss: 41868840960.0000\n",
      "Epoch 33/50\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 41505148928.0000 - val_loss: 41298792448.0000\n",
      "Epoch 34/50\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 40768958464.0000 - val_loss: 41024565248.0000\n",
      "Epoch 35/50\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 40051646464.0000 - val_loss: 39421829120.0000\n",
      "Epoch 36/50\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 39324356608.0000 - val_loss: 38898843648.0000\n",
      "Epoch 37/50\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 38538043392.0000 - val_loss: 37642723328.0000\n",
      "Epoch 38/50\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 37764374528.0000 - val_loss: 37353893888.0000\n",
      "Epoch 39/50\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 37015719936.0000 - val_loss: 36704636928.0000\n",
      "Epoch 40/50\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 36270366720.0000 - val_loss: 36141600768.0000\n",
      "Epoch 41/50\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 35496177664.0000 - val_loss: 34713927680.0000\n",
      "Epoch 42/50\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 34703667200.0000 - val_loss: 34947694592.0000\n",
      "Epoch 43/50\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 33910654976.0000 - val_loss: 33809033216.0000\n",
      "Epoch 44/50\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 33152509952.0000 - val_loss: 33035821056.0000\n",
      "Epoch 45/50\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 32396498944.0000 - val_loss: 31885703168.0000\n",
      "Epoch 46/50\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 31548205056.0000 - val_loss: 31140343808.0000\n",
      "Epoch 47/50\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 30736551936.0000 - val_loss: 30214703104.0000\n",
      "Epoch 48/50\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 29974779904.0000 - val_loss: 29419759616.0000\n",
      "Epoch 49/50\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 29201092608.0000 - val_loss: 30397495296.0000\n",
      "Epoch 50/50\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 28439369728.0000 - val_loss: 27574618112.0000\n",
      "Model: \"sequential_69\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_672 (Dense)           (None, 64)                896       \n",
      "                                                                 \n",
      " dropout_37 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_673 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dropout_38 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_674 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dropout_39 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_675 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dropout_40 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_676 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dropout_41 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_677 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dropout_42 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_678 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dropout_43 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_679 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dropout_44 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_680 (Dense)           (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 30,081\n",
      "Trainable params: 30,081\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "549/549 [==============================] - 3s 4ms/step - loss: 19633147904.0000 - val_loss: 4557799936.0000\n",
      "Epoch 2/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 11104808960.0000 - val_loss: 5345197056.0000\n",
      "Epoch 3/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 10148582400.0000 - val_loss: 4343841280.0000\n",
      "Epoch 4/50\n",
      "549/549 [==============================] - 2s 4ms/step - loss: 9477984256.0000 - val_loss: 3436506624.0000\n",
      "Epoch 5/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 9280590848.0000 - val_loss: 4218039808.0000\n",
      "Epoch 6/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 9042203648.0000 - val_loss: 3636879104.0000\n",
      "Epoch 7/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 8809891840.0000 - val_loss: 4627676160.0000\n",
      "Epoch 8/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 8632611840.0000 - val_loss: 3532833280.0000\n",
      "Epoch 9/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 8627346432.0000 - val_loss: 4237868800.0000\n",
      "Epoch 10/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 8543729152.0000 - val_loss: 3342198272.0000\n",
      "Epoch 11/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 8331056128.0000 - val_loss: 3439672320.0000\n",
      "Epoch 12/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 8229032960.0000 - val_loss: 3743127296.0000\n",
      "Epoch 13/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 8341051392.0000 - val_loss: 3595482112.0000\n",
      "Epoch 14/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 8331507712.0000 - val_loss: 3496631808.0000\n",
      "Epoch 15/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 8239929344.0000 - val_loss: 3507515136.0000\n",
      "Epoch 16/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 8178717696.0000 - val_loss: 3400787712.0000\n",
      "Epoch 17/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 8139170304.0000 - val_loss: 3562110464.0000\n",
      "Epoch 18/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 8084582912.0000 - val_loss: 4131483648.0000\n",
      "Epoch 19/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 7933933568.0000 - val_loss: 3926463232.0000\n",
      "Epoch 20/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 7804321280.0000 - val_loss: 3218560256.0000\n",
      "Epoch 21/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 7976344064.0000 - val_loss: 3386846208.0000\n",
      "Epoch 22/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 7853229568.0000 - val_loss: 3771614976.0000\n",
      "Epoch 23/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 7741158400.0000 - val_loss: 3664969472.0000\n",
      "Epoch 24/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 7799537152.0000 - val_loss: 3226594048.0000\n",
      "Epoch 25/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 7721823744.0000 - val_loss: 3524989696.0000\n",
      "Epoch 26/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 7629721600.0000 - val_loss: 3447871232.0000\n",
      "Epoch 27/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 7711213568.0000 - val_loss: 3425292800.0000\n",
      "Epoch 28/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 7747193856.0000 - val_loss: 3241220608.0000\n",
      "Epoch 29/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 7842801152.0000 - val_loss: 4016367616.0000\n",
      "Epoch 30/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 7581532160.0000 - val_loss: 4094583552.0000\n",
      "Epoch 31/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 7777522688.0000 - val_loss: 3741408256.0000\n",
      "Epoch 32/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 7825235968.0000 - val_loss: 3462095872.0000\n",
      "Epoch 33/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 7564008960.0000 - val_loss: 3523243776.0000\n",
      "Epoch 34/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 7678800896.0000 - val_loss: 3833131264.0000\n",
      "Epoch 35/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 7607501824.0000 - val_loss: 3810156800.0000\n",
      "Epoch 36/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 7662756352.0000 - val_loss: 3836205824.0000\n",
      "Epoch 37/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 7574644736.0000 - val_loss: 3480051712.0000\n",
      "Epoch 38/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 7554873344.0000 - val_loss: 3865966848.0000\n",
      "Epoch 39/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 7452307456.0000 - val_loss: 3286125824.0000\n",
      "Epoch 40/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 7585306624.0000 - val_loss: 3326741248.0000\n",
      "Epoch 41/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 7532856832.0000 - val_loss: 3551042560.0000\n",
      "Epoch 42/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 7442038272.0000 - val_loss: 3406870784.0000\n",
      "Epoch 43/50\n",
      "549/549 [==============================] - 2s 4ms/step - loss: 7348956160.0000 - val_loss: 3385525760.0000\n",
      "Epoch 44/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 7490970624.0000 - val_loss: 3383731968.0000\n",
      "Epoch 45/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 7587451392.0000 - val_loss: 3385932288.0000\n",
      "Epoch 46/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 7447891456.0000 - val_loss: 3620158976.0000\n",
      "Epoch 47/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 7484489216.0000 - val_loss: 3480692480.0000\n",
      "Epoch 48/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 7341717504.0000 - val_loss: 3376060160.0000\n",
      "Epoch 49/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 7360512512.0000 - val_loss: 3491619840.0000\n",
      "Epoch 50/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 7256585728.0000 - val_loss: 3474664192.0000\n",
      "Model: \"sequential_70\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_681 (Dense)           (None, 64)                896       \n",
      "                                                                 \n",
      " batch_normalization_16 (Bat  (None, 64)               256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_45 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_682 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " batch_normalization_17 (Bat  (None, 64)               256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_46 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_683 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " batch_normalization_18 (Bat  (None, 64)               256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_47 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_684 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " batch_normalization_19 (Bat  (None, 64)               256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_48 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_685 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " batch_normalization_20 (Bat  (None, 64)               256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_49 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_686 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " batch_normalization_21 (Bat  (None, 64)               256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_50 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_687 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " batch_normalization_22 (Bat  (None, 64)               256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_51 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_688 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " batch_normalization_23 (Bat  (None, 64)               256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_52 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_689 (Dense)           (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 32,129\n",
      "Trainable params: 31,105\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "549/549 [==============================] - 6s 6ms/step - loss: 56031866880.0000 - val_loss: 56418521088.0000\n",
      "Epoch 2/50\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 55976013824.0000 - val_loss: 56339836928.0000\n",
      "Epoch 3/50\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 55875055616.0000 - val_loss: 56222142464.0000\n",
      "Epoch 4/50\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 55734898688.0000 - val_loss: 56075812864.0000\n",
      "Epoch 5/50\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 55559049216.0000 - val_loss: 55872983040.0000\n",
      "Epoch 6/50\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 55357587456.0000 - val_loss: 55574728704.0000\n",
      "Epoch 7/50\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 55128186880.0000 - val_loss: 55366737920.0000\n",
      "Epoch 8/50\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 54873604096.0000 - val_loss: 55041503232.0000\n",
      "Epoch 9/50\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 54597234688.0000 - val_loss: 54722482176.0000\n",
      "Epoch 10/50\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 54293372928.0000 - val_loss: 54417297408.0000\n",
      "Epoch 11/50\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 53968289792.0000 - val_loss: 54195109888.0000\n",
      "Epoch 12/50\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 53620527104.0000 - val_loss: 53804912640.0000\n",
      "Epoch 13/50\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 53256175616.0000 - val_loss: 53352120320.0000\n",
      "Epoch 14/50\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 52858093568.0000 - val_loss: 53146550272.0000\n",
      "Epoch 15/50\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 52451868672.0000 - val_loss: 52799582208.0000\n",
      "Epoch 16/50\n",
      "549/549 [==============================] - 3s 6ms/step - loss: 52028231680.0000 - val_loss: 52157046784.0000\n",
      "Epoch 17/50\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 51576602624.0000 - val_loss: 51447193600.0000\n",
      "Epoch 18/50\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 51111428096.0000 - val_loss: 51159179264.0000\n",
      "Epoch 19/50\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 50623283200.0000 - val_loss: 50737786880.0000\n",
      "Epoch 20/50\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 50100588544.0000 - val_loss: 50037960704.0000\n",
      "Epoch 21/50\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 49616330752.0000 - val_loss: 49487683584.0000\n",
      "Epoch 22/50\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 49060519936.0000 - val_loss: 49033932800.0000\n",
      "Epoch 23/50\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 48507736064.0000 - val_loss: 48500854784.0000\n",
      "Epoch 24/50\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 47934042112.0000 - val_loss: 47696990208.0000\n",
      "Epoch 25/50\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 47357054976.0000 - val_loss: 47434117120.0000\n",
      "Epoch 26/50\n",
      "549/549 [==============================] - 3s 6ms/step - loss: 46743117824.0000 - val_loss: 46499868672.0000\n",
      "Epoch 27/50\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 46130610176.0000 - val_loss: 46138789888.0000\n",
      "Epoch 28/50\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 45508919296.0000 - val_loss: 45691654144.0000\n",
      "Epoch 29/50\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 44876939264.0000 - val_loss: 44513521664.0000\n",
      "Epoch 30/50\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 44217655296.0000 - val_loss: 44124147712.0000\n",
      "Epoch 31/50\n",
      "549/549 [==============================] - 3s 6ms/step - loss: 43566551040.0000 - val_loss: 43113811968.0000\n",
      "Epoch 32/50\n",
      "549/549 [==============================] - 3s 6ms/step - loss: 42881863680.0000 - val_loss: 42455773184.0000\n",
      "Epoch 33/50\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 42212921344.0000 - val_loss: 42151006208.0000\n",
      "Epoch 34/50\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 41499803648.0000 - val_loss: 41150570496.0000\n",
      "Epoch 35/50\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 40835657728.0000 - val_loss: 40083255296.0000\n",
      "Epoch 36/50\n",
      "549/549 [==============================] - 3s 6ms/step - loss: 40091811840.0000 - val_loss: 39639277568.0000\n",
      "Epoch 37/50\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 39369314304.0000 - val_loss: 38596784128.0000\n",
      "Epoch 38/50\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 38598037504.0000 - val_loss: 38465454080.0000\n",
      "Epoch 39/50\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 37870465024.0000 - val_loss: 37877481472.0000\n",
      "Epoch 40/50\n",
      "549/549 [==============================] - 3s 6ms/step - loss: 37178163200.0000 - val_loss: 37015011328.0000\n",
      "Epoch 41/50\n",
      "549/549 [==============================] - 3s 6ms/step - loss: 36473872384.0000 - val_loss: 36235354112.0000\n",
      "Epoch 42/50\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 35625086976.0000 - val_loss: 35430432768.0000\n",
      "Epoch 43/50\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 34877370368.0000 - val_loss: 33905756160.0000\n",
      "Epoch 44/50\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 34155814912.0000 - val_loss: 33326268416.0000\n",
      "Epoch 45/50\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 33408604160.0000 - val_loss: 32736206848.0000\n",
      "Epoch 46/50\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 32619986944.0000 - val_loss: 32346109952.0000\n",
      "Epoch 47/50\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 31796545536.0000 - val_loss: 31764647936.0000\n",
      "Epoch 48/50\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 31061706752.0000 - val_loss: 29797890048.0000\n",
      "Epoch 49/50\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 30303858688.0000 - val_loss: 31108851712.0000\n",
      "Epoch 50/50\n",
      "549/549 [==============================] - 3s 5ms/step - loss: 29542084608.0000 - val_loss: 29100767232.0000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>configuracion</th>\n",
       "      <th>error_test</th>\n",
       "      <th>error_train</th>\n",
       "      <th>segundos_entrenamiento</th>\n",
       "      <th>epoca</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>ANN_capas=[64; 64; 64; 64; 64; 64; 64; 64; 1]_activaciones=['elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1</td>\n",
       "      <td>53376.532371</td>\n",
       "      <td>51400.826141</td>\n",
       "      <td>67.506442</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ANN_capas=[64; 64; 64; 64; 64; 64; 64; 64; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1</td>\n",
       "      <td>54231.385747</td>\n",
       "      <td>51958.811996</td>\n",
       "      <td>63.929625</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>ANN_capas=[32; 50; 64; 80; 80; 64; 50; 32; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1</td>\n",
       "      <td>54464.082256</td>\n",
       "      <td>52447.311771</td>\n",
       "      <td>65.858972</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>ANN_capas=[64; 64; 64; 64; 64; 64; 64; 64; 1]_activaciones=['selu'; 'selu'; 'selu'; 'selu'; 'selu'; 'selu'; 'selu'; 'selu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1</td>\n",
       "      <td>55036.204520</td>\n",
       "      <td>50085.477017</td>\n",
       "      <td>63.393345</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>ANN_capas=[50; 50; 50; 50; 50; 50; 50; 50; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1</td>\n",
       "      <td>55458.095748</td>\n",
       "      <td>52839.030423</td>\n",
       "      <td>58.103801</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>ANN_capas=[80; 80; 64; 64; 50; 50; 32; 32; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1</td>\n",
       "      <td>55472.535331</td>\n",
       "      <td>51841.128383</td>\n",
       "      <td>63.146074</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>ANN_capas=[64; 64; 64; 64; 64; 64; 64; 64; 1]_activaciones=['elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=nadam_paciencia=-1</td>\n",
       "      <td>55548.626626</td>\n",
       "      <td>51087.213273</td>\n",
       "      <td>73.071667</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>ANN_capas=[64; 64; 64; 64; 64; 64; 64; 64; 1]_activaciones=['elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=rmsprop_paciencia=-1</td>\n",
       "      <td>55743.102174</td>\n",
       "      <td>51816.125367</td>\n",
       "      <td>60.515572</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>ANN_capas=[64; 64; 64; 64; 64; 64; 64; 64; 1]_activaciones=['elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_porcentajesDropOut=[0.1; 0.1; 0.1; 0.1; 0.1; 0.1; 0.1; 0.1]_batchSize=32_optimizador=adam_paciencia=-1</td>\n",
       "      <td>56164.539987</td>\n",
       "      <td>60837.986028</td>\n",
       "      <td>83.210063</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ANN_capas=[8; 8; 8; 8; 8; 8; 8; 8; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=10</td>\n",
       "      <td>56168.076912</td>\n",
       "      <td>65505.463589</td>\n",
       "      <td>52.175863</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ANN_capas=[80; 80; 80; 80; 80; 80; 80; 80; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1</td>\n",
       "      <td>56252.450400</td>\n",
       "      <td>50596.551345</td>\n",
       "      <td>68.058731</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ANN_capas=[10; 10; 10; 10; 10; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=10</td>\n",
       "      <td>56579.769105</td>\n",
       "      <td>65503.054219</td>\n",
       "      <td>46.783819</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>ANN_capas=[50; 50; 50; 64; 64; 64; 80; 80; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1</td>\n",
       "      <td>56641.712968</td>\n",
       "      <td>50598.160283</td>\n",
       "      <td>71.024920</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>ANN_capas=[50; 50; 64; 80; 80; 64; 50; 50; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1</td>\n",
       "      <td>57028.581185</td>\n",
       "      <td>51121.124244</td>\n",
       "      <td>66.832304</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ANN_capas=[64; 64; 64; 64; 64; 64; 64; 64; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=10</td>\n",
       "      <td>57062.799371</td>\n",
       "      <td>57590.639239</td>\n",
       "      <td>17.061472</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                       configuracion  \\\n",
       "33                                                              ANN_capas=[64; 64; 64; 64; 64; 64; 64; 64; 1]_activaciones=['elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1   \n",
       "14                                                      ANN_capas=[64; 64; 64; 64; 64; 64; 64; 64; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1   \n",
       "24                                                      ANN_capas=[32; 50; 64; 80; 80; 64; 50; 32; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1   \n",
       "32                                                      ANN_capas=[64; 64; 64; 64; 64; 64; 64; 64; 1]_activaciones=['selu'; 'selu'; 'selu'; 'selu'; 'selu'; 'selu'; 'selu'; 'selu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1   \n",
       "18                                                      ANN_capas=[50; 50; 50; 50; 50; 50; 50; 50; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1   \n",
       "22                                                      ANN_capas=[80; 80; 64; 64; 50; 50; 32; 32; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1   \n",
       "38                                                             ANN_capas=[64; 64; 64; 64; 64; 64; 64; 64; 1]_activaciones=['elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=nadam_paciencia=-1   \n",
       "34                                                           ANN_capas=[64; 64; 64; 64; 64; 64; 64; 64; 1]_activaciones=['elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=rmsprop_paciencia=-1   \n",
       "40  ANN_capas=[64; 64; 64; 64; 64; 64; 64; 64; 1]_activaciones=['elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_porcentajesDropOut=[0.1; 0.1; 0.1; 0.1; 0.1; 0.1; 0.1; 0.1]_batchSize=32_optimizador=adam_paciencia=-1   \n",
       "7                                                               ANN_capas=[8; 8; 8; 8; 8; 8; 8; 8; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=10   \n",
       "15                                                      ANN_capas=[80; 80; 80; 80; 80; 80; 80; 80; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1   \n",
       "3                                                                                           ANN_capas=[10; 10; 10; 10; 10; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=10   \n",
       "20                                                      ANN_capas=[50; 50; 50; 64; 64; 64; 80; 80; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1   \n",
       "21                                                      ANN_capas=[50; 50; 64; 80; 80; 64; 50; 50; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1   \n",
       "10                                                      ANN_capas=[64; 64; 64; 64; 64; 64; 64; 64; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=10   \n",
       "\n",
       "      error_test   error_train  segundos_entrenamiento  epoca  \n",
       "33  53376.532371  51400.826141               67.506442   50.0  \n",
       "14  54231.385747  51958.811996               63.929625   50.0  \n",
       "24  54464.082256  52447.311771               65.858972   50.0  \n",
       "32  55036.204520  50085.477017               63.393345   50.0  \n",
       "18  55458.095748  52839.030423               58.103801   50.0  \n",
       "22  55472.535331  51841.128383               63.146074   50.0  \n",
       "38  55548.626626  51087.213273               73.071667   50.0  \n",
       "34  55743.102174  51816.125367               60.515572   50.0  \n",
       "40  56164.539987  60837.986028               83.210063   50.0  \n",
       "7   56168.076912  65505.463589               52.175863   50.0  \n",
       "15  56252.450400  50596.551345               68.058731   50.0  \n",
       "3   56579.769105  65503.054219               46.783819   50.0  \n",
       "20  56641.712968  50598.160283               71.024920   50.0  \n",
       "21  57028.581185  51121.124244               66.832304   50.0  \n",
       "10  57062.799371  57590.639239               17.061472   13.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Experimentos para número de capas\n",
    "experimentoDropOut = [0.1, 0.2, 0.4]\n",
    "listaCapas = [64, 64, 64, 64, 64, 64, 64, 64, 1]\n",
    "listaActivaciones = ['elu', 'elu', 'elu', 'elu', 'elu', 'elu', 'elu', 'elu', 'linear']\n",
    "for experimento in experimentoDropOut:\n",
    "    listaDropOut = []\n",
    "    for i in range(8):\n",
    "        listaDropOut.append(experimento)\n",
    "    redAnn, configuracion = construirANN(caracteristicasEstandarizadas.shape[1], listaCapas, listaActivaciones, \\\n",
    "                                         porcentajesDropout = listaDropOut)\n",
    "    redAnn.summary()\n",
    "    entrenarANN(redAnn, configuracion, caracteristicasEstandarizadas, etiquetas, 'mean_squared_error', \\\n",
    "                tamanioBatch=32, epocas=50, paciencia = -1)\n",
    "    redAnn, configuracion = construirANN(caracteristicasEstandarizadas.shape[1], listaCapas, listaActivaciones, \\\n",
    "                                         porcentajesDropout = listaDropOut, aplicarBatchNormalization=1)\n",
    "    redAnn.summary()\n",
    "    entrenarANN(redAnn, configuracion, caracteristicasEstandarizadas, etiquetas, 'mean_squared_error', \\\n",
    "                tamanioBatch=32, epocas=50, paciencia = -1)\n",
    "\n",
    "mostrarResultadosBitacora('ANN')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "earned-problem",
   "metadata": {},
   "source": [
    "Como se puede observar, la aplicación de DropOut ayudó a disminuir el error en los datos de validación más que en los datos de entrenamiento (redujo sobre ajuste) sin embargo redujo la tasa de disminución del error en los datos de entrenamiento. Se aplicará DropOut más adelante al modelo para reducir el sobreajuste.\n",
    "\n",
    "### Inicializadores\n",
    "Se aplicarán los siguientes inicializadores para los pesos del modelo y se observará si alguno tiene un mejor impacto en la etapa de entrenamiento:\n",
    "* glorot_normal\n",
    "* glorot_uniform\n",
    "* random_normal\n",
    "* random_uniform\n",
    "* he_normal\n",
    "* he_uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "saved-information",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_71\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_690 (Dense)           (None, 64)                896       \n",
      "                                                                 \n",
      " dense_691 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_692 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_693 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_694 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_695 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_696 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_697 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_698 (Dense)           (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 30,081\n",
      "Trainable params: 30,081\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 12432685056.0000 - val_loss: 3697792768.0000\n",
      "Epoch 2/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4506978304.0000 - val_loss: 3481970688.0000\n",
      "Epoch 3/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4244157952.0000 - val_loss: 3329118976.0000\n",
      "Epoch 4/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4009621248.0000 - val_loss: 3373665024.0000\n",
      "Epoch 5/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3863524864.0000 - val_loss: 3225048576.0000\n",
      "Epoch 6/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3737971456.0000 - val_loss: 3453003776.0000\n",
      "Epoch 7/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3712550656.0000 - val_loss: 3371750912.0000\n",
      "Epoch 8/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3616733696.0000 - val_loss: 3190694400.0000\n",
      "Epoch 9/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3571264256.0000 - val_loss: 3280616960.0000\n",
      "Epoch 10/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3538974464.0000 - val_loss: 3682791680.0000\n",
      "Epoch 11/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3509109248.0000 - val_loss: 3420565248.0000\n",
      "Epoch 12/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3462610432.0000 - val_loss: 3479739136.0000\n",
      "Epoch 13/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3407487488.0000 - val_loss: 3300666368.0000\n",
      "Epoch 14/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3384050944.0000 - val_loss: 3296740864.0000\n",
      "Epoch 15/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3341104640.0000 - val_loss: 3286019072.0000\n",
      "Epoch 16/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3296480256.0000 - val_loss: 3235059200.0000\n",
      "Epoch 17/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3259750912.0000 - val_loss: 3191392256.0000\n",
      "Epoch 18/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3203909632.0000 - val_loss: 3319334144.0000\n",
      "Epoch 19/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3203299328.0000 - val_loss: 3356514560.0000\n",
      "Epoch 20/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3151558400.0000 - val_loss: 3225750272.0000\n",
      "Epoch 21/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3134938112.0000 - val_loss: 3146741248.0000\n",
      "Epoch 22/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3108808704.0000 - val_loss: 3136299520.0000\n",
      "Epoch 23/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3055940352.0000 - val_loss: 3426864128.0000\n",
      "Epoch 24/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3047226880.0000 - val_loss: 3280007936.0000\n",
      "Epoch 25/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3036039424.0000 - val_loss: 3232979200.0000\n",
      "Epoch 26/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2995928064.0000 - val_loss: 3385101312.0000\n",
      "Epoch 27/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2979100928.0000 - val_loss: 3361043456.0000\n",
      "Epoch 28/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2992400896.0000 - val_loss: 3161053696.0000\n",
      "Epoch 29/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2937249024.0000 - val_loss: 3231102976.0000\n",
      "Epoch 30/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2941253376.0000 - val_loss: 3133384448.0000\n",
      "Epoch 31/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2929842688.0000 - val_loss: 2956175104.0000\n",
      "Epoch 32/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2871335168.0000 - val_loss: 3046553344.0000\n",
      "Epoch 33/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2896074240.0000 - val_loss: 3215866112.0000\n",
      "Epoch 34/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2861034752.0000 - val_loss: 3170288128.0000\n",
      "Epoch 35/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 2835293952.0000 - val_loss: 3089046272.0000\n",
      "Epoch 36/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 2854860800.0000 - val_loss: 3070708992.0000\n",
      "Epoch 37/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 2803884032.0000 - val_loss: 3398989568.0000\n",
      "Epoch 38/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 2798155776.0000 - val_loss: 3240382976.0000\n",
      "Epoch 39/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 2784472832.0000 - val_loss: 3231597824.0000\n",
      "Epoch 40/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2777851392.0000 - val_loss: 3101131264.0000\n",
      "Epoch 41/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 2736182784.0000 - val_loss: 3224374528.0000\n",
      "Epoch 42/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2727910400.0000 - val_loss: 3402798336.0000\n",
      "Epoch 43/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 2713099008.0000 - val_loss: 3079744256.0000\n",
      "Epoch 44/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 2715497472.0000 - val_loss: 3199204096.0000\n",
      "Epoch 45/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 2699538432.0000 - val_loss: 3182025216.0000\n",
      "Epoch 46/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2691250176.0000 - val_loss: 3222525952.0000\n",
      "Epoch 47/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 2662195712.0000 - val_loss: 3324109824.0000\n",
      "Epoch 48/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 2654170112.0000 - val_loss: 3198194432.0000\n",
      "Epoch 49/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2619137280.0000 - val_loss: 3187032320.0000\n",
      "Epoch 50/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 2638790400.0000 - val_loss: 3134851328.0000\n",
      "Model: \"sequential_72\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_699 (Dense)           (None, 64)                896       \n",
      "                                                                 \n",
      " dense_700 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_701 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_702 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_703 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_704 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_705 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_706 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_707 (Dense)           (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 30,081\n",
      "Trainable params: 30,081\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_73\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_708 (Dense)           (None, 64)                896       \n",
      "                                                                 \n",
      " dense_709 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_710 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_711 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_712 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_713 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_714 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_715 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_716 (Dense)           (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 30,081\n",
      "Trainable params: 30,081\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 12966770688.0000 - val_loss: 3827892736.0000\n",
      "Epoch 2/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 4878843392.0000 - val_loss: 3652702720.0000\n",
      "Epoch 3/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 4728415232.0000 - val_loss: 3592309248.0000\n",
      "Epoch 4/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 4559773696.0000 - val_loss: 3607533568.0000\n",
      "Epoch 5/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4457348608.0000 - val_loss: 3536127488.0000\n",
      "Epoch 6/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4347856384.0000 - val_loss: 3536462336.0000\n",
      "Epoch 7/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4281223936.0000 - val_loss: 3461091584.0000\n",
      "Epoch 8/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4189430272.0000 - val_loss: 3289443072.0000\n",
      "Epoch 9/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4117500672.0000 - val_loss: 3304510464.0000\n",
      "Epoch 10/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4041702912.0000 - val_loss: 3703817984.0000\n",
      "Epoch 11/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3965614848.0000 - val_loss: 3403105280.0000\n",
      "Epoch 12/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3868954880.0000 - val_loss: 3501242368.0000\n",
      "Epoch 13/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3796134656.0000 - val_loss: 3270281984.0000\n",
      "Epoch 14/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3748567040.0000 - val_loss: 3254178048.0000\n",
      "Epoch 15/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3721540864.0000 - val_loss: 3247770112.0000\n",
      "Epoch 16/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3675951360.0000 - val_loss: 3127096064.0000\n",
      "Epoch 17/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3620579328.0000 - val_loss: 3226418688.0000\n",
      "Epoch 18/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3570827264.0000 - val_loss: 3352588032.0000\n",
      "Epoch 19/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3581689856.0000 - val_loss: 3319351040.0000\n",
      "Epoch 20/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3551527168.0000 - val_loss: 3219238400.0000\n",
      "Epoch 21/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3540124672.0000 - val_loss: 3216032768.0000\n",
      "Epoch 22/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3510909696.0000 - val_loss: 3162931968.0000\n",
      "Epoch 23/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3464343552.0000 - val_loss: 3247677696.0000\n",
      "Epoch 24/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 3451361280.0000 - val_loss: 3355780864.0000\n",
      "Epoch 25/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 3431040000.0000 - val_loss: 3388058368.0000\n",
      "Epoch 26/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3389128448.0000 - val_loss: 3464285696.0000\n",
      "Epoch 27/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3373497088.0000 - val_loss: 3610548224.0000\n",
      "Epoch 28/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3373387520.0000 - val_loss: 3305849600.0000\n",
      "Epoch 29/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3323068672.0000 - val_loss: 3295351552.0000\n",
      "Epoch 30/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3313394944.0000 - val_loss: 3295970304.0000\n",
      "Epoch 31/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3290872320.0000 - val_loss: 3221152256.0000\n",
      "Epoch 32/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3234076160.0000 - val_loss: 3302475776.0000\n",
      "Epoch 33/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3235973376.0000 - val_loss: 3647704576.0000\n",
      "Epoch 34/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3206390272.0000 - val_loss: 3427439360.0000\n",
      "Epoch 35/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3180985600.0000 - val_loss: 3202365696.0000\n",
      "Epoch 36/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3189173248.0000 - val_loss: 3262486784.0000\n",
      "Epoch 37/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3128755968.0000 - val_loss: 3411107584.0000\n",
      "Epoch 38/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3118919936.0000 - val_loss: 3372638720.0000\n",
      "Epoch 39/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3111713792.0000 - val_loss: 3352558848.0000\n",
      "Epoch 40/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3083700224.0000 - val_loss: 3262529024.0000\n",
      "Epoch 41/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3055862528.0000 - val_loss: 3319072256.0000\n",
      "Epoch 42/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3040078336.0000 - val_loss: 3367653632.0000\n",
      "Epoch 43/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3018198016.0000 - val_loss: 3312367616.0000\n",
      "Epoch 44/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 3030695680.0000 - val_loss: 3191414272.0000\n",
      "Epoch 45/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2999283456.0000 - val_loss: 3192298240.0000\n",
      "Epoch 46/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2999175168.0000 - val_loss: 3233495040.0000\n",
      "Epoch 47/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3010687232.0000 - val_loss: 3335140864.0000\n",
      "Epoch 48/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2973432320.0000 - val_loss: 3409074432.0000\n",
      "Epoch 49/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2927568128.0000 - val_loss: 3275546880.0000\n",
      "Epoch 50/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2939349248.0000 - val_loss: 3152152576.0000\n",
      "Model: \"sequential_74\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_717 (Dense)           (None, 64)                896       \n",
      "                                                                 \n",
      " dense_718 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_719 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_720 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_721 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_722 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_723 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_724 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_725 (Dense)           (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 30,081\n",
      "Trainable params: 30,081\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 13851627520.0000 - val_loss: 3753780736.0000\n",
      "Epoch 2/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4873620480.0000 - val_loss: 3613991424.0000\n",
      "Epoch 3/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 4745161216.0000 - val_loss: 3550980608.0000\n",
      "Epoch 4/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 4558253568.0000 - val_loss: 3480788992.0000\n",
      "Epoch 5/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4416857600.0000 - val_loss: 3361311232.0000\n",
      "Epoch 6/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4248007168.0000 - val_loss: 3270683648.0000\n",
      "Epoch 7/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4170388992.0000 - val_loss: 3256886528.0000\n",
      "Epoch 8/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4034334208.0000 - val_loss: 3136667392.0000\n",
      "Epoch 9/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3923994880.0000 - val_loss: 3159094016.0000\n",
      "Epoch 10/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3850349056.0000 - val_loss: 3411919360.0000\n",
      "Epoch 11/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3792765440.0000 - val_loss: 3124813824.0000\n",
      "Epoch 12/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3754668800.0000 - val_loss: 3330360320.0000\n",
      "Epoch 13/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3714998528.0000 - val_loss: 3176752896.0000\n",
      "Epoch 14/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3693624832.0000 - val_loss: 3151035904.0000\n",
      "Epoch 15/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3684484608.0000 - val_loss: 3101120768.0000\n",
      "Epoch 16/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3656492032.0000 - val_loss: 3048425216.0000\n",
      "Epoch 17/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3636895744.0000 - val_loss: 3115324928.0000\n",
      "Epoch 18/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3609451264.0000 - val_loss: 3181061888.0000\n",
      "Epoch 19/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3621377536.0000 - val_loss: 3168928512.0000\n",
      "Epoch 20/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3590366720.0000 - val_loss: 3106469120.0000\n",
      "Epoch 21/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3600666112.0000 - val_loss: 3084359168.0000\n",
      "Epoch 22/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3595452672.0000 - val_loss: 3045813248.0000\n",
      "Epoch 23/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 3547692288.0000 - val_loss: 3133794816.0000\n",
      "Epoch 24/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 3540096000.0000 - val_loss: 3110988288.0000\n",
      "Epoch 25/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3543321856.0000 - val_loss: 3125851392.0000\n",
      "Epoch 26/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3506142976.0000 - val_loss: 3331652096.0000\n",
      "Epoch 27/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3505700096.0000 - val_loss: 3352207616.0000\n",
      "Epoch 28/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3497352448.0000 - val_loss: 3104875776.0000\n",
      "Epoch 29/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3471819008.0000 - val_loss: 3152534016.0000\n",
      "Epoch 30/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3468947712.0000 - val_loss: 3160234240.0000\n",
      "Epoch 31/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3447770624.0000 - val_loss: 3171005952.0000\n",
      "Epoch 32/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3417730816.0000 - val_loss: 3235929856.0000\n",
      "Epoch 33/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3430861568.0000 - val_loss: 3319729920.0000\n",
      "Epoch 34/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3395113216.0000 - val_loss: 3349937920.0000\n",
      "Epoch 35/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3373899008.0000 - val_loss: 3150685952.0000\n",
      "Epoch 36/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 3383400448.0000 - val_loss: 3142084096.0000\n",
      "Epoch 37/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 3340236032.0000 - val_loss: 3232145408.0000\n",
      "Epoch 38/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3320871680.0000 - val_loss: 3195392512.0000\n",
      "Epoch 39/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3326992128.0000 - val_loss: 3186557440.0000\n",
      "Epoch 40/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3301986560.0000 - val_loss: 3222825984.0000\n",
      "Epoch 41/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3282787840.0000 - val_loss: 3160974592.0000\n",
      "Epoch 42/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3260322304.0000 - val_loss: 3204876800.0000\n",
      "Epoch 43/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3243069952.0000 - val_loss: 3341101056.0000\n",
      "Epoch 44/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3255747328.0000 - val_loss: 3214057728.0000\n",
      "Epoch 45/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3213956352.0000 - val_loss: 3193953792.0000\n",
      "Epoch 46/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3262227968.0000 - val_loss: 3250861056.0000\n",
      "Epoch 47/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3191676160.0000 - val_loss: 3223564800.0000\n",
      "Epoch 48/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3190618624.0000 - val_loss: 3431413760.0000\n",
      "Epoch 49/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3138740992.0000 - val_loss: 3444837120.0000\n",
      "Epoch 50/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3149036800.0000 - val_loss: 3330005760.0000\n",
      "Model: \"sequential_75\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_726 (Dense)           (None, 64)                896       \n",
      "                                                                 \n",
      " dense_727 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_728 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_729 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_730 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_731 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_732 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_733 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_734 (Dense)           (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 30,081\n",
      "Trainable params: 30,081\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 12194246656.0000 - val_loss: 3720688128.0000\n",
      "Epoch 2/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4205760768.0000 - val_loss: 3368045824.0000\n",
      "Epoch 3/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3888785664.0000 - val_loss: 3365055232.0000\n",
      "Epoch 4/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3681522944.0000 - val_loss: 3525159168.0000\n",
      "Epoch 5/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3540920064.0000 - val_loss: 3347832832.0000\n",
      "Epoch 6/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3419584512.0000 - val_loss: 3716026624.0000\n",
      "Epoch 7/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3389938176.0000 - val_loss: 3505831680.0000\n",
      "Epoch 8/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3279376128.0000 - val_loss: 3262261760.0000\n",
      "Epoch 9/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3218852352.0000 - val_loss: 3402330368.0000\n",
      "Epoch 10/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3157341440.0000 - val_loss: 3609347328.0000\n",
      "Epoch 11/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3103178752.0000 - val_loss: 3358951680.0000\n",
      "Epoch 12/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3048422400.0000 - val_loss: 3269314048.0000\n",
      "Epoch 13/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2970362624.0000 - val_loss: 3273439232.0000\n",
      "Epoch 14/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2961082880.0000 - val_loss: 3321939456.0000\n",
      "Epoch 15/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2924417024.0000 - val_loss: 3291426560.0000\n",
      "Epoch 16/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2892212992.0000 - val_loss: 3278170112.0000\n",
      "Epoch 17/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2854561792.0000 - val_loss: 3597015552.0000\n",
      "Epoch 18/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2816846848.0000 - val_loss: 3435003904.0000\n",
      "Epoch 19/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2815194624.0000 - val_loss: 3392903936.0000\n",
      "Epoch 20/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2785617920.0000 - val_loss: 3345224448.0000\n",
      "Epoch 21/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2758826752.0000 - val_loss: 3415901952.0000\n",
      "Epoch 22/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2757180416.0000 - val_loss: 3430420480.0000\n",
      "Epoch 23/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2708533504.0000 - val_loss: 3655242752.0000\n",
      "Epoch 24/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2696798720.0000 - val_loss: 3568346368.0000\n",
      "Epoch 25/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2678062080.0000 - val_loss: 3432252416.0000\n",
      "Epoch 26/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2645939456.0000 - val_loss: 3459522048.0000\n",
      "Epoch 27/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2633805056.0000 - val_loss: 3614039040.0000\n",
      "Epoch 28/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2652580864.0000 - val_loss: 3335543040.0000\n",
      "Epoch 29/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2602071552.0000 - val_loss: 3771905792.0000\n",
      "Epoch 30/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2603959552.0000 - val_loss: 3437124864.0000\n",
      "Epoch 31/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2588274688.0000 - val_loss: 3210183936.0000\n",
      "Epoch 32/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2542649344.0000 - val_loss: 3249569280.0000\n",
      "Epoch 33/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2563273728.0000 - val_loss: 3494707712.0000\n",
      "Epoch 34/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2535934976.0000 - val_loss: 3582522624.0000\n",
      "Epoch 35/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2513913088.0000 - val_loss: 3422757632.0000\n",
      "Epoch 36/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2537187328.0000 - val_loss: 3370891520.0000\n",
      "Epoch 37/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2492375808.0000 - val_loss: 3673368320.0000\n",
      "Epoch 38/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2483863040.0000 - val_loss: 3512803072.0000\n",
      "Epoch 39/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2483179776.0000 - val_loss: 3660641024.0000\n",
      "Epoch 40/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2471445504.0000 - val_loss: 3384805120.0000\n",
      "Epoch 41/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2434459392.0000 - val_loss: 3579480832.0000\n",
      "Epoch 42/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2440413184.0000 - val_loss: 3688101120.0000\n",
      "Epoch 43/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2427536128.0000 - val_loss: 3446736640.0000\n",
      "Epoch 44/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2437965312.0000 - val_loss: 3552644608.0000\n",
      "Epoch 45/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2418759424.0000 - val_loss: 3554581248.0000\n",
      "Epoch 46/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2406424064.0000 - val_loss: 3603790336.0000\n",
      "Epoch 47/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2383931648.0000 - val_loss: 3623056896.0000\n",
      "Epoch 48/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2386297088.0000 - val_loss: 3444490240.0000\n",
      "Epoch 49/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2361775616.0000 - val_loss: 3477630464.0000\n",
      "Epoch 50/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2365355264.0000 - val_loss: 3471815424.0000\n",
      "Model: \"sequential_76\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_735 (Dense)           (None, 64)                896       \n",
      "                                                                 \n",
      " dense_736 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_737 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_738 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_739 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_740 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_741 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_742 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_743 (Dense)           (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 30,081\n",
      "Trainable params: 30,081\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "549/549 [==============================] - 3s 3ms/step - loss: 12474895360.0000 - val_loss: 3659444224.0000\n",
      "Epoch 2/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 4139112448.0000 - val_loss: 3680272896.0000\n",
      "Epoch 3/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 3799225856.0000 - val_loss: 3631311104.0000\n",
      "Epoch 4/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 3616992512.0000 - val_loss: 3804798208.0000\n",
      "Epoch 5/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3489292800.0000 - val_loss: 3493447680.0000\n",
      "Epoch 6/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 3322940928.0000 - val_loss: 3472279808.0000\n",
      "Epoch 7/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 3266185472.0000 - val_loss: 3511682304.0000\n",
      "Epoch 8/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3163948544.0000 - val_loss: 3217670912.0000\n",
      "Epoch 9/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3121745664.0000 - val_loss: 3415860736.0000\n",
      "Epoch 10/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3072951552.0000 - val_loss: 3428264960.0000\n",
      "Epoch 11/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 3031465728.0000 - val_loss: 3278895360.0000\n",
      "Epoch 12/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 2991342848.0000 - val_loss: 3440968448.0000\n",
      "Epoch 13/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2920397824.0000 - val_loss: 3366161920.0000\n",
      "Epoch 14/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2911613952.0000 - val_loss: 3297293568.0000\n",
      "Epoch 15/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2872180480.0000 - val_loss: 3384819200.0000\n",
      "Epoch 16/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 2852282112.0000 - val_loss: 3352266752.0000\n",
      "Epoch 17/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 2814911232.0000 - val_loss: 3692802560.0000\n",
      "Epoch 18/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 2776966144.0000 - val_loss: 3345576960.0000\n",
      "Epoch 19/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 2767314944.0000 - val_loss: 3304359680.0000\n",
      "Epoch 20/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 2756275456.0000 - val_loss: 3381393408.0000\n",
      "Epoch 21/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2722830592.0000 - val_loss: 3499818240.0000\n",
      "Epoch 22/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 2724139264.0000 - val_loss: 3524708352.0000\n",
      "Epoch 23/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 2675415552.0000 - val_loss: 3746863360.0000\n",
      "Epoch 24/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 2665628416.0000 - val_loss: 3766239232.0000\n",
      "Epoch 25/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 2651153920.0000 - val_loss: 3427335680.0000\n",
      "Epoch 26/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2626687488.0000 - val_loss: 3448750080.0000\n",
      "Epoch 27/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2606779392.0000 - val_loss: 3881130752.0000\n",
      "Epoch 28/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2635151872.0000 - val_loss: 3388213504.0000\n",
      "Epoch 29/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2575672064.0000 - val_loss: 3882110208.0000\n",
      "Epoch 30/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 2588169472.0000 - val_loss: 3610611968.0000\n",
      "Epoch 31/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 2570835456.0000 - val_loss: 3366658560.0000\n",
      "Epoch 32/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 2526381568.0000 - val_loss: 3517927424.0000\n",
      "Epoch 33/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 2564241152.0000 - val_loss: 3541453312.0000\n",
      "Epoch 34/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2521102336.0000 - val_loss: 3765593600.0000\n",
      "Epoch 35/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 2497388800.0000 - val_loss: 3629951744.0000\n",
      "Epoch 36/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 2524896768.0000 - val_loss: 3516310272.0000\n",
      "Epoch 37/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 2471612416.0000 - val_loss: 3954068736.0000\n",
      "Epoch 38/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 2470538752.0000 - val_loss: 3775970048.0000\n",
      "Epoch 39/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 2467628800.0000 - val_loss: 3910975488.0000\n",
      "Epoch 40/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 2464552448.0000 - val_loss: 3595168512.0000\n",
      "Epoch 41/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 2426830848.0000 - val_loss: 4054682624.0000\n",
      "Epoch 42/50\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2433655552.0000 - val_loss: 4075173376.0000\n",
      "Epoch 43/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 2417840640.0000 - val_loss: 3773755136.0000\n",
      "Epoch 44/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 2426837248.0000 - val_loss: 3680138240.0000\n",
      "Epoch 45/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 2412204032.0000 - val_loss: 3697468672.0000\n",
      "Epoch 46/50\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 2394088704.0000 - val_loss: 3838557696.0000\n",
      "Epoch 47/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 2373746688.0000 - val_loss: 4007084800.0000\n",
      "Epoch 48/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 2384745728.0000 - val_loss: 3836381952.0000\n",
      "Epoch 49/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 2348561152.0000 - val_loss: 3614803200.0000\n",
      "Epoch 50/50\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 2358443264.0000 - val_loss: 3922847232.0000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>configuracion</th>\n",
       "      <th>error_test</th>\n",
       "      <th>error_train</th>\n",
       "      <th>segundos_entrenamiento</th>\n",
       "      <th>epoca</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>ANN_capas=[64; 64; 64; 64; 64; 64; 64; 64; 1]_activaciones=['elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1</td>\n",
       "      <td>53376.532371</td>\n",
       "      <td>51400.826141</td>\n",
       "      <td>67.506442</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ANN_capas=[64; 64; 64; 64; 64; 64; 64; 64; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1</td>\n",
       "      <td>54231.385747</td>\n",
       "      <td>51958.811996</td>\n",
       "      <td>63.929625</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>ANN_capas=[32; 50; 64; 80; 80; 64; 50; 32; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1</td>\n",
       "      <td>54464.082256</td>\n",
       "      <td>52447.311771</td>\n",
       "      <td>65.858972</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>ANN_capas=[64; 64; 64; 64; 64; 64; 64; 64; 1]_activaciones=['selu'; 'selu'; 'selu'; 'selu'; 'selu'; 'selu'; 'selu'; 'selu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1</td>\n",
       "      <td>55036.204520</td>\n",
       "      <td>50085.477017</td>\n",
       "      <td>63.393345</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>ANN_capas=[50; 50; 50; 50; 50; 50; 50; 50; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1</td>\n",
       "      <td>55458.095748</td>\n",
       "      <td>52839.030423</td>\n",
       "      <td>58.103801</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>ANN_capas=[80; 80; 64; 64; 50; 50; 32; 32; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1</td>\n",
       "      <td>55472.535331</td>\n",
       "      <td>51841.128383</td>\n",
       "      <td>63.146074</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>ANN_capas=[64; 64; 64; 64; 64; 64; 64; 64; 1]_activaciones=['elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=nadam_paciencia=-1</td>\n",
       "      <td>55548.626626</td>\n",
       "      <td>51087.213273</td>\n",
       "      <td>73.071667</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>ANN_capas=[64; 64; 64; 64; 64; 64; 64; 64; 1]_activaciones=['elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=rmsprop_paciencia=-1</td>\n",
       "      <td>55743.102174</td>\n",
       "      <td>51816.125367</td>\n",
       "      <td>60.515572</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>ANN_capas=[64; 64; 64; 64; 64; 64; 64; 64; 1]_activaciones=['elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'linear']_inicializadorKernel=glorot_normal_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1</td>\n",
       "      <td>55989.743061</td>\n",
       "      <td>51369.158062</td>\n",
       "      <td>66.137771</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>ANN_capas=[64; 64; 64; 64; 64; 64; 64; 64; 1]_activaciones=['elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'linear']_inicializadorKernel=random_normal_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1</td>\n",
       "      <td>56144.034198</td>\n",
       "      <td>54215.765678</td>\n",
       "      <td>66.496849</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>ANN_capas=[64; 64; 64; 64; 64; 64; 64; 64; 1]_activaciones=['elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_porcentajesDropOut=[0.1; 0.1; 0.1; 0.1; 0.1; 0.1; 0.1; 0.1]_batchSize=32_optimizador=adam_paciencia=-1</td>\n",
       "      <td>56164.539987</td>\n",
       "      <td>60837.986028</td>\n",
       "      <td>83.210063</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ANN_capas=[8; 8; 8; 8; 8; 8; 8; 8; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=10</td>\n",
       "      <td>56168.076912</td>\n",
       "      <td>65505.463589</td>\n",
       "      <td>52.175863</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ANN_capas=[80; 80; 80; 80; 80; 80; 80; 80; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1</td>\n",
       "      <td>56252.450400</td>\n",
       "      <td>50596.551345</td>\n",
       "      <td>68.058731</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ANN_capas=[10; 10; 10; 10; 10; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=10</td>\n",
       "      <td>56579.769105</td>\n",
       "      <td>65503.054219</td>\n",
       "      <td>46.783819</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>ANN_capas=[50; 50; 50; 64; 64; 64; 80; 80; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1</td>\n",
       "      <td>56641.712968</td>\n",
       "      <td>50598.160283</td>\n",
       "      <td>71.024920</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                       configuracion  \\\n",
       "33                                                              ANN_capas=[64; 64; 64; 64; 64; 64; 64; 64; 1]_activaciones=['elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1   \n",
       "14                                                      ANN_capas=[64; 64; 64; 64; 64; 64; 64; 64; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1   \n",
       "24                                                      ANN_capas=[32; 50; 64; 80; 80; 64; 50; 32; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1   \n",
       "32                                                      ANN_capas=[64; 64; 64; 64; 64; 64; 64; 64; 1]_activaciones=['selu'; 'selu'; 'selu'; 'selu'; 'selu'; 'selu'; 'selu'; 'selu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1   \n",
       "18                                                      ANN_capas=[50; 50; 50; 50; 50; 50; 50; 50; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1   \n",
       "22                                                      ANN_capas=[80; 80; 64; 64; 50; 50; 32; 32; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1   \n",
       "38                                                             ANN_capas=[64; 64; 64; 64; 64; 64; 64; 64; 1]_activaciones=['elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=nadam_paciencia=-1   \n",
       "34                                                           ANN_capas=[64; 64; 64; 64; 64; 64; 64; 64; 1]_activaciones=['elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=rmsprop_paciencia=-1   \n",
       "46                                                               ANN_capas=[64; 64; 64; 64; 64; 64; 64; 64; 1]_activaciones=['elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'linear']_inicializadorKernel=glorot_normal_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1   \n",
       "47                                                               ANN_capas=[64; 64; 64; 64; 64; 64; 64; 64; 1]_activaciones=['elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'linear']_inicializadorKernel=random_normal_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1   \n",
       "40  ANN_capas=[64; 64; 64; 64; 64; 64; 64; 64; 1]_activaciones=['elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_porcentajesDropOut=[0.1; 0.1; 0.1; 0.1; 0.1; 0.1; 0.1; 0.1]_batchSize=32_optimizador=adam_paciencia=-1   \n",
       "7                                                               ANN_capas=[8; 8; 8; 8; 8; 8; 8; 8; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=10   \n",
       "15                                                      ANN_capas=[80; 80; 80; 80; 80; 80; 80; 80; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1   \n",
       "3                                                                                           ANN_capas=[10; 10; 10; 10; 10; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=10   \n",
       "20                                                      ANN_capas=[50; 50; 50; 64; 64; 64; 80; 80; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1   \n",
       "\n",
       "      error_test   error_train  segundos_entrenamiento  epoca  \n",
       "33  53376.532371  51400.826141               67.506442   50.0  \n",
       "14  54231.385747  51958.811996               63.929625   50.0  \n",
       "24  54464.082256  52447.311771               65.858972   50.0  \n",
       "32  55036.204520  50085.477017               63.393345   50.0  \n",
       "18  55458.095748  52839.030423               58.103801   50.0  \n",
       "22  55472.535331  51841.128383               63.146074   50.0  \n",
       "38  55548.626626  51087.213273               73.071667   50.0  \n",
       "34  55743.102174  51816.125367               60.515572   50.0  \n",
       "46  55989.743061  51369.158062               66.137771   50.0  \n",
       "47  56144.034198  54215.765678               66.496849   50.0  \n",
       "40  56164.539987  60837.986028               83.210063   50.0  \n",
       "7   56168.076912  65505.463589               52.175863   50.0  \n",
       "15  56252.450400  50596.551345               68.058731   50.0  \n",
       "3   56579.769105  65503.054219               46.783819   50.0  \n",
       "20  56641.712968  50598.160283               71.024920   50.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Experimentos para número de capas\n",
    "experimentosInicializadores = ['glorot_normal', 'glorot_uniform', 'random_normal', 'random_uniform', 'he_normal', 'he_uniform']\n",
    "listaCapas = [64, 64, 64, 64, 64, 64, 64, 64, 1]\n",
    "listaActivaciones = ['elu', 'elu', 'elu', 'elu', 'elu', 'elu', 'elu', 'elu', 'linear']\n",
    "for experimento in experimentosInicializadores:\n",
    "    redAnn, configuracion = construirANN(caracteristicasEstandarizadas.shape[1], listaCapas, listaActivaciones, \\\n",
    "                                        inicializadorKernel = experimento)\n",
    "    redAnn.summary()\n",
    "    entrenarANN(redAnn, configuracion, caracteristicasEstandarizadas, etiquetas, 'mean_squared_error', \\\n",
    "                tamanioBatch=32, epocas=50, paciencia = -1)\n",
    "\n",
    "mostrarResultadosBitacora('ANN')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "varying-supervision",
   "metadata": {},
   "source": [
    "De los datos anteriores se puede observar que la inicialización __glorot_uniform__ retornó los mejores resultados y que otras opciones aceptables podrían ser: __glorot_normal__ y __random_normal__.\n",
    "\n",
    "### Tamaño de batch\n",
    "Por último se aumentará el número de épocas hasta 150 y se observará que tamaño de batch tiene mejor rendimiento. Se utilizarán tamaños de batch de: 8, 16, 32, 64, 128, 256, 512, 1024 y 2048."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "decreased-great",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para graficar resultados de entrenamiento\n",
    "def graficarResultados(datosEntrenamiento, titulo):\n",
    "    erroresEntrenamiento = datosEntrenamiento.history['loss']\n",
    "    erroresValidacion = datosEntrenamiento.history['val_loss']\n",
    "    epocas = range(1, len(erroresEntrenamiento) + 1)\n",
    "    plt.plot(epocas, erroresEntrenamiento, 'b', label='Error de entrenamiento')\n",
    "    plt.plot(epocas, erroresValidacion, 'r', label='Error de validación')\n",
    "    plt.title(titulo)\n",
    "    plt.xlabel('Épocas')\n",
    "    plt.ylabel('Error (MSE)')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "acceptable-madagascar",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_81\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_780 (Dense)           (None, 64)                896       \n",
      "                                                                 \n",
      " dense_781 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_782 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_783 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_784 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_785 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_786 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_787 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_788 (Dense)           (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 30,081\n",
      "Trainable params: 30,081\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "2193/2193 [==============================] - 5s 2ms/step - loss: 6860725248.0000 - val_loss: 3366218240.0000\n",
      "Epoch 2/200\n",
      "2193/2193 [==============================] - 4s 2ms/step - loss: 4154469632.0000 - val_loss: 3789433856.0000\n",
      "Epoch 3/200\n",
      "2193/2193 [==============================] - 4s 2ms/step - loss: 3905260800.0000 - val_loss: 3794486528.0000\n",
      "Epoch 4/200\n",
      "2193/2193 [==============================] - 4s 2ms/step - loss: 3717815296.0000 - val_loss: 3516120320.0000\n",
      "Epoch 5/200\n",
      "2193/2193 [==============================] - 4s 2ms/step - loss: 3613472768.0000 - val_loss: 3371381504.0000\n",
      "Epoch 6/200\n",
      "2193/2193 [==============================] - 4s 2ms/step - loss: 3511635456.0000 - val_loss: 3218998272.0000\n",
      "Epoch 7/200\n",
      "2193/2193 [==============================] - 5s 2ms/step - loss: 3433840640.0000 - val_loss: 3431777792.0000\n",
      "Epoch 8/200\n",
      "2193/2193 [==============================] - 5s 2ms/step - loss: 3389336832.0000 - val_loss: 3265036288.0000\n",
      "Epoch 9/200\n",
      "2193/2193 [==============================] - 5s 2ms/step - loss: 3345383424.0000 - val_loss: 3210159616.0000\n",
      "Epoch 10/200\n",
      "2193/2193 [==============================] - 4s 2ms/step - loss: 3267684864.0000 - val_loss: 3304871424.0000\n",
      "Epoch 11/200\n",
      "2193/2193 [==============================] - 4s 2ms/step - loss: 3220377344.0000 - val_loss: 3277484800.0000\n",
      "Epoch 12/200\n",
      "2193/2193 [==============================] - 5s 2ms/step - loss: 3145130752.0000 - val_loss: 3195044352.0000\n",
      "Epoch 13/200\n",
      "2193/2193 [==============================] - 4s 2ms/step - loss: 3120243200.0000 - val_loss: 3119084544.0000\n",
      "Epoch 14/200\n",
      "2193/2193 [==============================] - 4s 2ms/step - loss: 3068562432.0000 - val_loss: 3213982720.0000\n",
      "Epoch 15/200\n",
      "2193/2193 [==============================] - 4s 2ms/step - loss: 3033267200.0000 - val_loss: 3293620992.0000\n",
      "Epoch 16/200\n",
      "2193/2193 [==============================] - 4s 2ms/step - loss: 3020287488.0000 - val_loss: 3219349248.0000\n",
      "Epoch 17/200\n",
      "2193/2193 [==============================] - 4s 2ms/step - loss: 2953863424.0000 - val_loss: 3270507776.0000\n",
      "Epoch 18/200\n",
      "2193/2193 [==============================] - 5s 2ms/step - loss: 2927707904.0000 - val_loss: 3187267328.0000\n",
      "Epoch 19/200\n",
      "2193/2193 [==============================] - 5s 2ms/step - loss: 2903051008.0000 - val_loss: 3366705920.0000\n",
      "Epoch 20/200\n",
      "2193/2193 [==============================] - 4s 2ms/step - loss: 2892943360.0000 - val_loss: 3153120256.0000\n",
      "Epoch 21/200\n",
      "2193/2193 [==============================] - 4s 2ms/step - loss: 2851720704.0000 - val_loss: 3121087232.0000\n",
      "Epoch 22/200\n",
      "2193/2193 [==============================] - 4s 2ms/step - loss: 2843093760.0000 - val_loss: 3157482240.0000\n",
      "Epoch 23/200\n",
      "2193/2193 [==============================] - 5s 2ms/step - loss: 2803305728.0000 - val_loss: 3510122496.0000\n",
      "Epoch 24/200\n",
      "2193/2193 [==============================] - 4s 2ms/step - loss: 2774879232.0000 - val_loss: 3557743872.0000\n",
      "Epoch 25/200\n",
      "2193/2193 [==============================] - 4s 2ms/step - loss: 2721825280.0000 - val_loss: 3088401664.0000\n",
      "Epoch 26/200\n",
      "2193/2193 [==============================] - 4s 2ms/step - loss: 2707684096.0000 - val_loss: 3287522560.0000\n",
      "Epoch 27/200\n",
      "2193/2193 [==============================] - 4s 2ms/step - loss: 2671493120.0000 - val_loss: 3271029760.0000\n",
      "Epoch 28/200\n",
      "2193/2193 [==============================] - 4s 2ms/step - loss: 2652710144.0000 - val_loss: 3252929536.0000\n",
      "Epoch 29/200\n",
      "2193/2193 [==============================] - 5s 2ms/step - loss: 2627302912.0000 - val_loss: 3371637504.0000\n",
      "Epoch 30/200\n",
      "2193/2193 [==============================] - 4s 2ms/step - loss: 2593221120.0000 - val_loss: 3300261120.0000\n",
      "Epoch 31/200\n",
      "2193/2193 [==============================] - 4s 2ms/step - loss: 2567809024.0000 - val_loss: 3273071104.0000\n",
      "Epoch 32/200\n",
      "2193/2193 [==============================] - 5s 2ms/step - loss: 2526635520.0000 - val_loss: 3506325248.0000\n",
      "Epoch 33/200\n",
      "2193/2193 [==============================] - 4s 2ms/step - loss: 2537483520.0000 - val_loss: 3806795264.0000\n",
      "Epoch 34/200\n",
      "2193/2193 [==============================] - 4s 2ms/step - loss: 2502158080.0000 - val_loss: 3550314496.0000\n",
      "Epoch 35/200\n",
      "2193/2193 [==============================] - 4s 2ms/step - loss: 2464931584.0000 - val_loss: 3516248320.0000\n",
      "Epoch 36/200\n",
      "2193/2193 [==============================] - 5s 2ms/step - loss: 2514675200.0000 - val_loss: 3893298688.0000\n",
      "Epoch 37/200\n",
      "2193/2193 [==============================] - 4s 2ms/step - loss: 2461567744.0000 - val_loss: 3633483520.0000\n",
      "Epoch 38/200\n",
      "2193/2193 [==============================] - 4s 2ms/step - loss: 2439032320.0000 - val_loss: 4011568384.0000\n",
      "Epoch 39/200\n",
      "2193/2193 [==============================] - 4s 2ms/step - loss: 2421245696.0000 - val_loss: 3684679424.0000\n",
      "Epoch 40/200\n",
      "2193/2193 [==============================] - 4s 2ms/step - loss: 2404789248.0000 - val_loss: 3600014848.0000\n",
      "Epoch 41/200\n",
      "2193/2193 [==============================] - 4s 2ms/step - loss: 2393108736.0000 - val_loss: 3810994176.0000\n",
      "Epoch 42/200\n",
      "2193/2193 [==============================] - 4s 2ms/step - loss: 2370052864.0000 - val_loss: 4384123392.0000\n",
      "Epoch 43/200\n",
      "2193/2193 [==============================] - 4s 2ms/step - loss: 2359006720.0000 - val_loss: 3824051968.0000\n",
      "Epoch 44/200\n",
      "2193/2193 [==============================] - 4s 2ms/step - loss: 2350075648.0000 - val_loss: 4144940544.0000\n",
      "Epoch 45/200\n",
      "2193/2193 [==============================] - 4s 2ms/step - loss: 2330489088.0000 - val_loss: 3984868096.0000\n",
      "Epoch 46/200\n",
      "2193/2193 [==============================] - 4s 2ms/step - loss: 2311036928.0000 - val_loss: 4381279744.0000\n",
      "Epoch 47/200\n",
      "2193/2193 [==============================] - 4s 2ms/step - loss: 2299751936.0000 - val_loss: 4052068864.0000\n",
      "Epoch 48/200\n",
      "2193/2193 [==============================] - 4s 2ms/step - loss: 2271460608.0000 - val_loss: 3860124928.0000\n",
      "Epoch 49/200\n",
      "2193/2193 [==============================] - 5s 2ms/step - loss: 2278840576.0000 - val_loss: 3926015232.0000\n",
      "Epoch 50/200\n",
      "2193/2193 [==============================] - 4s 2ms/step - loss: 2257781248.0000 - val_loss: 4113780736.0000\n",
      "Epoch 51/200\n",
      "2193/2193 [==============================] - 4s 2ms/step - loss: 2254282496.0000 - val_loss: 4321757184.0000\n",
      "Epoch 52/200\n",
      "2193/2193 [==============================] - 4s 2ms/step - loss: 2248971264.0000 - val_loss: 4346729472.0000\n",
      "Epoch 53/200\n",
      "2193/2193 [==============================] - 4s 2ms/step - loss: 2214800128.0000 - val_loss: 3954128384.0000\n",
      "Epoch 54/200\n",
      "2193/2193 [==============================] - 4s 2ms/step - loss: 2221962752.0000 - val_loss: 4150123008.0000\n",
      "Epoch 55/200\n",
      "2193/2193 [==============================] - 4s 2ms/step - loss: 2191747840.0000 - val_loss: 4028760064.0000\n",
      "Epoch 56/200\n",
      "2193/2193 [==============================] - 4s 2ms/step - loss: 2182127616.0000 - val_loss: 3885420800.0000\n",
      "Epoch 57/200\n",
      "2193/2193 [==============================] - 4s 2ms/step - loss: 2169583616.0000 - val_loss: 4187426048.0000\n",
      "Epoch 58/200\n",
      "2193/2193 [==============================] - 5s 2ms/step - loss: 2149839872.0000 - val_loss: 4621786112.0000\n",
      "Epoch 59/200\n",
      "2193/2193 [==============================] - 4s 2ms/step - loss: 2157745152.0000 - val_loss: 5178014720.0000\n",
      "Epoch 60/200\n",
      "2193/2193 [==============================] - 4s 2ms/step - loss: 2155807232.0000 - val_loss: 4134615040.0000\n",
      "Epoch 61/200\n",
      "2193/2193 [==============================] - 5s 2ms/step - loss: 2114387968.0000 - val_loss: 4267298304.0000\n",
      "Epoch 62/200\n",
      "2193/2193 [==============================] - 4s 2ms/step - loss: 2085430272.0000 - val_loss: 4336377344.0000\n",
      "Epoch 63/200\n",
      "2193/2193 [==============================] - 4s 2ms/step - loss: 2086683904.0000 - val_loss: 4304046080.0000\n",
      "Epoch 64/200\n",
      "2193/2193 [==============================] - 4s 2ms/step - loss: 2079440256.0000 - val_loss: 4527842816.0000\n",
      "Epoch 65/200\n",
      "2193/2193 [==============================] - 4s 2ms/step - loss: 2088854912.0000 - val_loss: 4723792896.0000\n",
      "Epoch 66/200\n",
      "2193/2193 [==============================] - 4s 2ms/step - loss: 2059505792.0000 - val_loss: 4468293120.0000\n",
      "Epoch 67/200\n",
      "2193/2193 [==============================] - 4s 2ms/step - loss: 2065078912.0000 - val_loss: 4592047104.0000\n",
      "Epoch 68/200\n",
      "2193/2193 [==============================] - 5s 2ms/step - loss: 2024076800.0000 - val_loss: 4252566016.0000\n",
      "Epoch 69/200\n",
      "2193/2193 [==============================] - 4s 2ms/step - loss: 2030438272.0000 - val_loss: 4119071744.0000\n",
      "Epoch 70/200\n",
      "2193/2193 [==============================] - 4s 2ms/step - loss: 2017643648.0000 - val_loss: 4252501760.0000\n",
      "Epoch 71/200\n",
      "2193/2193 [==============================] - 4s 2ms/step - loss: 1996737280.0000 - val_loss: 4736663552.0000\n",
      "Epoch 72/200\n",
      "2193/2193 [==============================] - 5s 2ms/step - loss: 1979812480.0000 - val_loss: 4404548096.0000\n",
      "Epoch 73/200\n",
      "2193/2193 [==============================] - 5s 2ms/step - loss: 1993878528.0000 - val_loss: 4543457280.0000\n",
      "Epoch 74/200\n",
      "2193/2193 [==============================] - 4s 2ms/step - loss: 1972430592.0000 - val_loss: 5112295936.0000\n",
      "Epoch 75/200\n",
      "2193/2193 [==============================] - 4s 2ms/step - loss: 1943277056.0000 - val_loss: 4266123008.0000\n",
      "Epoch 76/200\n",
      "2193/2193 [==============================] - 4s 2ms/step - loss: 1958497024.0000 - val_loss: 4336371200.0000\n",
      "Epoch 77/200\n",
      "2193/2193 [==============================] - 4s 2ms/step - loss: 1945694720.0000 - val_loss: 4477690880.0000\n",
      "Epoch 78/200\n",
      "2193/2193 [==============================] - 4s 2ms/step - loss: 1940052224.0000 - val_loss: 4760971264.0000\n",
      "Epoch 79/200\n",
      "2193/2193 [==============================] - 4s 2ms/step - loss: 1923508608.0000 - val_loss: 4938526720.0000\n",
      "Epoch 80/200\n",
      "2193/2193 [==============================] - 4s 2ms/step - loss: 1897560576.0000 - val_loss: 4554425344.0000\n",
      "Epoch 81/200\n",
      "2193/2193 [==============================] - 4s 2ms/step - loss: 1906040320.0000 - val_loss: 4603890176.0000\n",
      "Epoch 82/200\n",
      "2193/2193 [==============================] - 4s 2ms/step - loss: 1891798016.0000 - val_loss: 4892702208.0000\n",
      "Epoch 83/200\n",
      "2193/2193 [==============================] - 4s 2ms/step - loss: 1867918976.0000 - val_loss: 5186373632.0000\n",
      "Epoch 84/200\n",
      "2193/2193 [==============================] - 4s 2ms/step - loss: 1861939456.0000 - val_loss: 4725482496.0000\n",
      "Epoch 85/200\n",
      "2193/2193 [==============================] - 4s 2ms/step - loss: 1866844288.0000 - val_loss: 4854455296.0000\n",
      "Epoch 86/200\n",
      "2193/2193 [==============================] - 4s 2ms/step - loss: 1830619008.0000 - val_loss: 5533942272.0000\n",
      "Epoch 87/200\n",
      "2193/2193 [==============================] - 4s 2ms/step - loss: 1850715648.0000 - val_loss: 4567805952.0000\n",
      "Epoch 88/200\n",
      "2193/2193 [==============================] - 4s 2ms/step - loss: 1836815104.0000 - val_loss: 4942184448.0000\n",
      "Epoch 89/200\n",
      "2193/2193 [==============================] - 4s 2ms/step - loss: 1797473408.0000 - val_loss: 4798396416.0000\n",
      "Epoch 90/200\n",
      "2193/2193 [==============================] - 4s 2ms/step - loss: 1826492928.0000 - val_loss: 5317050368.0000\n",
      "Epoch 91/200\n",
      "2193/2193 [==============================] - 4s 2ms/step - loss: 1806217472.0000 - val_loss: 4757837824.0000\n",
      "Epoch 92/200\n",
      "2193/2193 [==============================] - 4s 2ms/step - loss: 1783247360.0000 - val_loss: 4575233536.0000\n",
      "Epoch 93/200\n",
      "2193/2193 [==============================] - 4s 2ms/step - loss: 1796215552.0000 - val_loss: 4561844224.0000\n",
      "Epoch 94/200\n",
      "2193/2193 [==============================] - 4s 2ms/step - loss: 1782213760.0000 - val_loss: 4420089856.0000\n",
      "Epoch 95/200\n",
      "2193/2193 [==============================] - 4s 2ms/step - loss: 1780049152.0000 - val_loss: 4545019904.0000\n",
      "Epoch 96/200\n",
      "2193/2193 [==============================] - 4s 2ms/step - loss: 1754596352.0000 - val_loss: 4966598656.0000\n",
      "Epoch 97/200\n",
      "2193/2193 [==============================] - 4s 2ms/step - loss: 1766213248.0000 - val_loss: 4808162816.0000\n",
      "Epoch 98/200\n",
      "2193/2193 [==============================] - 4s 2ms/step - loss: 1742543104.0000 - val_loss: 5334846976.0000\n",
      "Epoch 99/200\n",
      "2193/2193 [==============================] - 5s 2ms/step - loss: 1741424384.0000 - val_loss: 5081944064.0000\n",
      "Epoch 100/200\n",
      "2193/2193 [==============================] - 4s 2ms/step - loss: 1736219136.0000 - val_loss: 5211195904.0000\n",
      "Epoch 101/200\n",
      "2193/2193 [==============================] - 4s 2ms/step - loss: 1719989632.0000 - val_loss: 5525462528.0000\n",
      "Epoch 102/200\n",
      "2193/2193 [==============================] - 4s 2ms/step - loss: 1718564608.0000 - val_loss: 4747202048.0000\n",
      "Epoch 103/200\n",
      "2193/2193 [==============================] - 5s 2ms/step - loss: 1699200640.0000 - val_loss: 4635153920.0000\n",
      "Epoch 104/200\n",
      "2193/2193 [==============================] - 4s 2ms/step - loss: 1691158656.0000 - val_loss: 4662005248.0000\n",
      "Epoch 105/200\n",
      "2193/2193 [==============================] - 4s 2ms/step - loss: 1701191168.0000 - val_loss: 4525765120.0000\n",
      "Epoch 106/200\n",
      "2193/2193 [==============================] - 4s 2ms/step - loss: 1677542528.0000 - val_loss: 4507408384.0000\n",
      "Epoch 107/200\n",
      "2193/2193 [==============================] - 4s 2ms/step - loss: 1682007296.0000 - val_loss: 4712251392.0000\n",
      "Epoch 108/200\n",
      "2193/2193 [==============================] - 4s 2ms/step - loss: 1695336448.0000 - val_loss: 4809124352.0000\n",
      "Epoch 109/200\n",
      "2193/2193 [==============================] - 5s 2ms/step - loss: 1662475008.0000 - val_loss: 4586280960.0000\n",
      "Epoch 110/200\n",
      "2193/2193 [==============================] - 4s 2ms/step - loss: 1692200576.0000 - val_loss: 4732110848.0000\n",
      "Epoch 111/200\n",
      "2193/2193 [==============================] - 4s 2ms/step - loss: 1639251712.0000 - val_loss: 4801985536.0000\n",
      "Epoch 112/200\n",
      "2193/2193 [==============================] - 4s 2ms/step - loss: 1656533760.0000 - val_loss: 5719191040.0000\n",
      "Epoch 113/200\n",
      "2193/2193 [==============================] - 5s 2ms/step - loss: 1642444160.0000 - val_loss: 4582012928.0000\n",
      "Epoch 114/200\n",
      "2193/2193 [==============================] - 5s 2ms/step - loss: 1628838144.0000 - val_loss: 4838612992.0000\n",
      "Epoch 115/200\n",
      "2193/2193 [==============================] - 5s 2ms/step - loss: 1630462080.0000 - val_loss: 4700007936.0000\n",
      "Epoch 116/200\n",
      "2193/2193 [==============================] - 4s 2ms/step - loss: 1619331584.0000 - val_loss: 5161050624.0000\n",
      "Epoch 117/200\n",
      "2193/2193 [==============================] - 5s 2ms/step - loss: 1584237824.0000 - val_loss: 5193604608.0000\n",
      "Epoch 118/200\n",
      "2193/2193 [==============================] - 4s 2ms/step - loss: 1608730240.0000 - val_loss: 5105284096.0000\n",
      "Epoch 119/200\n",
      "2193/2193 [==============================] - 4s 2ms/step - loss: 1600766208.0000 - val_loss: 4598166528.0000\n",
      "Epoch 120/200\n",
      "2193/2193 [==============================] - 4s 2ms/step - loss: 1589448576.0000 - val_loss: 4704553984.0000\n",
      "Epoch 121/200\n",
      "2193/2193 [==============================] - 4s 2ms/step - loss: 1586220416.0000 - val_loss: 5101898240.0000\n",
      "Epoch 122/200\n",
      "2193/2193 [==============================] - 5s 2ms/step - loss: 1577928832.0000 - val_loss: 4990635520.0000\n",
      "Epoch 123/200\n",
      "2193/2193 [==============================] - 4s 2ms/step - loss: 1556931840.0000 - val_loss: 4885326848.0000\n",
      "Epoch 124/200\n",
      "2193/2193 [==============================] - 4s 2ms/step - loss: 1557636736.0000 - val_loss: 4964924416.0000\n",
      "Epoch 125/200\n",
      "2193/2193 [==============================] - 5s 2ms/step - loss: 1560803072.0000 - val_loss: 5506935296.0000\n",
      "Epoch 126/200\n",
      "2193/2193 [==============================] - 4s 2ms/step - loss: 1533030144.0000 - val_loss: 4801302528.0000\n",
      "Epoch 127/200\n",
      "2193/2193 [==============================] - 4s 2ms/step - loss: 1542835072.0000 - val_loss: 5385971200.0000\n",
      "Epoch 128/200\n",
      "2193/2193 [==============================] - 5s 2ms/step - loss: 1533377792.0000 - val_loss: 4712744448.0000\n",
      "Epoch 129/200\n",
      "2193/2193 [==============================] - 4s 2ms/step - loss: 1541185408.0000 - val_loss: 4874578432.0000\n",
      "Epoch 130/200\n",
      "2193/2193 [==============================] - 4s 2ms/step - loss: 1529224448.0000 - val_loss: 4889504768.0000\n",
      "Epoch 131/200\n",
      "2193/2193 [==============================] - 4s 2ms/step - loss: 1487303552.0000 - val_loss: 5452451840.0000\n",
      "Epoch 132/200\n",
      "2193/2193 [==============================] - 4s 2ms/step - loss: 1516422656.0000 - val_loss: 4938235904.0000\n",
      "Epoch 133/200\n",
      "2193/2193 [==============================] - 4s 2ms/step - loss: 1520985728.0000 - val_loss: 5086012928.0000\n",
      "Epoch 134/200\n",
      "2193/2193 [==============================] - 4s 2ms/step - loss: 1503984896.0000 - val_loss: 4500759040.0000\n",
      "Epoch 135/200\n",
      "2193/2193 [==============================] - 4s 2ms/step - loss: 1503753088.0000 - val_loss: 5092546048.0000\n",
      "Epoch 136/200\n",
      "2193/2193 [==============================] - 5s 2ms/step - loss: 1471842176.0000 - val_loss: 4628149248.0000\n",
      "Epoch 137/200\n",
      "2193/2193 [==============================] - 5s 2ms/step - loss: 1484397696.0000 - val_loss: 4896323072.0000\n",
      "Epoch 138/200\n",
      "2193/2193 [==============================] - 4s 2ms/step - loss: 1483571328.0000 - val_loss: 4986792448.0000\n",
      "Epoch 139/200\n",
      "2193/2193 [==============================] - 4s 2ms/step - loss: 1493082496.0000 - val_loss: 4544455680.0000\n",
      "Epoch 140/200\n",
      "2193/2193 [==============================] - 4s 2ms/step - loss: 1484172672.0000 - val_loss: 4711842304.0000\n",
      "Epoch 141/200\n",
      "2193/2193 [==============================] - 4s 2ms/step - loss: 1486269056.0000 - val_loss: 5027344896.0000\n",
      "Epoch 142/200\n",
      "2193/2193 [==============================] - 4s 2ms/step - loss: 1447032576.0000 - val_loss: 5008600576.0000\n",
      "Epoch 143/200\n",
      "2193/2193 [==============================] - 5s 2ms/step - loss: 1477942784.0000 - val_loss: 4845135872.0000\n",
      "Epoch 144/200\n",
      "2193/2193 [==============================] - 5s 2ms/step - loss: 1435335168.0000 - val_loss: 4901570560.0000\n",
      "Epoch 145/200\n",
      "2193/2193 [==============================] - 4s 2ms/step - loss: 1457309824.0000 - val_loss: 4998443008.0000\n",
      "Epoch 146/200\n",
      "2193/2193 [==============================] - 4s 2ms/step - loss: 1441623168.0000 - val_loss: 4859486720.0000\n",
      "Epoch 147/200\n",
      "2193/2193 [==============================] - 4s 2ms/step - loss: 1406756096.0000 - val_loss: 4728070656.0000\n",
      "Epoch 148/200\n",
      "2193/2193 [==============================] - 4s 2ms/step - loss: 1444453504.0000 - val_loss: 4889094144.0000\n",
      "Epoch 149/200\n",
      "2193/2193 [==============================] - 4s 2ms/step - loss: 1416015488.0000 - val_loss: 5522888704.0000\n",
      "Epoch 150/200\n",
      "2193/2193 [==============================] - 4s 2ms/step - loss: 1396754048.0000 - val_loss: 5628079616.0000\n",
      "Epoch 151/200\n",
      "2193/2193 [==============================] - 4s 2ms/step - loss: 1415258112.0000 - val_loss: 4642890752.0000\n",
      "Epoch 152/200\n",
      "2193/2193 [==============================] - 5s 2ms/step - loss: 1401580032.0000 - val_loss: 5602105344.0000\n",
      "Epoch 153/200\n",
      "2193/2193 [==============================] - 4s 2ms/step - loss: 1396799104.0000 - val_loss: 4864829440.0000\n",
      "Epoch 154/200\n",
      "2193/2193 [==============================] - 4s 2ms/step - loss: 1386987008.0000 - val_loss: 4682284032.0000\n",
      "Epoch 155/200\n",
      "2193/2193 [==============================] - 5s 2ms/step - loss: 1387721728.0000 - val_loss: 4857698816.0000\n",
      "Epoch 156/200\n",
      "2193/2193 [==============================] - 5s 2ms/step - loss: 1418935040.0000 - val_loss: 4501973504.0000\n",
      "Epoch 157/200\n",
      "2193/2193 [==============================] - 4s 2ms/step - loss: 1363953792.0000 - val_loss: 4737596928.0000\n",
      "Epoch 158/200\n",
      "2193/2193 [==============================] - 4s 2ms/step - loss: 1384402560.0000 - val_loss: 5016635904.0000\n",
      "Epoch 159/200\n",
      "2193/2193 [==============================] - 4s 2ms/step - loss: 1353415296.0000 - val_loss: 4801975296.0000\n",
      "Epoch 160/200\n",
      "2193/2193 [==============================] - 4s 2ms/step - loss: 1339830912.0000 - val_loss: 4858150400.0000\n",
      "Epoch 161/200\n",
      "2193/2193 [==============================] - 5s 2ms/step - loss: 1362251776.0000 - val_loss: 5316471808.0000\n",
      "Epoch 162/200\n",
      "2193/2193 [==============================] - 4s 2ms/step - loss: 1377592448.0000 - val_loss: 5024552448.0000\n",
      "Epoch 163/200\n",
      "2193/2193 [==============================] - 5s 2ms/step - loss: 1389374720.0000 - val_loss: 5261236736.0000\n",
      "Epoch 164/200\n",
      "2193/2193 [==============================] - 4s 2ms/step - loss: 1328441600.0000 - val_loss: 5016825344.0000\n",
      "Epoch 165/200\n",
      "2193/2193 [==============================] - 5s 2ms/step - loss: 1369112832.0000 - val_loss: 4723501568.0000\n",
      "Epoch 166/200\n",
      "2193/2193 [==============================] - 4s 2ms/step - loss: 1317760768.0000 - val_loss: 4638969856.0000\n",
      "Epoch 167/200\n",
      "2193/2193 [==============================] - 4s 2ms/step - loss: 1348260224.0000 - val_loss: 4427713024.0000\n",
      "Epoch 168/200\n",
      "2193/2193 [==============================] - 5s 2ms/step - loss: 1332006272.0000 - val_loss: 5071233536.0000\n",
      "Epoch 169/200\n",
      "2193/2193 [==============================] - 5s 2ms/step - loss: 1336940416.0000 - val_loss: 4799110144.0000\n",
      "Epoch 170/200\n",
      "2193/2193 [==============================] - 4s 2ms/step - loss: 1312080000.0000 - val_loss: 5354964992.0000\n",
      "Epoch 171/200\n",
      "2193/2193 [==============================] - 4s 2ms/step - loss: 1324231168.0000 - val_loss: 5554280448.0000\n",
      "Epoch 172/200\n",
      "2193/2193 [==============================] - 4s 2ms/step - loss: 1297400192.0000 - val_loss: 5053491200.0000\n",
      "Epoch 173/200\n",
      "2193/2193 [==============================] - 4s 2ms/step - loss: 1308034432.0000 - val_loss: 4745687040.0000\n",
      "Epoch 174/200\n",
      "2193/2193 [==============================] - 4s 2ms/step - loss: 1320642944.0000 - val_loss: 4858487296.0000\n",
      "Epoch 175/200\n",
      "2193/2193 [==============================] - 5s 2ms/step - loss: 1304169728.0000 - val_loss: 4611656704.0000\n",
      "Epoch 176/200\n",
      "2193/2193 [==============================] - 4s 2ms/step - loss: 1299375744.0000 - val_loss: 5515401216.0000\n",
      "Epoch 177/200\n",
      "2193/2193 [==============================] - 5s 2ms/step - loss: 1329979392.0000 - val_loss: 5006320640.0000\n",
      "Epoch 178/200\n",
      "2193/2193 [==============================] - 4s 2ms/step - loss: 1268029568.0000 - val_loss: 4852845568.0000\n",
      "Epoch 179/200\n",
      "2193/2193 [==============================] - 4s 2ms/step - loss: 1293949440.0000 - val_loss: 4610550784.0000\n",
      "Epoch 180/200\n",
      "2193/2193 [==============================] - 4s 2ms/step - loss: 1323301376.0000 - val_loss: 4871814656.0000\n",
      "Epoch 181/200\n",
      "2193/2193 [==============================] - 4s 2ms/step - loss: 1311169920.0000 - val_loss: 5184348160.0000\n",
      "Epoch 182/200\n",
      "2193/2193 [==============================] - 5s 2ms/step - loss: 1272898048.0000 - val_loss: 4901442560.0000\n",
      "Epoch 183/200\n",
      "2193/2193 [==============================] - 4s 2ms/step - loss: 1256998144.0000 - val_loss: 4800928768.0000\n",
      "Epoch 184/200\n",
      "2193/2193 [==============================] - 4s 2ms/step - loss: 1261426432.0000 - val_loss: 4879058432.0000\n",
      "Epoch 185/200\n",
      "2193/2193 [==============================] - 5s 2ms/step - loss: 1298750080.0000 - val_loss: 4804483584.0000\n",
      "Epoch 186/200\n",
      "2193/2193 [==============================] - 4s 2ms/step - loss: 1294481920.0000 - val_loss: 4694342656.0000\n",
      "Epoch 187/200\n",
      "2193/2193 [==============================] - 4s 2ms/step - loss: 1236198784.0000 - val_loss: 4885016064.0000\n",
      "Epoch 188/200\n",
      "2193/2193 [==============================] - 6s 3ms/step - loss: 1245228032.0000 - val_loss: 4959170560.0000\n",
      "Epoch 189/200\n",
      "2193/2193 [==============================] - 5s 2ms/step - loss: 1266686080.0000 - val_loss: 5069174272.0000\n",
      "Epoch 190/200\n",
      "2193/2193 [==============================] - 5s 2ms/step - loss: 1242603904.0000 - val_loss: 4668928512.0000\n",
      "Epoch 191/200\n",
      "2193/2193 [==============================] - 5s 2ms/step - loss: 1296379264.0000 - val_loss: 5416969728.0000\n",
      "Epoch 192/200\n",
      "2193/2193 [==============================] - 5s 2ms/step - loss: 1210685824.0000 - val_loss: 4808504832.0000\n",
      "Epoch 193/200\n",
      "2193/2193 [==============================] - 4s 2ms/step - loss: 1245321088.0000 - val_loss: 4983613952.0000\n",
      "Epoch 194/200\n",
      "2193/2193 [==============================] - 5s 2ms/step - loss: 1214593792.0000 - val_loss: 4771032064.0000\n",
      "Epoch 195/200\n",
      "2193/2193 [==============================] - 5s 2ms/step - loss: 1239019520.0000 - val_loss: 4818396160.0000\n",
      "Epoch 196/200\n",
      "2193/2193 [==============================] - 5s 2ms/step - loss: 1246353024.0000 - val_loss: 5057418240.0000\n",
      "Epoch 197/200\n",
      "2193/2193 [==============================] - 5s 2ms/step - loss: 1235010048.0000 - val_loss: 5041722368.0000\n",
      "Epoch 198/200\n",
      "2193/2193 [==============================] - 4s 2ms/step - loss: 1226454400.0000 - val_loss: 5026816512.0000\n",
      "Epoch 199/200\n",
      "2193/2193 [==============================] - 5s 2ms/step - loss: 1186994560.0000 - val_loss: 4864663552.0000\n",
      "Epoch 200/200\n",
      "2193/2193 [==============================] - 4s 2ms/step - loss: 1202062208.0000 - val_loss: 4805577216.0000\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_82\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_789 (Dense)           (None, 64)                896       \n",
      "                                                                 \n",
      " dense_790 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_791 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_792 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_793 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_794 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_795 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_796 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_797 (Dense)           (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 30,081\n",
      "Trainable params: 30,081\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "1097/1097 [==============================] - 3s 2ms/step - loss: 8872223744.0000 - val_loss: 3523025408.0000\n",
      "Epoch 2/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 4360829952.0000 - val_loss: 3772617216.0000\n",
      "Epoch 3/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 4023087360.0000 - val_loss: 3359779072.0000\n",
      "Epoch 4/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 3813966592.0000 - val_loss: 3453914112.0000\n",
      "Epoch 5/200\n",
      "1097/1097 [==============================] - 3s 2ms/step - loss: 3747589120.0000 - val_loss: 3321141248.0000\n",
      "Epoch 6/200\n",
      "1097/1097 [==============================] - 3s 2ms/step - loss: 3647061504.0000 - val_loss: 3480860160.0000\n",
      "Epoch 7/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 3592947712.0000 - val_loss: 3435760896.0000\n",
      "Epoch 8/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 3553588224.0000 - val_loss: 3273632000.0000\n",
      "Epoch 9/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 3463879680.0000 - val_loss: 3461232640.0000\n",
      "Epoch 10/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 3403033600.0000 - val_loss: 3428579584.0000\n",
      "Epoch 11/200\n",
      "1097/1097 [==============================] - 3s 2ms/step - loss: 3337752064.0000 - val_loss: 3453337344.0000\n",
      "Epoch 12/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 3270195456.0000 - val_loss: 3368612352.0000\n",
      "Epoch 13/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 3203484672.0000 - val_loss: 3264496384.0000\n",
      "Epoch 14/200\n",
      "1097/1097 [==============================] - 3s 2ms/step - loss: 3154319616.0000 - val_loss: 3338838272.0000\n",
      "Epoch 15/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 3117635584.0000 - val_loss: 3288984064.0000\n",
      "Epoch 16/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 3093160192.0000 - val_loss: 3478039552.0000\n",
      "Epoch 17/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 3043289088.0000 - val_loss: 3186926592.0000\n",
      "Epoch 18/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 3014290176.0000 - val_loss: 3215998976.0000\n",
      "Epoch 19/200\n",
      "1097/1097 [==============================] - 3s 2ms/step - loss: 2983158784.0000 - val_loss: 3182170112.0000\n",
      "Epoch 20/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 2971576576.0000 - val_loss: 3113563648.0000\n",
      "Epoch 21/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 2959246848.0000 - val_loss: 3178511872.0000\n",
      "Epoch 22/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 2920179456.0000 - val_loss: 3095364864.0000\n",
      "Epoch 23/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 2863456000.0000 - val_loss: 3206745856.0000\n",
      "Epoch 24/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 2875059712.0000 - val_loss: 3248487424.0000\n",
      "Epoch 25/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 2839883264.0000 - val_loss: 3039175680.0000\n",
      "Epoch 26/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 2824758528.0000 - val_loss: 3101970944.0000\n",
      "Epoch 27/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 2789011712.0000 - val_loss: 3071692032.0000\n",
      "Epoch 28/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 2783205376.0000 - val_loss: 2996182016.0000\n",
      "Epoch 29/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 2750326528.0000 - val_loss: 3172290560.0000\n",
      "Epoch 30/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 2743734784.0000 - val_loss: 3041217280.0000\n",
      "Epoch 31/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 2706004480.0000 - val_loss: 3039462912.0000\n",
      "Epoch 32/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 2661665792.0000 - val_loss: 3052578816.0000\n",
      "Epoch 33/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 2676235264.0000 - val_loss: 3090954240.0000\n",
      "Epoch 34/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 2640434176.0000 - val_loss: 3086681600.0000\n",
      "Epoch 35/200\n",
      "1097/1097 [==============================] - 3s 2ms/step - loss: 2625109248.0000 - val_loss: 2983157760.0000\n",
      "Epoch 36/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 2639324928.0000 - val_loss: 3156383744.0000\n",
      "Epoch 37/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 2602478592.0000 - val_loss: 3274106368.0000\n",
      "Epoch 38/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 2598873856.0000 - val_loss: 3307678464.0000\n",
      "Epoch 39/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 2555218176.0000 - val_loss: 3277613568.0000\n",
      "Epoch 40/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 2559795712.0000 - val_loss: 3263317248.0000\n",
      "Epoch 41/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 2539092480.0000 - val_loss: 3229114112.0000\n",
      "Epoch 42/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 2523001344.0000 - val_loss: 3777432064.0000\n",
      "Epoch 43/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 2502582784.0000 - val_loss: 3335283456.0000\n",
      "Epoch 44/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 2491115776.0000 - val_loss: 3455838720.0000\n",
      "Epoch 45/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 2490001664.0000 - val_loss: 3271151360.0000\n",
      "Epoch 46/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 2461501696.0000 - val_loss: 3810378752.0000\n",
      "Epoch 47/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 2428511488.0000 - val_loss: 3683561472.0000\n",
      "Epoch 48/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 2432071936.0000 - val_loss: 3393684736.0000\n",
      "Epoch 49/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 2394038272.0000 - val_loss: 3328268032.0000\n",
      "Epoch 50/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 2401324032.0000 - val_loss: 3457646592.0000\n",
      "Epoch 51/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 2380150272.0000 - val_loss: 3476307456.0000\n",
      "Epoch 52/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 2365762304.0000 - val_loss: 3964620032.0000\n",
      "Epoch 53/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 2361664768.0000 - val_loss: 3488990976.0000\n",
      "Epoch 54/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 2351101696.0000 - val_loss: 3707498240.0000\n",
      "Epoch 55/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 2340166656.0000 - val_loss: 3432607232.0000\n",
      "Epoch 56/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 2311728128.0000 - val_loss: 3450728448.0000\n",
      "Epoch 57/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 2314966016.0000 - val_loss: 4575746560.0000\n",
      "Epoch 58/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 2295220224.0000 - val_loss: 4190136832.0000\n",
      "Epoch 59/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 2289652224.0000 - val_loss: 4309544960.0000\n",
      "Epoch 60/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 2265275136.0000 - val_loss: 3651315456.0000\n",
      "Epoch 61/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 2256524544.0000 - val_loss: 3741168384.0000\n",
      "Epoch 62/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 2234411008.0000 - val_loss: 3998390528.0000\n",
      "Epoch 63/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 2234044672.0000 - val_loss: 3598111232.0000\n",
      "Epoch 64/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 2226985472.0000 - val_loss: 3817209088.0000\n",
      "Epoch 65/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 2222707968.0000 - val_loss: 3899311616.0000\n",
      "Epoch 66/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 2199356672.0000 - val_loss: 3990510848.0000\n",
      "Epoch 67/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 2192540928.0000 - val_loss: 4138822400.0000\n",
      "Epoch 68/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 2176165888.0000 - val_loss: 3658669056.0000\n",
      "Epoch 69/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 2173523712.0000 - val_loss: 3691287552.0000\n",
      "Epoch 70/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 2180738048.0000 - val_loss: 3726332672.0000\n",
      "Epoch 71/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 2158356736.0000 - val_loss: 3786102528.0000\n",
      "Epoch 72/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 2138431616.0000 - val_loss: 3950446080.0000\n",
      "Epoch 73/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 2152183296.0000 - val_loss: 3926479616.0000\n",
      "Epoch 74/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 2141935488.0000 - val_loss: 4226628352.0000\n",
      "Epoch 75/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 2104877952.0000 - val_loss: 3611455232.0000\n",
      "Epoch 76/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 2116039808.0000 - val_loss: 3739833088.0000\n",
      "Epoch 77/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 2110420608.0000 - val_loss: 3783766784.0000\n",
      "Epoch 78/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 2120799488.0000 - val_loss: 3836182784.0000\n",
      "Epoch 79/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 2085575552.0000 - val_loss: 4137392384.0000\n",
      "Epoch 80/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 2088809728.0000 - val_loss: 4003775488.0000\n",
      "Epoch 81/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 2068301312.0000 - val_loss: 3961360896.0000\n",
      "Epoch 82/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 2069134720.0000 - val_loss: 4214802944.0000\n",
      "Epoch 83/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 2070908160.0000 - val_loss: 4240662528.0000\n",
      "Epoch 84/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 2043557248.0000 - val_loss: 3858426112.0000\n",
      "Epoch 85/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 2044642688.0000 - val_loss: 3983790080.0000\n",
      "Epoch 86/200\n",
      "1097/1097 [==============================] - 3s 2ms/step - loss: 2004775936.0000 - val_loss: 4623519744.0000\n",
      "Epoch 87/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 2026536192.0000 - val_loss: 3809633536.0000\n",
      "Epoch 88/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 2012541824.0000 - val_loss: 4269359104.0000\n",
      "Epoch 89/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 1998035456.0000 - val_loss: 4021706496.0000\n",
      "Epoch 90/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 1998597248.0000 - val_loss: 4376428544.0000\n",
      "Epoch 91/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 1969304320.0000 - val_loss: 3925812992.0000\n",
      "Epoch 92/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 1983706752.0000 - val_loss: 4098553344.0000\n",
      "Epoch 93/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 1954937600.0000 - val_loss: 4104128768.0000\n",
      "Epoch 94/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 1971360000.0000 - val_loss: 3921322752.0000\n",
      "Epoch 95/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 1954629760.0000 - val_loss: 3975875328.0000\n",
      "Epoch 96/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 1960007936.0000 - val_loss: 4060561664.0000\n",
      "Epoch 97/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 1936404480.0000 - val_loss: 4274555648.0000\n",
      "Epoch 98/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 1905605248.0000 - val_loss: 4415654912.0000\n",
      "Epoch 99/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 1929237888.0000 - val_loss: 4130804224.0000\n",
      "Epoch 100/200\n",
      "1097/1097 [==============================] - 3s 2ms/step - loss: 1890263040.0000 - val_loss: 4373400576.0000\n",
      "Epoch 101/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 1875075840.0000 - val_loss: 4306549760.0000\n",
      "Epoch 102/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 1882255488.0000 - val_loss: 4150208256.0000\n",
      "Epoch 103/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 1887340032.0000 - val_loss: 4230014464.0000\n",
      "Epoch 104/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 1881122944.0000 - val_loss: 4183712000.0000\n",
      "Epoch 105/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 1878207104.0000 - val_loss: 4109204992.0000\n",
      "Epoch 106/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 1843793664.0000 - val_loss: 4031476736.0000\n",
      "Epoch 107/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 1856860928.0000 - val_loss: 4625889792.0000\n",
      "Epoch 108/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 1844704384.0000 - val_loss: 4305170432.0000\n",
      "Epoch 109/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 1864538752.0000 - val_loss: 4239674112.0000\n",
      "Epoch 110/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 1826513664.0000 - val_loss: 4282625280.0000\n",
      "Epoch 111/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 1813622912.0000 - val_loss: 4360000512.0000\n",
      "Epoch 112/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 1796379776.0000 - val_loss: 4631589888.0000\n",
      "Epoch 113/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 1778713728.0000 - val_loss: 4261155840.0000\n",
      "Epoch 114/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 1786426368.0000 - val_loss: 4559191040.0000\n",
      "Epoch 115/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 1799334912.0000 - val_loss: 4450576896.0000\n",
      "Epoch 116/200\n",
      "1097/1097 [==============================] - 3s 2ms/step - loss: 1797726208.0000 - val_loss: 4399904768.0000\n",
      "Epoch 117/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 1772058752.0000 - val_loss: 4420509696.0000\n",
      "Epoch 118/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 1746158336.0000 - val_loss: 4883814912.0000\n",
      "Epoch 119/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 1768724480.0000 - val_loss: 4571859968.0000\n",
      "Epoch 120/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 1749248640.0000 - val_loss: 4421734912.0000\n",
      "Epoch 121/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 1759900928.0000 - val_loss: 4277624064.0000\n",
      "Epoch 122/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 1747096832.0000 - val_loss: 4412091904.0000\n",
      "Epoch 123/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 1720901888.0000 - val_loss: 4646209536.0000\n",
      "Epoch 124/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 1716610688.0000 - val_loss: 4423286272.0000\n",
      "Epoch 125/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 1726484096.0000 - val_loss: 4883307520.0000\n",
      "Epoch 126/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 1706048768.0000 - val_loss: 4843321344.0000\n",
      "Epoch 127/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 1686820608.0000 - val_loss: 4797938176.0000\n",
      "Epoch 128/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 1689216128.0000 - val_loss: 4399607808.0000\n",
      "Epoch 129/200\n",
      "1097/1097 [==============================] - 3s 2ms/step - loss: 1680594816.0000 - val_loss: 4412650496.0000\n",
      "Epoch 130/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 1699727360.0000 - val_loss: 4701659136.0000\n",
      "Epoch 131/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 1674792832.0000 - val_loss: 4853144064.0000\n",
      "Epoch 132/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 1673143552.0000 - val_loss: 4574667264.0000\n",
      "Epoch 133/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 1665969792.0000 - val_loss: 4580002816.0000\n",
      "Epoch 134/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 1663903488.0000 - val_loss: 4648859136.0000\n",
      "Epoch 135/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 1645662592.0000 - val_loss: 4843598336.0000\n",
      "Epoch 136/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 1645758208.0000 - val_loss: 4309867520.0000\n",
      "Epoch 137/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 1637874432.0000 - val_loss: 4416139264.0000\n",
      "Epoch 138/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 1607050368.0000 - val_loss: 4329773568.0000\n",
      "Epoch 139/200\n",
      "1097/1097 [==============================] - 3s 2ms/step - loss: 1630920576.0000 - val_loss: 4772315648.0000\n",
      "Epoch 140/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 1608517376.0000 - val_loss: 4775120384.0000\n",
      "Epoch 141/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 1609557760.0000 - val_loss: 4535097856.0000\n",
      "Epoch 142/200\n",
      "1097/1097 [==============================] - 3s 2ms/step - loss: 1600244864.0000 - val_loss: 4952810496.0000\n",
      "Epoch 143/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 1592460672.0000 - val_loss: 4713879040.0000\n",
      "Epoch 144/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 1591493888.0000 - val_loss: 4491613184.0000\n",
      "Epoch 145/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 1610837120.0000 - val_loss: 4428690432.0000\n",
      "Epoch 146/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 1605036160.0000 - val_loss: 4121884928.0000\n",
      "Epoch 147/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 1551591424.0000 - val_loss: 4562081280.0000\n",
      "Epoch 148/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 1580587392.0000 - val_loss: 4587465216.0000\n",
      "Epoch 149/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 1549371392.0000 - val_loss: 4666170880.0000\n",
      "Epoch 150/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 1539766016.0000 - val_loss: 4504086528.0000\n",
      "Epoch 151/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 1527525504.0000 - val_loss: 4664553984.0000\n",
      "Epoch 152/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 1543157120.0000 - val_loss: 4667927552.0000\n",
      "Epoch 153/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 1522028800.0000 - val_loss: 4616068608.0000\n",
      "Epoch 154/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 1536559616.0000 - val_loss: 4533177856.0000\n",
      "Epoch 155/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 1510609152.0000 - val_loss: 4567352832.0000\n",
      "Epoch 156/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 1517818112.0000 - val_loss: 4238127360.0000\n",
      "Epoch 157/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 1517115136.0000 - val_loss: 4628049920.0000\n",
      "Epoch 158/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 1492877824.0000 - val_loss: 4460121600.0000\n",
      "Epoch 159/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 1475523584.0000 - val_loss: 4902117376.0000\n",
      "Epoch 160/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 1491905920.0000 - val_loss: 4217246976.0000\n",
      "Epoch 161/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 1497537792.0000 - val_loss: 4592332800.0000\n",
      "Epoch 162/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 1472429824.0000 - val_loss: 4342552576.0000\n",
      "Epoch 163/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 1471656960.0000 - val_loss: 4573511168.0000\n",
      "Epoch 164/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 1452448256.0000 - val_loss: 4399805952.0000\n",
      "Epoch 165/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 1482168448.0000 - val_loss: 4392509952.0000\n",
      "Epoch 166/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 1443103616.0000 - val_loss: 4388293632.0000\n",
      "Epoch 167/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 1459063808.0000 - val_loss: 4368887296.0000\n",
      "Epoch 168/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 1428376576.0000 - val_loss: 4456995840.0000\n",
      "Epoch 169/200\n",
      "1097/1097 [==============================] - 3s 2ms/step - loss: 1413450880.0000 - val_loss: 4736409088.0000\n",
      "Epoch 170/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 1420209664.0000 - val_loss: 4642998272.0000\n",
      "Epoch 171/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 1418883072.0000 - val_loss: 4526722560.0000\n",
      "Epoch 172/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 1435085568.0000 - val_loss: 4542848000.0000\n",
      "Epoch 173/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 1395619200.0000 - val_loss: 4297772544.0000\n",
      "Epoch 174/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 1415086464.0000 - val_loss: 4367505408.0000\n",
      "Epoch 175/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 1398600960.0000 - val_loss: 4557779456.0000\n",
      "Epoch 176/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 1378284544.0000 - val_loss: 4508151296.0000\n",
      "Epoch 177/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 1404220928.0000 - val_loss: 4473559040.0000\n",
      "Epoch 178/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 1381052288.0000 - val_loss: 4342593024.0000\n",
      "Epoch 179/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 1380323712.0000 - val_loss: 4484207104.0000\n",
      "Epoch 180/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 1360400512.0000 - val_loss: 4744915968.0000\n",
      "Epoch 181/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 1375979776.0000 - val_loss: 4553099776.0000\n",
      "Epoch 182/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 1354832256.0000 - val_loss: 4395774976.0000\n",
      "Epoch 183/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 1360107264.0000 - val_loss: 4685385216.0000\n",
      "Epoch 184/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 1336467584.0000 - val_loss: 4458892288.0000\n",
      "Epoch 185/200\n",
      "1097/1097 [==============================] - 3s 2ms/step - loss: 1350137472.0000 - val_loss: 4400615936.0000\n",
      "Epoch 186/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 1338742144.0000 - val_loss: 4404944896.0000\n",
      "Epoch 187/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 1331506816.0000 - val_loss: 4614828544.0000\n",
      "Epoch 188/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 1346131200.0000 - val_loss: 4693208064.0000\n",
      "Epoch 189/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 1310427264.0000 - val_loss: 4894653952.0000\n",
      "Epoch 190/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 1321821696.0000 - val_loss: 4443127296.0000\n",
      "Epoch 191/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 1312318208.0000 - val_loss: 4601549312.0000\n",
      "Epoch 192/200\n",
      "1097/1097 [==============================] - 3s 2ms/step - loss: 1308493312.0000 - val_loss: 4512707584.0000\n",
      "Epoch 193/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 1312721792.0000 - val_loss: 4542803968.0000\n",
      "Epoch 194/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 1292234624.0000 - val_loss: 4546836480.0000\n",
      "Epoch 195/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 1278753536.0000 - val_loss: 4756931072.0000\n",
      "Epoch 196/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 1288745856.0000 - val_loss: 4600727552.0000\n",
      "Epoch 197/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 1287660416.0000 - val_loss: 4719810048.0000\n",
      "Epoch 198/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 1280258560.0000 - val_loss: 4899217408.0000\n",
      "Epoch 199/200\n",
      "1097/1097 [==============================] - 2s 2ms/step - loss: 1275782528.0000 - val_loss: 4638800384.0000\n",
      "Epoch 200/200\n",
      "1097/1097 [==============================] - 3s 2ms/step - loss: 1265144448.0000 - val_loss: 4726069248.0000\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_83\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_798 (Dense)           (None, 64)                896       \n",
      "                                                                 \n",
      " dense_799 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_800 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_801 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_802 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_803 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_804 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_805 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_806 (Dense)           (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 30,081\n",
      "Trainable params: 30,081\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 12557889536.0000 - val_loss: 3697672448.0000\n",
      "Epoch 2/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4528478208.0000 - val_loss: 3389339136.0000\n",
      "Epoch 3/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4202723584.0000 - val_loss: 3308553216.0000\n",
      "Epoch 4/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3957412608.0000 - val_loss: 3379048960.0000\n",
      "Epoch 5/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3825446144.0000 - val_loss: 3280989184.0000\n",
      "Epoch 6/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3713457664.0000 - val_loss: 3574827264.0000\n",
      "Epoch 7/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3695789824.0000 - val_loss: 3529154304.0000\n",
      "Epoch 8/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3623845888.0000 - val_loss: 3312336384.0000\n",
      "Epoch 9/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3575429632.0000 - val_loss: 3480711424.0000\n",
      "Epoch 10/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3550134272.0000 - val_loss: 3778212096.0000\n",
      "Epoch 11/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3520719872.0000 - val_loss: 3481009920.0000\n",
      "Epoch 12/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3482146304.0000 - val_loss: 3619852288.0000\n",
      "Epoch 13/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3413114624.0000 - val_loss: 3480320000.0000\n",
      "Epoch 14/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3393126656.0000 - val_loss: 3459404032.0000\n",
      "Epoch 15/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3349471744.0000 - val_loss: 3527977728.0000\n",
      "Epoch 16/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3308579072.0000 - val_loss: 3463248128.0000\n",
      "Epoch 17/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3265932288.0000 - val_loss: 3478746368.0000\n",
      "Epoch 18/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3211909632.0000 - val_loss: 3580603648.0000\n",
      "Epoch 19/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3202082560.0000 - val_loss: 3527870208.0000\n",
      "Epoch 20/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3170595072.0000 - val_loss: 3410961152.0000\n",
      "Epoch 21/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3147824128.0000 - val_loss: 3269108736.0000\n",
      "Epoch 22/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3124166912.0000 - val_loss: 3204875264.0000\n",
      "Epoch 23/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3070687232.0000 - val_loss: 3542271232.0000\n",
      "Epoch 24/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3052813312.0000 - val_loss: 3269476864.0000\n",
      "Epoch 25/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3030784256.0000 - val_loss: 3222132224.0000\n",
      "Epoch 26/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2981434112.0000 - val_loss: 3330184960.0000\n",
      "Epoch 27/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2959887616.0000 - val_loss: 3408983552.0000\n",
      "Epoch 28/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2964387072.0000 - val_loss: 3100001792.0000\n",
      "Epoch 29/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2920799744.0000 - val_loss: 3083581952.0000\n",
      "Epoch 30/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2910818560.0000 - val_loss: 3068209152.0000\n",
      "Epoch 31/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2892419072.0000 - val_loss: 2887830784.0000\n",
      "Epoch 32/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2840213504.0000 - val_loss: 2969822208.0000\n",
      "Epoch 33/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2871948800.0000 - val_loss: 3027530240.0000\n",
      "Epoch 34/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2826061312.0000 - val_loss: 3088948736.0000\n",
      "Epoch 35/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2805245440.0000 - val_loss: 2921540608.0000\n",
      "Epoch 36/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2823396352.0000 - val_loss: 2935742976.0000\n",
      "Epoch 37/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2776606464.0000 - val_loss: 3234834944.0000\n",
      "Epoch 38/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2771848448.0000 - val_loss: 3037154304.0000\n",
      "Epoch 39/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2761400064.0000 - val_loss: 3040059904.0000\n",
      "Epoch 40/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2755213056.0000 - val_loss: 2953137408.0000\n",
      "Epoch 41/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2722328576.0000 - val_loss: 2941158400.0000\n",
      "Epoch 42/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2717822208.0000 - val_loss: 3119179776.0000\n",
      "Epoch 43/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2711713280.0000 - val_loss: 2889962496.0000\n",
      "Epoch 44/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2708334336.0000 - val_loss: 2967217152.0000\n",
      "Epoch 45/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2689121536.0000 - val_loss: 2965677312.0000\n",
      "Epoch 46/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2675070208.0000 - val_loss: 2961047808.0000\n",
      "Epoch 47/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2660881152.0000 - val_loss: 3022682112.0000\n",
      "Epoch 48/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2658064128.0000 - val_loss: 3161844992.0000\n",
      "Epoch 49/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2619944192.0000 - val_loss: 2986365440.0000\n",
      "Epoch 50/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2642044928.0000 - val_loss: 2849054208.0000\n",
      "Epoch 51/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2595761408.0000 - val_loss: 2934782464.0000\n",
      "Epoch 52/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2590974720.0000 - val_loss: 3452187136.0000\n",
      "Epoch 53/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2592978688.0000 - val_loss: 2989207296.0000\n",
      "Epoch 54/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2562789632.0000 - val_loss: 3075402496.0000\n",
      "Epoch 55/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2564840704.0000 - val_loss: 2990322688.0000\n",
      "Epoch 56/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2541359616.0000 - val_loss: 3027899648.0000\n",
      "Epoch 57/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2569622784.0000 - val_loss: 3598797568.0000\n",
      "Epoch 58/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2522608128.0000 - val_loss: 3442548736.0000\n",
      "Epoch 59/200\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 2516607232.0000 - val_loss: 3083143424.0000\n",
      "Epoch 60/200\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 2495257344.0000 - val_loss: 3068043520.0000\n",
      "Epoch 61/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2508360704.0000 - val_loss: 3094775552.0000\n",
      "Epoch 62/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2488688128.0000 - val_loss: 3313600000.0000\n",
      "Epoch 63/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2480350976.0000 - val_loss: 3092152320.0000\n",
      "Epoch 64/200\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 2476441856.0000 - val_loss: 3164744960.0000\n",
      "Epoch 65/200\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 2455801344.0000 - val_loss: 3313672448.0000\n",
      "Epoch 66/200\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 2452394752.0000 - val_loss: 3305232384.0000\n",
      "Epoch 67/200\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 2440843264.0000 - val_loss: 3332317696.0000\n",
      "Epoch 68/200\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 2425108480.0000 - val_loss: 3389885696.0000\n",
      "Epoch 69/200\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 2409332736.0000 - val_loss: 3370629888.0000\n",
      "Epoch 70/200\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 2399324160.0000 - val_loss: 3190681344.0000\n",
      "Epoch 71/200\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 2390216448.0000 - val_loss: 3212211712.0000\n",
      "Epoch 72/200\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 2387124480.0000 - val_loss: 3392129536.0000\n",
      "Epoch 73/200\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 2391259648.0000 - val_loss: 3337833728.0000\n",
      "Epoch 74/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2362926848.0000 - val_loss: 3550403840.0000\n",
      "Epoch 75/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2342524160.0000 - val_loss: 3465712896.0000\n",
      "Epoch 76/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2348219904.0000 - val_loss: 3370072576.0000\n",
      "Epoch 77/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2335972352.0000 - val_loss: 3330234624.0000\n",
      "Epoch 78/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2331812096.0000 - val_loss: 3410998528.0000\n",
      "Epoch 79/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2311167488.0000 - val_loss: 3398198016.0000\n",
      "Epoch 80/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2311912448.0000 - val_loss: 3564569856.0000\n",
      "Epoch 81/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2299071744.0000 - val_loss: 3513283328.0000\n",
      "Epoch 82/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2307017472.0000 - val_loss: 3467737344.0000\n",
      "Epoch 83/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2283046144.0000 - val_loss: 3566281728.0000\n",
      "Epoch 84/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2300840448.0000 - val_loss: 3632257024.0000\n",
      "Epoch 85/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2268046336.0000 - val_loss: 3709735680.0000\n",
      "Epoch 86/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2239699968.0000 - val_loss: 3691076608.0000\n",
      "Epoch 87/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2265406976.0000 - val_loss: 3486144000.0000\n",
      "Epoch 88/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2242221824.0000 - val_loss: 3702917120.0000\n",
      "Epoch 89/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2252496128.0000 - val_loss: 3514842368.0000\n",
      "Epoch 90/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2229152000.0000 - val_loss: 3739795200.0000\n",
      "Epoch 91/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2216169216.0000 - val_loss: 3721592576.0000\n",
      "Epoch 92/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2215618304.0000 - val_loss: 3544918528.0000\n",
      "Epoch 93/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2183311872.0000 - val_loss: 3616348928.0000\n",
      "Epoch 94/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2216349440.0000 - val_loss: 3475507968.0000\n",
      "Epoch 95/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2173542912.0000 - val_loss: 3701992704.0000\n",
      "Epoch 96/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2192183040.0000 - val_loss: 3647087104.0000\n",
      "Epoch 97/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2158454528.0000 - val_loss: 3730697472.0000\n",
      "Epoch 98/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2157568256.0000 - val_loss: 3827514112.0000\n",
      "Epoch 99/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2165456384.0000 - val_loss: 3567911680.0000\n",
      "Epoch 100/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2147580160.0000 - val_loss: 3961690112.0000\n",
      "Epoch 101/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2135174016.0000 - val_loss: 3679836672.0000\n",
      "Epoch 102/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2147014144.0000 - val_loss: 3759040256.0000\n",
      "Epoch 103/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2154826240.0000 - val_loss: 3673779712.0000\n",
      "Epoch 104/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2113141504.0000 - val_loss: 3679947008.0000\n",
      "Epoch 105/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2127327360.0000 - val_loss: 3634022656.0000\n",
      "Epoch 106/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2108701696.0000 - val_loss: 3713202688.0000\n",
      "Epoch 107/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2114350208.0000 - val_loss: 3892719616.0000\n",
      "Epoch 108/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2106370688.0000 - val_loss: 3862200064.0000\n",
      "Epoch 109/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2106104576.0000 - val_loss: 3797202944.0000\n",
      "Epoch 110/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2069725440.0000 - val_loss: 3708553984.0000\n",
      "Epoch 111/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2106882944.0000 - val_loss: 3806601728.0000\n",
      "Epoch 112/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2058854528.0000 - val_loss: 3894857984.0000\n",
      "Epoch 113/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2047385472.0000 - val_loss: 3815201024.0000\n",
      "Epoch 114/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2052138496.0000 - val_loss: 3813967616.0000\n",
      "Epoch 115/200\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 2071297920.0000 - val_loss: 3860882176.0000\n",
      "Epoch 116/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2030714752.0000 - val_loss: 3870597888.0000\n",
      "Epoch 117/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2028324736.0000 - val_loss: 3937687808.0000\n",
      "Epoch 118/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2026955264.0000 - val_loss: 4148482560.0000\n",
      "Epoch 119/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2027376768.0000 - val_loss: 3875259392.0000\n",
      "Epoch 120/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2018776064.0000 - val_loss: 3950618112.0000\n",
      "Epoch 121/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2028892416.0000 - val_loss: 3992250880.0000\n",
      "Epoch 122/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2006405760.0000 - val_loss: 3957033216.0000\n",
      "Epoch 123/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2003786240.0000 - val_loss: 4002367232.0000\n",
      "Epoch 124/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 1986915584.0000 - val_loss: 3969132032.0000\n",
      "Epoch 125/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 1993177344.0000 - val_loss: 4148318976.0000\n",
      "Epoch 126/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 1959723136.0000 - val_loss: 3904024320.0000\n",
      "Epoch 127/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 1980925696.0000 - val_loss: 3967666176.0000\n",
      "Epoch 128/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 1976905216.0000 - val_loss: 4118478592.0000\n",
      "Epoch 129/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 1953000576.0000 - val_loss: 3816011008.0000\n",
      "Epoch 130/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 1955548416.0000 - val_loss: 4088450048.0000\n",
      "Epoch 131/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 1938694656.0000 - val_loss: 4200057344.0000\n",
      "Epoch 132/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 1934834560.0000 - val_loss: 4245642240.0000\n",
      "Epoch 133/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 1963648768.0000 - val_loss: 4227531264.0000\n",
      "Epoch 134/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 1938212992.0000 - val_loss: 4424140800.0000\n",
      "Epoch 135/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 1925982848.0000 - val_loss: 4311006720.0000\n",
      "Epoch 136/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 1916384512.0000 - val_loss: 3907759616.0000\n",
      "Epoch 137/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 1911165184.0000 - val_loss: 4078919424.0000\n",
      "Epoch 138/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 1909518080.0000 - val_loss: 4085957632.0000\n",
      "Epoch 139/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 1899742464.0000 - val_loss: 4456064000.0000\n",
      "Epoch 140/200\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 1893602048.0000 - val_loss: 4334051840.0000\n",
      "Epoch 141/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 1891527936.0000 - val_loss: 4204912128.0000\n",
      "Epoch 142/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 1862169856.0000 - val_loss: 4558758400.0000\n",
      "Epoch 143/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 1889376768.0000 - val_loss: 4205191680.0000\n",
      "Epoch 144/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 1889962880.0000 - val_loss: 4094251008.0000\n",
      "Epoch 145/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 1888615552.0000 - val_loss: 4239885824.0000\n",
      "Epoch 146/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 1858991744.0000 - val_loss: 4049866752.0000\n",
      "Epoch 147/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 1834270592.0000 - val_loss: 4294926080.0000\n",
      "Epoch 148/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 1844684672.0000 - val_loss: 4502565888.0000\n",
      "Epoch 149/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 1856658944.0000 - val_loss: 4336555520.0000\n",
      "Epoch 150/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 1827392768.0000 - val_loss: 4110480128.0000\n",
      "Epoch 151/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 1845481600.0000 - val_loss: 4173353216.0000\n",
      "Epoch 152/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 1839011968.0000 - val_loss: 4534751232.0000\n",
      "Epoch 153/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 1805343488.0000 - val_loss: 4203455744.0000\n",
      "Epoch 154/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 1818669056.0000 - val_loss: 4509511168.0000\n",
      "Epoch 155/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 1793113088.0000 - val_loss: 4278294784.0000\n",
      "Epoch 156/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 1807445376.0000 - val_loss: 4207405824.0000\n",
      "Epoch 157/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 1809669248.0000 - val_loss: 4438532096.0000\n",
      "Epoch 158/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 1797059840.0000 - val_loss: 4325066240.0000\n",
      "Epoch 159/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 1791350528.0000 - val_loss: 4550291968.0000\n",
      "Epoch 160/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 1785366784.0000 - val_loss: 4260824320.0000\n",
      "Epoch 161/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 1789601536.0000 - val_loss: 4527750656.0000\n",
      "Epoch 162/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 1776301312.0000 - val_loss: 4430913536.0000\n",
      "Epoch 163/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 1763097344.0000 - val_loss: 4505331200.0000\n",
      "Epoch 164/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 1760470400.0000 - val_loss: 4218198272.0000\n",
      "Epoch 165/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 1763599488.0000 - val_loss: 4346312704.0000\n",
      "Epoch 166/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 1755094144.0000 - val_loss: 4327051264.0000\n",
      "Epoch 167/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 1743578496.0000 - val_loss: 4668689920.0000\n",
      "Epoch 168/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 1751541888.0000 - val_loss: 4546734080.0000\n",
      "Epoch 169/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 1707758464.0000 - val_loss: 4727660544.0000\n",
      "Epoch 170/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 1734634112.0000 - val_loss: 4338921472.0000\n",
      "Epoch 171/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 1718438016.0000 - val_loss: 4498554880.0000\n",
      "Epoch 172/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 1706833536.0000 - val_loss: 4449438720.0000\n",
      "Epoch 173/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 1701111424.0000 - val_loss: 4472685568.0000\n",
      "Epoch 174/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 1698669568.0000 - val_loss: 4529839104.0000\n",
      "Epoch 175/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 1709073408.0000 - val_loss: 4858887168.0000\n",
      "Epoch 176/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 1713868544.0000 - val_loss: 4978370048.0000\n",
      "Epoch 177/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 1684145536.0000 - val_loss: 4710363136.0000\n",
      "Epoch 178/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 1693947904.0000 - val_loss: 4481059328.0000\n",
      "Epoch 179/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 1680705536.0000 - val_loss: 4488629760.0000\n",
      "Epoch 180/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 1662182016.0000 - val_loss: 4520615936.0000\n",
      "Epoch 181/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 1691053056.0000 - val_loss: 5025316864.0000\n",
      "Epoch 182/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 1649421440.0000 - val_loss: 4393196544.0000\n",
      "Epoch 183/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 1660840064.0000 - val_loss: 4516317184.0000\n",
      "Epoch 184/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 1632927104.0000 - val_loss: 4506494464.0000\n",
      "Epoch 185/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 1680139904.0000 - val_loss: 4417315840.0000\n",
      "Epoch 186/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 1638262528.0000 - val_loss: 4526948352.0000\n",
      "Epoch 187/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 1618931584.0000 - val_loss: 4989288448.0000\n",
      "Epoch 188/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 1628230784.0000 - val_loss: 4657225728.0000\n",
      "Epoch 189/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 1631529216.0000 - val_loss: 4948566528.0000\n",
      "Epoch 190/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 1609085952.0000 - val_loss: 4485550592.0000\n",
      "Epoch 191/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 1604845568.0000 - val_loss: 4679791104.0000\n",
      "Epoch 192/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 1631238912.0000 - val_loss: 4616130048.0000\n",
      "Epoch 193/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 1621225856.0000 - val_loss: 4666958336.0000\n",
      "Epoch 194/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 1592238464.0000 - val_loss: 4414174208.0000\n",
      "Epoch 195/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 1586756736.0000 - val_loss: 4842669568.0000\n",
      "Epoch 196/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 1582785280.0000 - val_loss: 5033111552.0000\n",
      "Epoch 197/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 1577583360.0000 - val_loss: 4740923904.0000\n",
      "Epoch 198/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 1571816192.0000 - val_loss: 4899633152.0000\n",
      "Epoch 199/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 1552064768.0000 - val_loss: 4803577856.0000\n",
      "Epoch 200/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 1587153280.0000 - val_loss: 4745487360.0000\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_84\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_807 (Dense)           (None, 64)                896       \n",
      "                                                                 \n",
      " dense_808 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_809 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_810 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_811 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_812 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_813 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_814 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_815 (Dense)           (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 30,081\n",
      "Trainable params: 30,081\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "275/275 [==============================] - 1s 3ms/step - loss: 19939289088.0000 - val_loss: 3639592704.0000\n",
      "Epoch 2/200\n",
      "275/275 [==============================] - 1s 2ms/step - loss: 4745666560.0000 - val_loss: 3668362240.0000\n",
      "Epoch 3/200\n",
      "275/275 [==============================] - 1s 2ms/step - loss: 4448184832.0000 - val_loss: 3390063360.0000\n",
      "Epoch 4/200\n",
      "275/275 [==============================] - 1s 2ms/step - loss: 4205301760.0000 - val_loss: 3371964928.0000\n",
      "Epoch 5/200\n",
      "275/275 [==============================] - 1s 2ms/step - loss: 4040072960.0000 - val_loss: 3321089024.0000\n",
      "Epoch 6/200\n",
      "275/275 [==============================] - 1s 2ms/step - loss: 3907692800.0000 - val_loss: 3358583552.0000\n",
      "Epoch 7/200\n",
      "275/275 [==============================] - 1s 2ms/step - loss: 3839568640.0000 - val_loss: 3575636992.0000\n",
      "Epoch 8/200\n",
      "275/275 [==============================] - 1s 2ms/step - loss: 3731605248.0000 - val_loss: 3427723520.0000\n",
      "Epoch 9/200\n",
      "275/275 [==============================] - 1s 2ms/step - loss: 3711895552.0000 - val_loss: 3353977344.0000\n",
      "Epoch 10/200\n",
      "275/275 [==============================] - 1s 2ms/step - loss: 3652474112.0000 - val_loss: 3407303424.0000\n",
      "Epoch 11/200\n",
      "275/275 [==============================] - 1s 2ms/step - loss: 3614491648.0000 - val_loss: 3408777984.0000\n",
      "Epoch 12/200\n",
      "275/275 [==============================] - 1s 2ms/step - loss: 3631310336.0000 - val_loss: 3858371584.0000\n",
      "Epoch 13/200\n",
      "275/275 [==============================] - 1s 2ms/step - loss: 3575109376.0000 - val_loss: 3429639168.0000\n",
      "Epoch 14/200\n",
      "275/275 [==============================] - 1s 2ms/step - loss: 3511589120.0000 - val_loss: 3458296064.0000\n",
      "Epoch 15/200\n",
      "275/275 [==============================] - 1s 2ms/step - loss: 3508764928.0000 - val_loss: 3691057664.0000\n",
      "Epoch 16/200\n",
      "275/275 [==============================] - 1s 2ms/step - loss: 3479797504.0000 - val_loss: 3535872000.0000\n",
      "Epoch 17/200\n",
      "275/275 [==============================] - 1s 3ms/step - loss: 3433574656.0000 - val_loss: 3534509312.0000\n",
      "Epoch 18/200\n",
      "275/275 [==============================] - 1s 2ms/step - loss: 3420538368.0000 - val_loss: 3608064256.0000\n",
      "Epoch 19/200\n",
      "275/275 [==============================] - 1s 2ms/step - loss: 3399198720.0000 - val_loss: 3769401344.0000\n",
      "Epoch 20/200\n",
      "275/275 [==============================] - 1s 2ms/step - loss: 3361190144.0000 - val_loss: 3593947392.0000\n",
      "Epoch 21/200\n",
      "275/275 [==============================] - 1s 3ms/step - loss: 3363308800.0000 - val_loss: 3599905280.0000\n",
      "Epoch 22/200\n",
      "275/275 [==============================] - 1s 2ms/step - loss: 3343281408.0000 - val_loss: 3443039232.0000\n",
      "Epoch 23/200\n",
      "275/275 [==============================] - 1s 2ms/step - loss: 3320685056.0000 - val_loss: 3451560960.0000\n",
      "Epoch 24/200\n",
      "275/275 [==============================] - 1s 2ms/step - loss: 3279472128.0000 - val_loss: 3621656832.0000\n",
      "Epoch 25/200\n",
      "275/275 [==============================] - 1s 2ms/step - loss: 3261184000.0000 - val_loss: 3551015936.0000\n",
      "Epoch 26/200\n",
      "275/275 [==============================] - 1s 2ms/step - loss: 3228468736.0000 - val_loss: 3791574272.0000\n",
      "Epoch 27/200\n",
      "275/275 [==============================] - 1s 2ms/step - loss: 3210649600.0000 - val_loss: 3730326272.0000\n",
      "Epoch 28/200\n",
      "275/275 [==============================] - 1s 2ms/step - loss: 3190936832.0000 - val_loss: 3478756096.0000\n",
      "Epoch 29/200\n",
      "275/275 [==============================] - 1s 2ms/step - loss: 3180900608.0000 - val_loss: 3615801088.0000\n",
      "Epoch 30/200\n",
      "275/275 [==============================] - 1s 2ms/step - loss: 3154128128.0000 - val_loss: 3625969408.0000\n",
      "Epoch 31/200\n",
      "275/275 [==============================] - 1s 2ms/step - loss: 3117671424.0000 - val_loss: 3431165952.0000\n",
      "Epoch 32/200\n",
      "275/275 [==============================] - 1s 2ms/step - loss: 3119234560.0000 - val_loss: 3544663808.0000\n",
      "Epoch 33/200\n",
      "275/275 [==============================] - 1s 2ms/step - loss: 3083068416.0000 - val_loss: 3334957312.0000\n",
      "Epoch 34/200\n",
      "275/275 [==============================] - 1s 2ms/step - loss: 3074241536.0000 - val_loss: 3752451072.0000\n",
      "Epoch 35/200\n",
      "275/275 [==============================] - 1s 2ms/step - loss: 3055109888.0000 - val_loss: 3316288000.0000\n",
      "Epoch 36/200\n",
      "275/275 [==============================] - 1s 2ms/step - loss: 3044915456.0000 - val_loss: 3424717056.0000\n",
      "Epoch 37/200\n",
      "275/275 [==============================] - 1s 2ms/step - loss: 3029832704.0000 - val_loss: 3402567168.0000\n",
      "Epoch 38/200\n",
      "275/275 [==============================] - 1s 2ms/step - loss: 2976432128.0000 - val_loss: 3771409152.0000\n",
      "Epoch 39/200\n",
      "275/275 [==============================] - 1s 3ms/step - loss: 2996360960.0000 - val_loss: 3397584128.0000\n",
      "Epoch 40/200\n",
      "275/275 [==============================] - 1s 2ms/step - loss: 2996134144.0000 - val_loss: 3465704448.0000\n",
      "Epoch 41/200\n",
      "275/275 [==============================] - 1s 2ms/step - loss: 2964822016.0000 - val_loss: 3555241216.0000\n",
      "Epoch 42/200\n",
      "275/275 [==============================] - 1s 2ms/step - loss: 2932476928.0000 - val_loss: 3585162240.0000\n",
      "Epoch 43/200\n",
      "275/275 [==============================] - 1s 2ms/step - loss: 2932357376.0000 - val_loss: 3390639616.0000\n",
      "Epoch 44/200\n",
      "275/275 [==============================] - 1s 2ms/step - loss: 2945726464.0000 - val_loss: 3336069632.0000\n",
      "Epoch 45/200\n",
      "275/275 [==============================] - 1s 2ms/step - loss: 2909579520.0000 - val_loss: 3280396544.0000\n",
      "Epoch 46/200\n",
      "275/275 [==============================] - 1s 2ms/step - loss: 2902557952.0000 - val_loss: 3305614592.0000\n",
      "Epoch 47/200\n",
      "275/275 [==============================] - 1s 2ms/step - loss: 2907714304.0000 - val_loss: 3681038592.0000\n",
      "Epoch 48/200\n",
      "275/275 [==============================] - 1s 2ms/step - loss: 2900612608.0000 - val_loss: 3486597120.0000\n",
      "Epoch 49/200\n",
      "275/275 [==============================] - 1s 2ms/step - loss: 2860925696.0000 - val_loss: 3363411456.0000\n",
      "Epoch 50/200\n",
      "275/275 [==============================] - 1s 2ms/step - loss: 2870184960.0000 - val_loss: 3391011072.0000\n",
      "Epoch 51/200\n",
      "275/275 [==============================] - 1s 3ms/step - loss: 2854948096.0000 - val_loss: 3176099328.0000\n",
      "Epoch 52/200\n",
      "275/275 [==============================] - 1s 3ms/step - loss: 2841234176.0000 - val_loss: 3178808576.0000\n",
      "Epoch 53/200\n",
      "275/275 [==============================] - 1s 3ms/step - loss: 2852997376.0000 - val_loss: 3251637504.0000\n",
      "Epoch 54/200\n",
      "275/275 [==============================] - 1s 2ms/step - loss: 2833307136.0000 - val_loss: 3256679936.0000\n",
      "Epoch 55/200\n",
      "275/275 [==============================] - 1s 3ms/step - loss: 2816136192.0000 - val_loss: 3380066816.0000\n",
      "Epoch 56/200\n",
      "275/275 [==============================] - 1s 3ms/step - loss: 2823778304.0000 - val_loss: 3292345856.0000\n",
      "Epoch 57/200\n",
      "275/275 [==============================] - 1s 3ms/step - loss: 2808062464.0000 - val_loss: 3315446272.0000\n",
      "Epoch 58/200\n",
      "275/275 [==============================] - 1s 4ms/step - loss: 2780838400.0000 - val_loss: 3416460800.0000\n",
      "Epoch 59/200\n",
      "275/275 [==============================] - 1s 3ms/step - loss: 2761693952.0000 - val_loss: 3246983424.0000\n",
      "Epoch 60/200\n",
      "275/275 [==============================] - 1s 3ms/step - loss: 2744434944.0000 - val_loss: 3258904064.0000\n",
      "Epoch 61/200\n",
      "275/275 [==============================] - 1s 3ms/step - loss: 2779111424.0000 - val_loss: 3204939264.0000\n",
      "Epoch 62/200\n",
      "275/275 [==============================] - 1s 3ms/step - loss: 2768769792.0000 - val_loss: 3437505536.0000\n",
      "Epoch 63/200\n",
      "275/275 [==============================] - 1s 3ms/step - loss: 2762860032.0000 - val_loss: 3461506816.0000\n",
      "Epoch 64/200\n",
      "275/275 [==============================] - 1s 3ms/step - loss: 2738170880.0000 - val_loss: 3300975104.0000\n",
      "Epoch 65/200\n",
      "275/275 [==============================] - 1s 3ms/step - loss: 2729756416.0000 - val_loss: 3487547392.0000\n",
      "Epoch 66/200\n",
      "275/275 [==============================] - 1s 3ms/step - loss: 2717884672.0000 - val_loss: 3275352576.0000\n",
      "Epoch 67/200\n",
      "275/275 [==============================] - 1s 2ms/step - loss: 2775118336.0000 - val_loss: 3222082304.0000\n",
      "Epoch 68/200\n",
      "275/275 [==============================] - 1s 2ms/step - loss: 2702378496.0000 - val_loss: 3179205120.0000\n",
      "Epoch 69/200\n",
      "275/275 [==============================] - 1s 3ms/step - loss: 2710224128.0000 - val_loss: 3231598336.0000\n",
      "Epoch 70/200\n",
      "275/275 [==============================] - 1s 3ms/step - loss: 2701133568.0000 - val_loss: 3171080448.0000\n",
      "Epoch 71/200\n",
      "275/275 [==============================] - 1s 3ms/step - loss: 2703666688.0000 - val_loss: 3275642368.0000\n",
      "Epoch 72/200\n",
      "275/275 [==============================] - 1s 3ms/step - loss: 2680688128.0000 - val_loss: 3285375232.0000\n",
      "Epoch 73/200\n",
      "275/275 [==============================] - 1s 3ms/step - loss: 2674664704.0000 - val_loss: 3482831872.0000\n",
      "Epoch 74/200\n",
      "275/275 [==============================] - 1s 2ms/step - loss: 2672200192.0000 - val_loss: 3287673600.0000\n",
      "Epoch 75/200\n",
      "275/275 [==============================] - 1s 3ms/step - loss: 2647260160.0000 - val_loss: 3144751104.0000\n",
      "Epoch 76/200\n",
      "275/275 [==============================] - 1s 3ms/step - loss: 2687017216.0000 - val_loss: 3282797056.0000\n",
      "Epoch 77/200\n",
      "275/275 [==============================] - 1s 3ms/step - loss: 2661959168.0000 - val_loss: 3309659136.0000\n",
      "Epoch 78/200\n",
      "275/275 [==============================] - 1s 3ms/step - loss: 2664856576.0000 - val_loss: 3187181312.0000\n",
      "Epoch 79/200\n",
      "275/275 [==============================] - 1s 3ms/step - loss: 2640099328.0000 - val_loss: 3171406848.0000\n",
      "Epoch 80/200\n",
      "275/275 [==============================] - 1s 3ms/step - loss: 2638936320.0000 - val_loss: 3398643200.0000\n",
      "Epoch 81/200\n",
      "275/275 [==============================] - 1s 3ms/step - loss: 2635536640.0000 - val_loss: 3214845696.0000\n",
      "Epoch 82/200\n",
      "275/275 [==============================] - 1s 3ms/step - loss: 2631472896.0000 - val_loss: 3299884032.0000\n",
      "Epoch 83/200\n",
      "275/275 [==============================] - 1s 3ms/step - loss: 2614373120.0000 - val_loss: 3262589440.0000\n",
      "Epoch 84/200\n",
      "275/275 [==============================] - 1s 3ms/step - loss: 2616644096.0000 - val_loss: 3466075648.0000\n",
      "Epoch 85/200\n",
      "275/275 [==============================] - 1s 3ms/step - loss: 2606737408.0000 - val_loss: 3156562944.0000\n",
      "Epoch 86/200\n",
      "275/275 [==============================] - 1s 3ms/step - loss: 2588512768.0000 - val_loss: 3343483136.0000\n",
      "Epoch 87/200\n",
      "275/275 [==============================] - 1s 3ms/step - loss: 2594882560.0000 - val_loss: 3159727616.0000\n",
      "Epoch 88/200\n",
      "275/275 [==============================] - 1s 3ms/step - loss: 2593496064.0000 - val_loss: 3230071040.0000\n",
      "Epoch 89/200\n",
      "275/275 [==============================] - 1s 3ms/step - loss: 2570792960.0000 - val_loss: 3115685120.0000\n",
      "Epoch 90/200\n",
      "275/275 [==============================] - 1s 3ms/step - loss: 2563602432.0000 - val_loss: 3126013952.0000\n",
      "Epoch 91/200\n",
      "275/275 [==============================] - 1s 3ms/step - loss: 2571197440.0000 - val_loss: 3321336064.0000\n",
      "Epoch 92/200\n",
      "275/275 [==============================] - 1s 3ms/step - loss: 2569551104.0000 - val_loss: 3253741824.0000\n",
      "Epoch 93/200\n",
      "275/275 [==============================] - 1s 3ms/step - loss: 2560118528.0000 - val_loss: 3228385536.0000\n",
      "Epoch 94/200\n",
      "275/275 [==============================] - 1s 3ms/step - loss: 2547921664.0000 - val_loss: 3109615104.0000\n",
      "Epoch 95/200\n",
      "275/275 [==============================] - 1s 3ms/step - loss: 2526308864.0000 - val_loss: 3209850880.0000\n",
      "Epoch 96/200\n",
      "275/275 [==============================] - 1s 3ms/step - loss: 2541528064.0000 - val_loss: 3254219776.0000\n",
      "Epoch 97/200\n",
      "275/275 [==============================] - 1s 3ms/step - loss: 2519808000.0000 - val_loss: 3330938368.0000\n",
      "Epoch 98/200\n",
      "275/275 [==============================] - 1s 3ms/step - loss: 2570103296.0000 - val_loss: 3231909120.0000\n",
      "Epoch 99/200\n",
      "275/275 [==============================] - 1s 3ms/step - loss: 2532784896.0000 - val_loss: 3070391040.0000\n",
      "Epoch 100/200\n",
      "275/275 [==============================] - 1s 3ms/step - loss: 2542188032.0000 - val_loss: 3278142208.0000\n",
      "Epoch 101/200\n",
      "275/275 [==============================] - 1s 3ms/step - loss: 2495773952.0000 - val_loss: 3199189760.0000\n",
      "Epoch 102/200\n",
      "275/275 [==============================] - 1s 3ms/step - loss: 2520862208.0000 - val_loss: 3467490048.0000\n",
      "Epoch 103/200\n",
      "275/275 [==============================] - 1s 3ms/step - loss: 2507671296.0000 - val_loss: 3161310208.0000\n",
      "Epoch 104/200\n",
      "275/275 [==============================] - 1s 3ms/step - loss: 2493741056.0000 - val_loss: 3296446976.0000\n",
      "Epoch 105/200\n",
      "275/275 [==============================] - 1s 3ms/step - loss: 2482481152.0000 - val_loss: 3284381184.0000\n",
      "Epoch 106/200\n",
      "275/275 [==============================] - 1s 3ms/step - loss: 2484929792.0000 - val_loss: 3124888576.0000\n",
      "Epoch 107/200\n",
      "275/275 [==============================] - 1s 3ms/step - loss: 2474063872.0000 - val_loss: 3248423680.0000\n",
      "Epoch 108/200\n",
      "275/275 [==============================] - 1s 3ms/step - loss: 2506637312.0000 - val_loss: 3096727808.0000\n",
      "Epoch 109/200\n",
      "275/275 [==============================] - 1s 3ms/step - loss: 2481565440.0000 - val_loss: 3452289536.0000\n",
      "Epoch 110/200\n",
      "275/275 [==============================] - 1s 3ms/step - loss: 2485926912.0000 - val_loss: 3414222336.0000\n",
      "Epoch 111/200\n",
      "275/275 [==============================] - 1s 3ms/step - loss: 2473283072.0000 - val_loss: 3255292928.0000\n",
      "Epoch 112/200\n",
      "275/275 [==============================] - 1s 3ms/step - loss: 2441157632.0000 - val_loss: 3387332864.0000\n",
      "Epoch 113/200\n",
      "275/275 [==============================] - 1s 3ms/step - loss: 2430850816.0000 - val_loss: 3292005632.0000\n",
      "Epoch 114/200\n",
      "275/275 [==============================] - 1s 3ms/step - loss: 2441164544.0000 - val_loss: 3212160512.0000\n",
      "Epoch 115/200\n",
      "275/275 [==============================] - 1s 3ms/step - loss: 2426029312.0000 - val_loss: 3273879040.0000\n",
      "Epoch 116/200\n",
      "275/275 [==============================] - 1s 3ms/step - loss: 2429106944.0000 - val_loss: 3274790912.0000\n",
      "Epoch 117/200\n",
      "275/275 [==============================] - 1s 3ms/step - loss: 2410650624.0000 - val_loss: 3530066688.0000\n",
      "Epoch 118/200\n",
      "275/275 [==============================] - 1s 3ms/step - loss: 2449420800.0000 - val_loss: 3402301184.0000\n",
      "Epoch 119/200\n",
      "275/275 [==============================] - 1s 3ms/step - loss: 2435958528.0000 - val_loss: 3382678784.0000\n",
      "Epoch 120/200\n",
      "275/275 [==============================] - 1s 3ms/step - loss: 2410132736.0000 - val_loss: 3501223680.0000\n",
      "Epoch 121/200\n",
      "275/275 [==============================] - 1s 3ms/step - loss: 2405901056.0000 - val_loss: 3378623744.0000\n",
      "Epoch 122/200\n",
      "275/275 [==============================] - 1s 3ms/step - loss: 2398456832.0000 - val_loss: 3352669440.0000\n",
      "Epoch 123/200\n",
      "275/275 [==============================] - 1s 3ms/step - loss: 2370727168.0000 - val_loss: 3655971584.0000\n",
      "Epoch 124/200\n",
      "275/275 [==============================] - 1s 3ms/step - loss: 2378019072.0000 - val_loss: 3381094656.0000\n",
      "Epoch 125/200\n",
      "275/275 [==============================] - 1s 3ms/step - loss: 2389158400.0000 - val_loss: 3516719104.0000\n",
      "Epoch 126/200\n",
      "275/275 [==============================] - 1s 3ms/step - loss: 2368577280.0000 - val_loss: 3500510464.0000\n",
      "Epoch 127/200\n",
      "275/275 [==============================] - 1s 3ms/step - loss: 2365188608.0000 - val_loss: 3380661504.0000\n",
      "Epoch 128/200\n",
      "275/275 [==============================] - 1s 3ms/step - loss: 2370431488.0000 - val_loss: 3609812224.0000\n",
      "Epoch 129/200\n",
      "275/275 [==============================] - 1s 3ms/step - loss: 2342966272.0000 - val_loss: 3418460672.0000\n",
      "Epoch 130/200\n",
      "275/275 [==============================] - 1s 3ms/step - loss: 2347757056.0000 - val_loss: 3615715840.0000\n",
      "Epoch 131/200\n",
      "275/275 [==============================] - 1s 3ms/step - loss: 2350510848.0000 - val_loss: 3357035264.0000\n",
      "Epoch 132/200\n",
      "275/275 [==============================] - 1s 3ms/step - loss: 2340541952.0000 - val_loss: 3986162944.0000\n",
      "Epoch 133/200\n",
      "275/275 [==============================] - 1s 3ms/step - loss: 2347100160.0000 - val_loss: 3551424000.0000\n",
      "Epoch 134/200\n",
      "275/275 [==============================] - 1s 4ms/step - loss: 2355450624.0000 - val_loss: 3723378688.0000\n",
      "Epoch 135/200\n",
      "275/275 [==============================] - 1s 3ms/step - loss: 2330264832.0000 - val_loss: 3784100352.0000\n",
      "Epoch 136/200\n",
      "275/275 [==============================] - 1s 3ms/step - loss: 2321180928.0000 - val_loss: 3316864768.0000\n",
      "Epoch 137/200\n",
      "275/275 [==============================] - 1s 3ms/step - loss: 2313065216.0000 - val_loss: 3574307584.0000\n",
      "Epoch 138/200\n",
      "275/275 [==============================] - 1s 3ms/step - loss: 2330198528.0000 - val_loss: 3670478336.0000\n",
      "Epoch 139/200\n",
      "275/275 [==============================] - 1s 3ms/step - loss: 2328343296.0000 - val_loss: 3944346880.0000\n",
      "Epoch 140/200\n",
      "275/275 [==============================] - 1s 3ms/step - loss: 2283904768.0000 - val_loss: 3716430336.0000\n",
      "Epoch 141/200\n",
      "275/275 [==============================] - 1s 3ms/step - loss: 2300976896.0000 - val_loss: 3632648704.0000\n",
      "Epoch 142/200\n",
      "275/275 [==============================] - 1s 2ms/step - loss: 2304464896.0000 - val_loss: 3704260352.0000\n",
      "Epoch 143/200\n",
      "275/275 [==============================] - 1s 2ms/step - loss: 2273868800.0000 - val_loss: 3532716544.0000\n",
      "Epoch 144/200\n",
      "275/275 [==============================] - 1s 3ms/step - loss: 2286655232.0000 - val_loss: 3562497280.0000\n",
      "Epoch 145/200\n",
      "275/275 [==============================] - 1s 3ms/step - loss: 2286532352.0000 - val_loss: 3883755264.0000\n",
      "Epoch 146/200\n",
      "275/275 [==============================] - 1s 3ms/step - loss: 2298720000.0000 - val_loss: 3450722304.0000\n",
      "Epoch 147/200\n",
      "275/275 [==============================] - 1s 3ms/step - loss: 2257286912.0000 - val_loss: 3818176512.0000\n",
      "Epoch 148/200\n",
      "275/275 [==============================] - 1s 3ms/step - loss: 2270657792.0000 - val_loss: 3639132160.0000\n",
      "Epoch 149/200\n",
      "275/275 [==============================] - 1s 3ms/step - loss: 2270067712.0000 - val_loss: 3788621312.0000\n",
      "Epoch 150/200\n",
      "275/275 [==============================] - 1s 3ms/step - loss: 2264285952.0000 - val_loss: 3471442176.0000\n",
      "Epoch 151/200\n",
      "275/275 [==============================] - 1s 3ms/step - loss: 2277007872.0000 - val_loss: 3613036032.0000\n",
      "Epoch 152/200\n",
      "275/275 [==============================] - 1s 3ms/step - loss: 2248293888.0000 - val_loss: 3953932800.0000\n",
      "Epoch 153/200\n",
      "275/275 [==============================] - 1s 3ms/step - loss: 2244496896.0000 - val_loss: 3557140736.0000\n",
      "Epoch 154/200\n",
      "275/275 [==============================] - 1s 2ms/step - loss: 2227102208.0000 - val_loss: 4094948096.0000\n",
      "Epoch 155/200\n",
      "275/275 [==============================] - 1s 3ms/step - loss: 2276322304.0000 - val_loss: 3604081920.0000\n",
      "Epoch 156/200\n",
      "275/275 [==============================] - 1s 2ms/step - loss: 2227307264.0000 - val_loss: 3539869696.0000\n",
      "Epoch 157/200\n",
      "275/275 [==============================] - 1s 2ms/step - loss: 2243828224.0000 - val_loss: 3789848064.0000\n",
      "Epoch 158/200\n",
      "275/275 [==============================] - 1s 2ms/step - loss: 2231884800.0000 - val_loss: 3746060032.0000\n",
      "Epoch 159/200\n",
      "275/275 [==============================] - 1s 3ms/step - loss: 2227475200.0000 - val_loss: 3642402560.0000\n",
      "Epoch 160/200\n",
      "275/275 [==============================] - 1s 3ms/step - loss: 2232526336.0000 - val_loss: 3608466944.0000\n",
      "Epoch 161/200\n",
      "275/275 [==============================] - 1s 3ms/step - loss: 2230224640.0000 - val_loss: 3912641792.0000\n",
      "Epoch 162/200\n",
      "275/275 [==============================] - 1s 3ms/step - loss: 2215518208.0000 - val_loss: 3829368320.0000\n",
      "Epoch 163/200\n",
      "275/275 [==============================] - 1s 3ms/step - loss: 2196916480.0000 - val_loss: 3812516352.0000\n",
      "Epoch 164/200\n",
      "275/275 [==============================] - 1s 3ms/step - loss: 2207808768.0000 - val_loss: 3644776960.0000\n",
      "Epoch 165/200\n",
      "275/275 [==============================] - 1s 3ms/step - loss: 2209884160.0000 - val_loss: 3735759616.0000\n",
      "Epoch 166/200\n",
      "275/275 [==============================] - 1s 3ms/step - loss: 2207091712.0000 - val_loss: 4076413952.0000\n",
      "Epoch 167/200\n",
      "275/275 [==============================] - 1s 3ms/step - loss: 2222738432.0000 - val_loss: 3805547264.0000\n",
      "Epoch 168/200\n",
      "275/275 [==============================] - 1s 3ms/step - loss: 2213783296.0000 - val_loss: 3958331648.0000\n",
      "Epoch 169/200\n",
      "275/275 [==============================] - 1s 3ms/step - loss: 2189074176.0000 - val_loss: 4053421568.0000\n",
      "Epoch 170/200\n",
      "275/275 [==============================] - 1s 3ms/step - loss: 2196396032.0000 - val_loss: 3788300800.0000\n",
      "Epoch 171/200\n",
      "275/275 [==============================] - 1s 3ms/step - loss: 2177355520.0000 - val_loss: 3819839488.0000\n",
      "Epoch 172/200\n",
      "275/275 [==============================] - 1s 3ms/step - loss: 2178878720.0000 - val_loss: 3686660096.0000\n",
      "Epoch 173/200\n",
      "275/275 [==============================] - 1s 3ms/step - loss: 2180591104.0000 - val_loss: 3632285184.0000\n",
      "Epoch 174/200\n",
      "275/275 [==============================] - 1s 3ms/step - loss: 2161575168.0000 - val_loss: 3944441088.0000\n",
      "Epoch 175/200\n",
      "275/275 [==============================] - 1s 3ms/step - loss: 2192237056.0000 - val_loss: 3845397760.0000\n",
      "Epoch 176/200\n",
      "275/275 [==============================] - 1s 3ms/step - loss: 2169267456.0000 - val_loss: 3962072064.0000\n",
      "Epoch 177/200\n",
      "275/275 [==============================] - 1s 3ms/step - loss: 2182212352.0000 - val_loss: 4149888768.0000\n",
      "Epoch 178/200\n",
      "275/275 [==============================] - 1s 3ms/step - loss: 2176316672.0000 - val_loss: 3912484608.0000\n",
      "Epoch 179/200\n",
      "275/275 [==============================] - 1s 3ms/step - loss: 2149519872.0000 - val_loss: 3869945088.0000\n",
      "Epoch 180/200\n",
      "275/275 [==============================] - 1s 3ms/step - loss: 2158994944.0000 - val_loss: 4195106048.0000\n",
      "Epoch 181/200\n",
      "275/275 [==============================] - 1s 3ms/step - loss: 2173088000.0000 - val_loss: 4280338688.0000\n",
      "Epoch 182/200\n",
      "275/275 [==============================] - 1s 3ms/step - loss: 2140907392.0000 - val_loss: 3912317952.0000\n",
      "Epoch 183/200\n",
      "275/275 [==============================] - 1s 3ms/step - loss: 2148692480.0000 - val_loss: 3921112064.0000\n",
      "Epoch 184/200\n",
      "275/275 [==============================] - 1s 3ms/step - loss: 2151738112.0000 - val_loss: 3746882048.0000\n",
      "Epoch 185/200\n",
      "275/275 [==============================] - 1s 3ms/step - loss: 2137554176.0000 - val_loss: 3697449216.0000\n",
      "Epoch 186/200\n",
      "275/275 [==============================] - 1s 3ms/step - loss: 2138105216.0000 - val_loss: 3926757888.0000\n",
      "Epoch 187/200\n",
      "275/275 [==============================] - 1s 3ms/step - loss: 2140615296.0000 - val_loss: 3811453440.0000\n",
      "Epoch 188/200\n",
      "275/275 [==============================] - 1s 3ms/step - loss: 2138759808.0000 - val_loss: 3766389248.0000\n",
      "Epoch 189/200\n",
      "275/275 [==============================] - 1s 3ms/step - loss: 2120347392.0000 - val_loss: 4202106624.0000\n",
      "Epoch 190/200\n",
      "275/275 [==============================] - 1s 3ms/step - loss: 2098751616.0000 - val_loss: 4083724288.0000\n",
      "Epoch 191/200\n",
      "275/275 [==============================] - 1s 3ms/step - loss: 2109244544.0000 - val_loss: 4027751168.0000\n",
      "Epoch 192/200\n",
      "275/275 [==============================] - 1s 3ms/step - loss: 2105823872.0000 - val_loss: 3854285568.0000\n",
      "Epoch 193/200\n",
      "275/275 [==============================] - 1s 3ms/step - loss: 2099010816.0000 - val_loss: 4043412992.0000\n",
      "Epoch 194/200\n",
      "275/275 [==============================] - 1s 3ms/step - loss: 2130126976.0000 - val_loss: 3807334656.0000\n",
      "Epoch 195/200\n",
      "275/275 [==============================] - 1s 3ms/step - loss: 2113554048.0000 - val_loss: 3807856640.0000\n",
      "Epoch 196/200\n",
      "275/275 [==============================] - 1s 3ms/step - loss: 2101397888.0000 - val_loss: 4194770944.0000\n",
      "Epoch 197/200\n",
      "275/275 [==============================] - 1s 3ms/step - loss: 2087157120.0000 - val_loss: 3999822336.0000\n",
      "Epoch 198/200\n",
      "275/275 [==============================] - 1s 3ms/step - loss: 2103190528.0000 - val_loss: 4134004736.0000\n",
      "Epoch 199/200\n",
      "275/275 [==============================] - 1s 3ms/step - loss: 2099049472.0000 - val_loss: 3899232768.0000\n",
      "Epoch 200/200\n",
      "275/275 [==============================] - 1s 3ms/step - loss: 2065584512.0000 - val_loss: 3828677376.0000\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_85\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_816 (Dense)           (None, 64)                896       \n",
      "                                                                 \n",
      " dense_817 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_818 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_819 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_820 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_821 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_822 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_823 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_824 (Dense)           (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 30,081\n",
      "Trainable params: 30,081\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "138/138 [==============================] - 1s 5ms/step - loss: 34813763584.0000 - val_loss: 4205751552.0000\n",
      "Epoch 2/200\n",
      "138/138 [==============================] - 0s 3ms/step - loss: 5203814912.0000 - val_loss: 3658412288.0000\n",
      "Epoch 3/200\n",
      "138/138 [==============================] - 0s 3ms/step - loss: 4745032192.0000 - val_loss: 3551096832.0000\n",
      "Epoch 4/200\n",
      "138/138 [==============================] - 0s 3ms/step - loss: 4549580288.0000 - val_loss: 3629499392.0000\n",
      "Epoch 5/200\n",
      "138/138 [==============================] - 0s 3ms/step - loss: 4379431424.0000 - val_loss: 3380045056.0000\n",
      "Epoch 6/200\n",
      "138/138 [==============================] - 0s 3ms/step - loss: 4262800384.0000 - val_loss: 3403792128.0000\n",
      "Epoch 7/200\n",
      "138/138 [==============================] - 0s 3ms/step - loss: 4148228352.0000 - val_loss: 3473966080.0000\n",
      "Epoch 8/200\n",
      "138/138 [==============================] - 0s 3ms/step - loss: 4102248192.0000 - val_loss: 3378146816.0000\n",
      "Epoch 9/200\n",
      "138/138 [==============================] - 0s 3ms/step - loss: 3937152256.0000 - val_loss: 3308188160.0000\n",
      "Epoch 10/200\n",
      "138/138 [==============================] - 0s 4ms/step - loss: 3849479168.0000 - val_loss: 3369254912.0000\n",
      "Epoch 11/200\n",
      "138/138 [==============================] - 1s 4ms/step - loss: 3788229376.0000 - val_loss: 3423714048.0000\n",
      "Epoch 12/200\n",
      "138/138 [==============================] - 1s 4ms/step - loss: 3751296512.0000 - val_loss: 3451529728.0000\n",
      "Epoch 13/200\n",
      "138/138 [==============================] - 0s 3ms/step - loss: 3698082048.0000 - val_loss: 3351263232.0000\n",
      "Epoch 14/200\n",
      "138/138 [==============================] - 0s 3ms/step - loss: 3694715904.0000 - val_loss: 3406481408.0000\n",
      "Epoch 15/200\n",
      "138/138 [==============================] - 0s 3ms/step - loss: 3628639232.0000 - val_loss: 3609388032.0000\n",
      "Epoch 16/200\n",
      "138/138 [==============================] - 0s 3ms/step - loss: 3613318656.0000 - val_loss: 3491555584.0000\n",
      "Epoch 17/200\n",
      "138/138 [==============================] - 0s 4ms/step - loss: 3572366336.0000 - val_loss: 3325579776.0000\n",
      "Epoch 18/200\n",
      "138/138 [==============================] - 0s 3ms/step - loss: 3537908736.0000 - val_loss: 3398950656.0000\n",
      "Epoch 19/200\n",
      "138/138 [==============================] - 0s 3ms/step - loss: 3525264896.0000 - val_loss: 3457122304.0000\n",
      "Epoch 20/200\n",
      "138/138 [==============================] - 0s 3ms/step - loss: 3502573056.0000 - val_loss: 3463523840.0000\n",
      "Epoch 21/200\n",
      "138/138 [==============================] - 0s 3ms/step - loss: 3493966336.0000 - val_loss: 3578058752.0000\n",
      "Epoch 22/200\n",
      "138/138 [==============================] - 0s 3ms/step - loss: 3481266944.0000 - val_loss: 3437746944.0000\n",
      "Epoch 23/200\n",
      "138/138 [==============================] - 0s 3ms/step - loss: 3422717952.0000 - val_loss: 3532141568.0000\n",
      "Epoch 24/200\n",
      "138/138 [==============================] - 0s 3ms/step - loss: 3419434752.0000 - val_loss: 3435251712.0000\n",
      "Epoch 25/200\n",
      "138/138 [==============================] - 0s 3ms/step - loss: 3393829888.0000 - val_loss: 3502191616.0000\n",
      "Epoch 26/200\n",
      "138/138 [==============================] - 0s 3ms/step - loss: 3374992384.0000 - val_loss: 3722757120.0000\n",
      "Epoch 27/200\n",
      "138/138 [==============================] - 0s 4ms/step - loss: 3388796928.0000 - val_loss: 3611174144.0000\n",
      "Epoch 28/200\n",
      "138/138 [==============================] - 0s 3ms/step - loss: 3328403456.0000 - val_loss: 3417087232.0000\n",
      "Epoch 29/200\n",
      "138/138 [==============================] - 0s 3ms/step - loss: 3322646272.0000 - val_loss: 3396134912.0000\n",
      "Epoch 30/200\n",
      "138/138 [==============================] - 0s 4ms/step - loss: 3296460288.0000 - val_loss: 3804718336.0000\n",
      "Epoch 31/200\n",
      "138/138 [==============================] - 0s 3ms/step - loss: 3296947968.0000 - val_loss: 3583081984.0000\n",
      "Epoch 32/200\n",
      "138/138 [==============================] - 0s 3ms/step - loss: 3261367040.0000 - val_loss: 3414091264.0000\n",
      "Epoch 33/200\n",
      "138/138 [==============================] - 0s 3ms/step - loss: 3240257024.0000 - val_loss: 3282842880.0000\n",
      "Epoch 34/200\n",
      "138/138 [==============================] - 0s 3ms/step - loss: 3237993216.0000 - val_loss: 3519143936.0000\n",
      "Epoch 35/200\n",
      "138/138 [==============================] - 0s 3ms/step - loss: 3213939968.0000 - val_loss: 3310230016.0000\n",
      "Epoch 36/200\n",
      "138/138 [==============================] - 0s 3ms/step - loss: 3180549888.0000 - val_loss: 3295816960.0000\n",
      "Epoch 37/200\n",
      "138/138 [==============================] - 0s 3ms/step - loss: 3152710144.0000 - val_loss: 3410618880.0000\n",
      "Epoch 38/200\n",
      "138/138 [==============================] - 0s 3ms/step - loss: 3192338688.0000 - val_loss: 3519223040.0000\n",
      "Epoch 39/200\n",
      "138/138 [==============================] - 0s 3ms/step - loss: 3139375360.0000 - val_loss: 3346777600.0000\n",
      "Epoch 40/200\n",
      "138/138 [==============================] - 0s 3ms/step - loss: 3118496768.0000 - val_loss: 3409990912.0000\n",
      "Epoch 41/200\n",
      "138/138 [==============================] - 0s 3ms/step - loss: 3084561152.0000 - val_loss: 3464163840.0000\n",
      "Epoch 42/200\n",
      "138/138 [==============================] - 0s 3ms/step - loss: 3058771200.0000 - val_loss: 3767730944.0000\n",
      "Epoch 43/200\n",
      "138/138 [==============================] - 0s 3ms/step - loss: 3094323456.0000 - val_loss: 3201552384.0000\n",
      "Epoch 44/200\n",
      "138/138 [==============================] - 0s 3ms/step - loss: 3033724160.0000 - val_loss: 3344444928.0000\n",
      "Epoch 45/200\n",
      "138/138 [==============================] - 0s 3ms/step - loss: 3015767296.0000 - val_loss: 3266813184.0000\n",
      "Epoch 46/200\n",
      "138/138 [==============================] - 0s 3ms/step - loss: 3017606912.0000 - val_loss: 3319122688.0000\n",
      "Epoch 47/200\n",
      "138/138 [==============================] - 0s 3ms/step - loss: 3025756672.0000 - val_loss: 3288793344.0000\n",
      "Epoch 48/200\n",
      "138/138 [==============================] - 0s 3ms/step - loss: 3003205632.0000 - val_loss: 3298866944.0000\n",
      "Epoch 49/200\n",
      "138/138 [==============================] - 0s 3ms/step - loss: 2982334720.0000 - val_loss: 3316202496.0000\n",
      "Epoch 50/200\n",
      "138/138 [==============================] - 1s 4ms/step - loss: 2963811072.0000 - val_loss: 3365203712.0000\n",
      "Epoch 51/200\n",
      "138/138 [==============================] - 1s 4ms/step - loss: 2949602816.0000 - val_loss: 3171170560.0000\n",
      "Epoch 52/200\n",
      "138/138 [==============================] - 1s 4ms/step - loss: 2972902656.0000 - val_loss: 3266493696.0000\n",
      "Epoch 53/200\n",
      "138/138 [==============================] - 1s 4ms/step - loss: 2944123904.0000 - val_loss: 3291726848.0000\n",
      "Epoch 54/200\n",
      "138/138 [==============================] - 1s 4ms/step - loss: 2934792448.0000 - val_loss: 3330988544.0000\n",
      "Epoch 55/200\n",
      "138/138 [==============================] - 0s 3ms/step - loss: 2926060288.0000 - val_loss: 3145995776.0000\n",
      "Epoch 56/200\n",
      "138/138 [==============================] - 0s 4ms/step - loss: 2911941376.0000 - val_loss: 3322824448.0000\n",
      "Epoch 57/200\n",
      "138/138 [==============================] - 0s 4ms/step - loss: 2940250368.0000 - val_loss: 3310317568.0000\n",
      "Epoch 58/200\n",
      "138/138 [==============================] - 0s 3ms/step - loss: 2885312256.0000 - val_loss: 3193360384.0000\n",
      "Epoch 59/200\n",
      "138/138 [==============================] - 0s 4ms/step - loss: 2858721280.0000 - val_loss: 3232499968.0000\n",
      "Epoch 60/200\n",
      "138/138 [==============================] - 0s 4ms/step - loss: 2843305216.0000 - val_loss: 3285607168.0000\n",
      "Epoch 61/200\n",
      "138/138 [==============================] - 0s 3ms/step - loss: 2866186496.0000 - val_loss: 3178061824.0000\n",
      "Epoch 62/200\n",
      "138/138 [==============================] - 0s 4ms/step - loss: 2819727104.0000 - val_loss: 3189376000.0000\n",
      "Epoch 63/200\n",
      "138/138 [==============================] - 0s 3ms/step - loss: 2843363840.0000 - val_loss: 3351338240.0000\n",
      "Epoch 64/200\n",
      "138/138 [==============================] - 0s 3ms/step - loss: 2852658176.0000 - val_loss: 3285105920.0000\n",
      "Epoch 65/200\n",
      "138/138 [==============================] - 0s 3ms/step - loss: 2807785216.0000 - val_loss: 3369120000.0000\n",
      "Epoch 66/200\n",
      "138/138 [==============================] - 0s 3ms/step - loss: 2822562816.0000 - val_loss: 3554366208.0000\n",
      "Epoch 67/200\n",
      "138/138 [==============================] - 0s 3ms/step - loss: 2806654976.0000 - val_loss: 3140707328.0000\n",
      "Epoch 68/200\n",
      "138/138 [==============================] - 0s 3ms/step - loss: 2782894080.0000 - val_loss: 3037304064.0000\n",
      "Epoch 69/200\n",
      "138/138 [==============================] - 0s 3ms/step - loss: 2777413632.0000 - val_loss: 3222404096.0000\n",
      "Epoch 70/200\n",
      "138/138 [==============================] - 0s 4ms/step - loss: 2854580224.0000 - val_loss: 3170891520.0000\n",
      "Epoch 71/200\n",
      "138/138 [==============================] - 0s 3ms/step - loss: 2779657984.0000 - val_loss: 3125280768.0000\n",
      "Epoch 72/200\n",
      "138/138 [==============================] - 0s 3ms/step - loss: 2790642688.0000 - val_loss: 3108104704.0000\n",
      "Epoch 73/200\n",
      "138/138 [==============================] - 0s 3ms/step - loss: 2763730944.0000 - val_loss: 3283447040.0000\n",
      "Epoch 74/200\n",
      "138/138 [==============================] - 0s 3ms/step - loss: 2749960960.0000 - val_loss: 3198363648.0000\n",
      "Epoch 75/200\n",
      "138/138 [==============================] - 1s 4ms/step - loss: 2730028288.0000 - val_loss: 3166061056.0000\n",
      "Epoch 76/200\n",
      "138/138 [==============================] - 0s 3ms/step - loss: 2736868352.0000 - val_loss: 3126102784.0000\n",
      "Epoch 77/200\n",
      "138/138 [==============================] - 0s 3ms/step - loss: 2752815616.0000 - val_loss: 3403667456.0000\n",
      "Epoch 78/200\n",
      "138/138 [==============================] - 0s 3ms/step - loss: 2747250944.0000 - val_loss: 3074113280.0000\n",
      "Epoch 79/200\n",
      "138/138 [==============================] - 0s 3ms/step - loss: 2727090944.0000 - val_loss: 3087645952.0000\n",
      "Epoch 80/200\n",
      "138/138 [==============================] - 0s 3ms/step - loss: 2762049024.0000 - val_loss: 3079094272.0000\n",
      "Epoch 81/200\n",
      "138/138 [==============================] - 0s 3ms/step - loss: 2733999872.0000 - val_loss: 3026372864.0000\n",
      "Epoch 82/200\n",
      "138/138 [==============================] - 0s 3ms/step - loss: 2715493888.0000 - val_loss: 3215973120.0000\n",
      "Epoch 83/200\n",
      "138/138 [==============================] - 0s 4ms/step - loss: 2756244736.0000 - val_loss: 3125551104.0000\n",
      "Epoch 84/200\n",
      "138/138 [==============================] - 1s 4ms/step - loss: 2670702080.0000 - val_loss: 3343974912.0000\n",
      "Epoch 85/200\n",
      "138/138 [==============================] - 1s 4ms/step - loss: 2730491392.0000 - val_loss: 3159081984.0000\n",
      "Epoch 86/200\n",
      "138/138 [==============================] - 1s 4ms/step - loss: 2714664448.0000 - val_loss: 3460779776.0000\n",
      "Epoch 87/200\n",
      "138/138 [==============================] - 1s 4ms/step - loss: 2667283968.0000 - val_loss: 3139419904.0000\n",
      "Epoch 88/200\n",
      "138/138 [==============================] - 0s 4ms/step - loss: 2675401216.0000 - val_loss: 3212198656.0000\n",
      "Epoch 89/200\n",
      "138/138 [==============================] - 0s 4ms/step - loss: 2668760832.0000 - val_loss: 3048226304.0000\n",
      "Epoch 90/200\n",
      "138/138 [==============================] - 0s 3ms/step - loss: 2700303872.0000 - val_loss: 3042309376.0000\n",
      "Epoch 91/200\n",
      "138/138 [==============================] - 0s 3ms/step - loss: 2661320704.0000 - val_loss: 3406960128.0000\n",
      "Epoch 92/200\n",
      "138/138 [==============================] - 1s 4ms/step - loss: 2647200256.0000 - val_loss: 3148807424.0000\n",
      "Epoch 93/200\n",
      "138/138 [==============================] - 0s 4ms/step - loss: 2634899968.0000 - val_loss: 3287410176.0000\n",
      "Epoch 94/200\n",
      "138/138 [==============================] - 1s 4ms/step - loss: 2649728768.0000 - val_loss: 3121602560.0000\n",
      "Epoch 95/200\n",
      "138/138 [==============================] - 0s 4ms/step - loss: 2644070912.0000 - val_loss: 3069302528.0000\n",
      "Epoch 96/200\n",
      "138/138 [==============================] - 0s 4ms/step - loss: 2638180608.0000 - val_loss: 3190335488.0000\n",
      "Epoch 97/200\n",
      "138/138 [==============================] - 0s 4ms/step - loss: 2645438976.0000 - val_loss: 3169997312.0000\n",
      "Epoch 98/200\n",
      "138/138 [==============================] - 1s 4ms/step - loss: 2665369856.0000 - val_loss: 3124426496.0000\n",
      "Epoch 99/200\n",
      "138/138 [==============================] - 0s 3ms/step - loss: 2618981376.0000 - val_loss: 3195560448.0000\n",
      "Epoch 100/200\n",
      "138/138 [==============================] - 1s 4ms/step - loss: 2656371456.0000 - val_loss: 3235603712.0000\n",
      "Epoch 101/200\n",
      "138/138 [==============================] - 1s 4ms/step - loss: 2625769472.0000 - val_loss: 3163802112.0000\n",
      "Epoch 102/200\n",
      "138/138 [==============================] - 1s 4ms/step - loss: 2653121536.0000 - val_loss: 3201402880.0000\n",
      "Epoch 103/200\n",
      "138/138 [==============================] - 0s 4ms/step - loss: 2613363456.0000 - val_loss: 3303363840.0000\n",
      "Epoch 104/200\n",
      "138/138 [==============================] - 0s 4ms/step - loss: 2595304448.0000 - val_loss: 3014827264.0000\n",
      "Epoch 105/200\n",
      "138/138 [==============================] - 0s 4ms/step - loss: 2591652608.0000 - val_loss: 3131497984.0000\n",
      "Epoch 106/200\n",
      "138/138 [==============================] - 0s 3ms/step - loss: 2588084992.0000 - val_loss: 3112988416.0000\n",
      "Epoch 107/200\n",
      "138/138 [==============================] - 0s 4ms/step - loss: 2590836736.0000 - val_loss: 3008934656.0000\n",
      "Epoch 108/200\n",
      "138/138 [==============================] - 0s 4ms/step - loss: 2581280256.0000 - val_loss: 3072243712.0000\n",
      "Epoch 109/200\n",
      "138/138 [==============================] - 0s 4ms/step - loss: 2595043328.0000 - val_loss: 3151956480.0000\n",
      "Epoch 110/200\n",
      "138/138 [==============================] - 0s 3ms/step - loss: 2573601536.0000 - val_loss: 3112901120.0000\n",
      "Epoch 111/200\n",
      "138/138 [==============================] - 1s 4ms/step - loss: 2559322880.0000 - val_loss: 3142297344.0000\n",
      "Epoch 112/200\n",
      "138/138 [==============================] - 0s 3ms/step - loss: 2581886208.0000 - val_loss: 3229299200.0000\n",
      "Epoch 113/200\n",
      "138/138 [==============================] - 1s 4ms/step - loss: 2525486336.0000 - val_loss: 3159550464.0000\n",
      "Epoch 114/200\n",
      "138/138 [==============================] - 1s 4ms/step - loss: 2604287488.0000 - val_loss: 3188561408.0000\n",
      "Epoch 115/200\n",
      "138/138 [==============================] - 1s 4ms/step - loss: 2540207616.0000 - val_loss: 3326953728.0000\n",
      "Epoch 116/200\n",
      "138/138 [==============================] - 1s 4ms/step - loss: 2555456256.0000 - val_loss: 3043745792.0000\n",
      "Epoch 117/200\n",
      "138/138 [==============================] - 1s 4ms/step - loss: 2551719424.0000 - val_loss: 3232667904.0000\n",
      "Epoch 118/200\n",
      "138/138 [==============================] - 1s 4ms/step - loss: 2568611584.0000 - val_loss: 3086025472.0000\n",
      "Epoch 119/200\n",
      "138/138 [==============================] - 1s 4ms/step - loss: 2541361664.0000 - val_loss: 3248427520.0000\n",
      "Epoch 120/200\n",
      "138/138 [==============================] - 1s 4ms/step - loss: 2526034944.0000 - val_loss: 3367152640.0000\n",
      "Epoch 121/200\n",
      "138/138 [==============================] - 1s 4ms/step - loss: 2516254720.0000 - val_loss: 3195099136.0000\n",
      "Epoch 122/200\n",
      "138/138 [==============================] - 1s 4ms/step - loss: 2506078464.0000 - val_loss: 3224648192.0000\n",
      "Epoch 123/200\n",
      "138/138 [==============================] - 1s 4ms/step - loss: 2507186688.0000 - val_loss: 3471620352.0000\n",
      "Epoch 124/200\n",
      "138/138 [==============================] - 1s 4ms/step - loss: 2507764224.0000 - val_loss: 3137068800.0000\n",
      "Epoch 125/200\n",
      "138/138 [==============================] - 0s 4ms/step - loss: 2536498688.0000 - val_loss: 3215250432.0000\n",
      "Epoch 126/200\n",
      "138/138 [==============================] - 1s 4ms/step - loss: 2525524992.0000 - val_loss: 3299283712.0000\n",
      "Epoch 127/200\n",
      "138/138 [==============================] - 1s 4ms/step - loss: 2488520192.0000 - val_loss: 3262475008.0000\n",
      "Epoch 128/200\n",
      "138/138 [==============================] - 1s 4ms/step - loss: 2491214848.0000 - val_loss: 3209345280.0000\n",
      "Epoch 129/200\n",
      "138/138 [==============================] - 1s 4ms/step - loss: 2479891200.0000 - val_loss: 3256650752.0000\n",
      "Epoch 130/200\n",
      "138/138 [==============================] - 1s 4ms/step - loss: 2469792512.0000 - val_loss: 3322849536.0000\n",
      "Epoch 131/200\n",
      "138/138 [==============================] - 0s 3ms/step - loss: 2503517696.0000 - val_loss: 3222195456.0000\n",
      "Epoch 132/200\n",
      "138/138 [==============================] - 0s 3ms/step - loss: 2463622144.0000 - val_loss: 3410418944.0000\n",
      "Epoch 133/200\n",
      "138/138 [==============================] - 0s 3ms/step - loss: 2465548544.0000 - val_loss: 3408919808.0000\n",
      "Epoch 134/200\n",
      "138/138 [==============================] - 0s 3ms/step - loss: 2486390272.0000 - val_loss: 3211474432.0000\n",
      "Epoch 135/200\n",
      "138/138 [==============================] - 0s 4ms/step - loss: 2467413760.0000 - val_loss: 3316331264.0000\n",
      "Epoch 136/200\n",
      "138/138 [==============================] - 1s 4ms/step - loss: 2445018880.0000 - val_loss: 3218635520.0000\n",
      "Epoch 137/200\n",
      "138/138 [==============================] - 0s 3ms/step - loss: 2434443264.0000 - val_loss: 3325413888.0000\n",
      "Epoch 138/200\n",
      "138/138 [==============================] - 0s 3ms/step - loss: 2451764992.0000 - val_loss: 3291956992.0000\n",
      "Epoch 139/200\n",
      "138/138 [==============================] - 0s 3ms/step - loss: 2456934912.0000 - val_loss: 3387281408.0000\n",
      "Epoch 140/200\n",
      "138/138 [==============================] - 0s 3ms/step - loss: 2407720704.0000 - val_loss: 3281850624.0000\n",
      "Epoch 141/200\n",
      "138/138 [==============================] - 0s 3ms/step - loss: 2434793472.0000 - val_loss: 3368497664.0000\n",
      "Epoch 142/200\n",
      "138/138 [==============================] - 0s 3ms/step - loss: 2420960768.0000 - val_loss: 3349457920.0000\n",
      "Epoch 143/200\n",
      "138/138 [==============================] - 0s 3ms/step - loss: 2429967104.0000 - val_loss: 3288133376.0000\n",
      "Epoch 144/200\n",
      "138/138 [==============================] - 0s 3ms/step - loss: 2415246592.0000 - val_loss: 3314913280.0000\n",
      "Epoch 145/200\n",
      "138/138 [==============================] - 0s 3ms/step - loss: 2451862528.0000 - val_loss: 3586225664.0000\n",
      "Epoch 146/200\n",
      "138/138 [==============================] - 0s 3ms/step - loss: 2437137664.0000 - val_loss: 3245026048.0000\n",
      "Epoch 147/200\n",
      "138/138 [==============================] - 0s 3ms/step - loss: 2393668352.0000 - val_loss: 3406656768.0000\n",
      "Epoch 148/200\n",
      "138/138 [==============================] - 0s 3ms/step - loss: 2395735040.0000 - val_loss: 3388324096.0000\n",
      "Epoch 149/200\n",
      "138/138 [==============================] - 0s 3ms/step - loss: 2380583680.0000 - val_loss: 3392844032.0000\n",
      "Epoch 150/200\n",
      "138/138 [==============================] - 0s 3ms/step - loss: 2388352256.0000 - val_loss: 3404654848.0000\n",
      "Epoch 151/200\n",
      "138/138 [==============================] - 0s 3ms/step - loss: 2406363648.0000 - val_loss: 3330722816.0000\n",
      "Epoch 152/200\n",
      "138/138 [==============================] - 0s 3ms/step - loss: 2384252160.0000 - val_loss: 3670794240.0000\n",
      "Epoch 153/200\n",
      "138/138 [==============================] - 0s 3ms/step - loss: 2402848768.0000 - val_loss: 3425724160.0000\n",
      "Epoch 154/200\n",
      "138/138 [==============================] - 0s 3ms/step - loss: 2372344576.0000 - val_loss: 3774353664.0000\n",
      "Epoch 155/200\n",
      "138/138 [==============================] - 0s 3ms/step - loss: 2400612352.0000 - val_loss: 3378478080.0000\n",
      "Epoch 156/200\n",
      "138/138 [==============================] - 0s 4ms/step - loss: 2357816320.0000 - val_loss: 3407656192.0000\n",
      "Epoch 157/200\n",
      "138/138 [==============================] - 0s 3ms/step - loss: 2404004096.0000 - val_loss: 3674251520.0000\n",
      "Epoch 158/200\n",
      "138/138 [==============================] - 0s 3ms/step - loss: 2348841472.0000 - val_loss: 3445416448.0000\n",
      "Epoch 159/200\n",
      "138/138 [==============================] - 0s 3ms/step - loss: 2378000128.0000 - val_loss: 3347293696.0000\n",
      "Epoch 160/200\n",
      "138/138 [==============================] - 0s 3ms/step - loss: 2371132160.0000 - val_loss: 3480962304.0000\n",
      "Epoch 161/200\n",
      "138/138 [==============================] - 0s 3ms/step - loss: 2361928448.0000 - val_loss: 3611786240.0000\n",
      "Epoch 162/200\n",
      "138/138 [==============================] - 0s 3ms/step - loss: 2359576320.0000 - val_loss: 3632518656.0000\n",
      "Epoch 163/200\n",
      "138/138 [==============================] - 0s 3ms/step - loss: 2335821568.0000 - val_loss: 3695902464.0000\n",
      "Epoch 164/200\n",
      "138/138 [==============================] - 0s 3ms/step - loss: 2340834560.0000 - val_loss: 3454306816.0000\n",
      "Epoch 165/200\n",
      "138/138 [==============================] - 0s 3ms/step - loss: 2333053952.0000 - val_loss: 3340226304.0000\n",
      "Epoch 166/200\n",
      "138/138 [==============================] - 0s 4ms/step - loss: 2339208448.0000 - val_loss: 3784113408.0000\n",
      "Epoch 167/200\n",
      "138/138 [==============================] - 0s 3ms/step - loss: 2322712320.0000 - val_loss: 3604996864.0000\n",
      "Epoch 168/200\n",
      "138/138 [==============================] - 1s 4ms/step - loss: 2332701696.0000 - val_loss: 3593391360.0000\n",
      "Epoch 169/200\n",
      "138/138 [==============================] - 1s 4ms/step - loss: 2332121856.0000 - val_loss: 3607351296.0000\n",
      "Epoch 170/200\n",
      "138/138 [==============================] - 1s 4ms/step - loss: 2293433344.0000 - val_loss: 3678860544.0000\n",
      "Epoch 171/200\n",
      "138/138 [==============================] - 1s 4ms/step - loss: 2300408576.0000 - val_loss: 3591669760.0000\n",
      "Epoch 172/200\n",
      "138/138 [==============================] - 1s 4ms/step - loss: 2288488448.0000 - val_loss: 3594213376.0000\n",
      "Epoch 173/200\n",
      "138/138 [==============================] - 1s 4ms/step - loss: 2318313728.0000 - val_loss: 3460841984.0000\n",
      "Epoch 174/200\n",
      "138/138 [==============================] - 1s 4ms/step - loss: 2335447552.0000 - val_loss: 3682096640.0000\n",
      "Epoch 175/200\n",
      "138/138 [==============================] - 1s 4ms/step - loss: 2297799680.0000 - val_loss: 3692728832.0000\n",
      "Epoch 176/200\n",
      "138/138 [==============================] - 1s 4ms/step - loss: 2294797824.0000 - val_loss: 3463137280.0000\n",
      "Epoch 177/200\n",
      "138/138 [==============================] - 1s 4ms/step - loss: 2298601984.0000 - val_loss: 3907281664.0000\n",
      "Epoch 178/200\n",
      "138/138 [==============================] - 0s 3ms/step - loss: 2311930880.0000 - val_loss: 3554409216.0000\n",
      "Epoch 179/200\n",
      "138/138 [==============================] - 1s 4ms/step - loss: 2296162816.0000 - val_loss: 3568786176.0000\n",
      "Epoch 180/200\n",
      "138/138 [==============================] - 1s 4ms/step - loss: 2269763328.0000 - val_loss: 3712854784.0000\n",
      "Epoch 181/200\n",
      "138/138 [==============================] - 1s 4ms/step - loss: 2301503488.0000 - val_loss: 3815848448.0000\n",
      "Epoch 182/200\n",
      "138/138 [==============================] - 0s 4ms/step - loss: 2272144128.0000 - val_loss: 3848043520.0000\n",
      "Epoch 183/200\n",
      "138/138 [==============================] - 0s 3ms/step - loss: 2292586496.0000 - val_loss: 3499840512.0000\n",
      "Epoch 184/200\n",
      "138/138 [==============================] - 0s 3ms/step - loss: 2297828352.0000 - val_loss: 3753395712.0000\n",
      "Epoch 185/200\n",
      "138/138 [==============================] - 0s 3ms/step - loss: 2251715584.0000 - val_loss: 3493574144.0000\n",
      "Epoch 186/200\n",
      "138/138 [==============================] - 0s 3ms/step - loss: 2282330624.0000 - val_loss: 3517137920.0000\n",
      "Epoch 187/200\n",
      "138/138 [==============================] - 0s 3ms/step - loss: 2257446400.0000 - val_loss: 3712920576.0000\n",
      "Epoch 188/200\n",
      "138/138 [==============================] - 0s 3ms/step - loss: 2247161600.0000 - val_loss: 3488641024.0000\n",
      "Epoch 189/200\n",
      "138/138 [==============================] - 0s 3ms/step - loss: 2238702592.0000 - val_loss: 3688040704.0000\n",
      "Epoch 190/200\n",
      "138/138 [==============================] - 0s 3ms/step - loss: 2241126912.0000 - val_loss: 3530902272.0000\n",
      "Epoch 191/200\n",
      "138/138 [==============================] - 0s 3ms/step - loss: 2249999872.0000 - val_loss: 3793913856.0000\n",
      "Epoch 192/200\n",
      "138/138 [==============================] - 0s 4ms/step - loss: 2235458816.0000 - val_loss: 3780920064.0000\n",
      "Epoch 193/200\n",
      "138/138 [==============================] - 0s 3ms/step - loss: 2264002048.0000 - val_loss: 3818681856.0000\n",
      "Epoch 194/200\n",
      "138/138 [==============================] - 1s 4ms/step - loss: 2236748032.0000 - val_loss: 3522464256.0000\n",
      "Epoch 195/200\n",
      "138/138 [==============================] - 1s 4ms/step - loss: 2251870976.0000 - val_loss: 3939328256.0000\n",
      "Epoch 196/200\n",
      "138/138 [==============================] - 0s 4ms/step - loss: 2216559360.0000 - val_loss: 3784111616.0000\n",
      "Epoch 197/200\n",
      "138/138 [==============================] - 1s 4ms/step - loss: 2258060288.0000 - val_loss: 3851504384.0000\n",
      "Epoch 198/200\n",
      "138/138 [==============================] - 0s 3ms/step - loss: 2248750848.0000 - val_loss: 3942763520.0000\n",
      "Epoch 199/200\n",
      "138/138 [==============================] - 0s 3ms/step - loss: 2245523456.0000 - val_loss: 3918874624.0000\n",
      "Epoch 200/200\n",
      "138/138 [==============================] - 0s 3ms/step - loss: 2205710336.0000 - val_loss: 3840164096.0000\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_86\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_825 (Dense)           (None, 64)                896       \n",
      "                                                                 \n",
      " dense_826 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_827 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_828 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_829 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_830 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_831 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_832 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_833 (Dense)           (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 30,081\n",
      "Trainable params: 30,081\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 55200141312.0000 - val_loss: 48859291648.0000\n",
      "Epoch 2/200\n",
      "69/69 [==============================] - 0s 7ms/step - loss: 13558256640.0000 - val_loss: 4268412416.0000\n",
      "Epoch 3/200\n",
      "69/69 [==============================] - 0s 7ms/step - loss: 5311890432.0000 - val_loss: 3679798528.0000\n",
      "Epoch 4/200\n",
      "69/69 [==============================] - 0s 7ms/step - loss: 4877760512.0000 - val_loss: 3606890752.0000\n",
      "Epoch 5/200\n",
      "69/69 [==============================] - 0s 7ms/step - loss: 4710078976.0000 - val_loss: 3565281792.0000\n",
      "Epoch 6/200\n",
      "69/69 [==============================] - 0s 7ms/step - loss: 4581347840.0000 - val_loss: 3510378752.0000\n",
      "Epoch 7/200\n",
      "69/69 [==============================] - 0s 7ms/step - loss: 4512524288.0000 - val_loss: 3488082688.0000\n",
      "Epoch 8/200\n",
      "69/69 [==============================] - 0s 7ms/step - loss: 4387898368.0000 - val_loss: 3415682560.0000\n",
      "Epoch 9/200\n",
      "69/69 [==============================] - 0s 7ms/step - loss: 4297373696.0000 - val_loss: 3472916480.0000\n",
      "Epoch 10/200\n",
      "69/69 [==============================] - 0s 7ms/step - loss: 4216454656.0000 - val_loss: 3361237760.0000\n",
      "Epoch 11/200\n",
      "69/69 [==============================] - 0s 7ms/step - loss: 4121708032.0000 - val_loss: 3351289088.0000\n",
      "Epoch 12/200\n",
      "69/69 [==============================] - 0s 7ms/step - loss: 4051568128.0000 - val_loss: 3344065792.0000\n",
      "Epoch 13/200\n",
      "69/69 [==============================] - 0s 7ms/step - loss: 3966640384.0000 - val_loss: 3331993856.0000\n",
      "Epoch 14/200\n",
      "69/69 [==============================] - 0s 7ms/step - loss: 3907599872.0000 - val_loss: 3322146560.0000\n",
      "Epoch 15/200\n",
      "69/69 [==============================] - 1s 8ms/step - loss: 3850811392.0000 - val_loss: 3453449472.0000\n",
      "Epoch 16/200\n",
      "69/69 [==============================] - 1s 8ms/step - loss: 3807800832.0000 - val_loss: 3294634496.0000\n",
      "Epoch 17/200\n",
      "69/69 [==============================] - 1s 8ms/step - loss: 3768503552.0000 - val_loss: 3323041280.0000\n",
      "Epoch 18/200\n",
      "69/69 [==============================] - 1s 7ms/step - loss: 3721508608.0000 - val_loss: 3305392640.0000\n",
      "Epoch 19/200\n",
      "69/69 [==============================] - 1s 7ms/step - loss: 3690779904.0000 - val_loss: 3380876544.0000\n",
      "Epoch 20/200\n",
      "69/69 [==============================] - 1s 7ms/step - loss: 3665169920.0000 - val_loss: 3297367552.0000\n",
      "Epoch 21/200\n",
      "69/69 [==============================] - 0s 7ms/step - loss: 3645219072.0000 - val_loss: 3356416512.0000\n",
      "Epoch 22/200\n",
      "69/69 [==============================] - 1s 7ms/step - loss: 3624844800.0000 - val_loss: 3364266752.0000\n",
      "Epoch 23/200\n",
      "69/69 [==============================] - 0s 7ms/step - loss: 3630864896.0000 - val_loss: 3358785792.0000\n",
      "Epoch 24/200\n",
      "69/69 [==============================] - 1s 7ms/step - loss: 3596331776.0000 - val_loss: 3348732416.0000\n",
      "Epoch 25/200\n",
      "69/69 [==============================] - 1s 7ms/step - loss: 3564477440.0000 - val_loss: 3410859520.0000\n",
      "Epoch 26/200\n",
      "69/69 [==============================] - 1s 7ms/step - loss: 3553159680.0000 - val_loss: 3375978240.0000\n",
      "Epoch 27/200\n",
      "69/69 [==============================] - 1s 7ms/step - loss: 3537934848.0000 - val_loss: 3381639936.0000\n",
      "Epoch 28/200\n",
      "69/69 [==============================] - 1s 7ms/step - loss: 3523783680.0000 - val_loss: 3337174528.0000\n",
      "Epoch 29/200\n",
      "69/69 [==============================] - 0s 7ms/step - loss: 3510060800.0000 - val_loss: 3356508160.0000\n",
      "Epoch 30/200\n",
      "69/69 [==============================] - 1s 7ms/step - loss: 3490650112.0000 - val_loss: 3471617280.0000\n",
      "Epoch 31/200\n",
      "69/69 [==============================] - 0s 7ms/step - loss: 3477196800.0000 - val_loss: 3650370048.0000\n",
      "Epoch 32/200\n",
      "69/69 [==============================] - 1s 7ms/step - loss: 3461004032.0000 - val_loss: 3448799744.0000\n",
      "Epoch 33/200\n",
      "69/69 [==============================] - 1s 7ms/step - loss: 3456410880.0000 - val_loss: 3392177664.0000\n",
      "Epoch 34/200\n",
      "69/69 [==============================] - 1s 7ms/step - loss: 3431761664.0000 - val_loss: 3475747584.0000\n",
      "Epoch 35/200\n",
      "69/69 [==============================] - 1s 7ms/step - loss: 3417144576.0000 - val_loss: 3428314368.0000\n",
      "Epoch 36/200\n",
      "69/69 [==============================] - 1s 7ms/step - loss: 3423939584.0000 - val_loss: 3496647680.0000\n",
      "Epoch 37/200\n",
      "69/69 [==============================] - 1s 7ms/step - loss: 3401457920.0000 - val_loss: 3504435456.0000\n",
      "Epoch 38/200\n",
      "69/69 [==============================] - 1s 7ms/step - loss: 3379083520.0000 - val_loss: 3595769856.0000\n",
      "Epoch 39/200\n",
      "69/69 [==============================] - 1s 7ms/step - loss: 3372026368.0000 - val_loss: 3546642688.0000\n",
      "Epoch 40/200\n",
      "69/69 [==============================] - 1s 8ms/step - loss: 3361221376.0000 - val_loss: 3584501760.0000\n",
      "Epoch 41/200\n",
      "69/69 [==============================] - 1s 7ms/step - loss: 3328904960.0000 - val_loss: 3451058432.0000\n",
      "Epoch 42/200\n",
      "69/69 [==============================] - 0s 7ms/step - loss: 3342400000.0000 - val_loss: 3522599680.0000\n",
      "Epoch 43/200\n",
      "69/69 [==============================] - 0s 7ms/step - loss: 3321541888.0000 - val_loss: 3768000512.0000\n",
      "Epoch 44/200\n",
      "69/69 [==============================] - 0s 7ms/step - loss: 3312229888.0000 - val_loss: 3546840832.0000\n",
      "Epoch 45/200\n",
      "69/69 [==============================] - 1s 7ms/step - loss: 3285882112.0000 - val_loss: 3471255552.0000\n",
      "Epoch 46/200\n",
      "69/69 [==============================] - 1s 7ms/step - loss: 3285427712.0000 - val_loss: 3629002752.0000\n",
      "Epoch 47/200\n",
      "69/69 [==============================] - 1s 7ms/step - loss: 3268139520.0000 - val_loss: 3502032896.0000\n",
      "Epoch 48/200\n",
      "69/69 [==============================] - 0s 7ms/step - loss: 3271847424.0000 - val_loss: 3567873024.0000\n",
      "Epoch 49/200\n",
      "69/69 [==============================] - 0s 7ms/step - loss: 3258295552.0000 - val_loss: 3586244608.0000\n",
      "Epoch 50/200\n",
      "69/69 [==============================] - 1s 7ms/step - loss: 3261624832.0000 - val_loss: 3508624384.0000\n",
      "Epoch 51/200\n",
      "69/69 [==============================] - 1s 7ms/step - loss: 3207608832.0000 - val_loss: 3647735040.0000\n",
      "Epoch 52/200\n",
      "69/69 [==============================] - 1s 7ms/step - loss: 3225104128.0000 - val_loss: 3591151872.0000\n",
      "Epoch 53/200\n",
      "69/69 [==============================] - 1s 8ms/step - loss: 3213760256.0000 - val_loss: 3622223872.0000\n",
      "Epoch 54/200\n",
      "69/69 [==============================] - 1s 9ms/step - loss: 3190788096.0000 - val_loss: 3811545344.0000\n",
      "Epoch 55/200\n",
      "69/69 [==============================] - 1s 8ms/step - loss: 3195499264.0000 - val_loss: 3598656512.0000\n",
      "Epoch 56/200\n",
      "69/69 [==============================] - 1s 7ms/step - loss: 3161173760.0000 - val_loss: 3550049536.0000\n",
      "Epoch 57/200\n",
      "69/69 [==============================] - 1s 7ms/step - loss: 3139812608.0000 - val_loss: 3544334592.0000\n",
      "Epoch 58/200\n",
      "69/69 [==============================] - 1s 7ms/step - loss: 3136630016.0000 - val_loss: 3604301824.0000\n",
      "Epoch 59/200\n",
      "69/69 [==============================] - 0s 7ms/step - loss: 3145884160.0000 - val_loss: 3393859584.0000\n",
      "Epoch 60/200\n",
      "69/69 [==============================] - 1s 7ms/step - loss: 3126906368.0000 - val_loss: 3446374400.0000\n",
      "Epoch 61/200\n",
      "69/69 [==============================] - 1s 8ms/step - loss: 3106572800.0000 - val_loss: 3524962560.0000\n",
      "Epoch 62/200\n",
      "69/69 [==============================] - 1s 8ms/step - loss: 3093643776.0000 - val_loss: 3476316416.0000\n",
      "Epoch 63/200\n",
      "69/69 [==============================] - 1s 7ms/step - loss: 3088353792.0000 - val_loss: 3652355840.0000\n",
      "Epoch 64/200\n",
      "69/69 [==============================] - 0s 7ms/step - loss: 3095846912.0000 - val_loss: 3607771392.0000\n",
      "Epoch 65/200\n",
      "69/69 [==============================] - 0s 7ms/step - loss: 3056436992.0000 - val_loss: 3483998976.0000\n",
      "Epoch 66/200\n",
      "69/69 [==============================] - 0s 7ms/step - loss: 3061740800.0000 - val_loss: 3529719808.0000\n",
      "Epoch 67/200\n",
      "69/69 [==============================] - 0s 7ms/step - loss: 3038994944.0000 - val_loss: 3615444736.0000\n",
      "Epoch 68/200\n",
      "69/69 [==============================] - 1s 7ms/step - loss: 3030986752.0000 - val_loss: 3453717504.0000\n",
      "Epoch 69/200\n",
      "69/69 [==============================] - 1s 7ms/step - loss: 3026683904.0000 - val_loss: 3642953472.0000\n",
      "Epoch 70/200\n",
      "69/69 [==============================] - 1s 8ms/step - loss: 3009413888.0000 - val_loss: 3485218048.0000\n",
      "Epoch 71/200\n",
      "69/69 [==============================] - 1s 7ms/step - loss: 2987124480.0000 - val_loss: 3561109504.0000\n",
      "Epoch 72/200\n",
      "69/69 [==============================] - 1s 7ms/step - loss: 2991570944.0000 - val_loss: 3402048256.0000\n",
      "Epoch 73/200\n",
      "69/69 [==============================] - 0s 7ms/step - loss: 3002571008.0000 - val_loss: 3489103360.0000\n",
      "Epoch 74/200\n",
      "69/69 [==============================] - 0s 7ms/step - loss: 2993464832.0000 - val_loss: 3709876736.0000\n",
      "Epoch 75/200\n",
      "69/69 [==============================] - 0s 7ms/step - loss: 2982594816.0000 - val_loss: 3481065984.0000\n",
      "Epoch 76/200\n",
      "69/69 [==============================] - 0s 7ms/step - loss: 2970449408.0000 - val_loss: 3486634496.0000\n",
      "Epoch 77/200\n",
      "69/69 [==============================] - 0s 7ms/step - loss: 2950005760.0000 - val_loss: 3638490624.0000\n",
      "Epoch 78/200\n",
      "69/69 [==============================] - 0s 7ms/step - loss: 2929267456.0000 - val_loss: 3527006720.0000\n",
      "Epoch 79/200\n",
      "69/69 [==============================] - 0s 7ms/step - loss: 2927956224.0000 - val_loss: 3543360256.0000\n",
      "Epoch 80/200\n",
      "69/69 [==============================] - 0s 7ms/step - loss: 2937185280.0000 - val_loss: 3679883776.0000\n",
      "Epoch 81/200\n",
      "69/69 [==============================] - 1s 7ms/step - loss: 2913096960.0000 - val_loss: 3554030592.0000\n",
      "Epoch 82/200\n",
      "69/69 [==============================] - 1s 7ms/step - loss: 2903193856.0000 - val_loss: 3479421696.0000\n",
      "Epoch 83/200\n",
      "69/69 [==============================] - 1s 8ms/step - loss: 2889005824.0000 - val_loss: 3426930176.0000\n",
      "Epoch 84/200\n",
      "69/69 [==============================] - 1s 7ms/step - loss: 2878073600.0000 - val_loss: 3566838272.0000\n",
      "Epoch 85/200\n",
      "69/69 [==============================] - 1s 7ms/step - loss: 2903290624.0000 - val_loss: 3536416000.0000\n",
      "Epoch 86/200\n",
      "69/69 [==============================] - 1s 7ms/step - loss: 2890137600.0000 - val_loss: 3517803776.0000\n",
      "Epoch 87/200\n",
      "69/69 [==============================] - 1s 7ms/step - loss: 2859546624.0000 - val_loss: 3475680000.0000\n",
      "Epoch 88/200\n",
      "69/69 [==============================] - 1s 7ms/step - loss: 2847831040.0000 - val_loss: 3604873472.0000\n",
      "Epoch 89/200\n",
      "69/69 [==============================] - 0s 7ms/step - loss: 2866385920.0000 - val_loss: 3664232192.0000\n",
      "Epoch 90/200\n",
      "69/69 [==============================] - 1s 7ms/step - loss: 2875030784.0000 - val_loss: 3358653184.0000\n",
      "Epoch 91/200\n",
      "69/69 [==============================] - 1s 8ms/step - loss: 2861954304.0000 - val_loss: 3545317376.0000\n",
      "Epoch 92/200\n",
      "69/69 [==============================] - 1s 7ms/step - loss: 2834315264.0000 - val_loss: 3391791872.0000\n",
      "Epoch 93/200\n",
      "69/69 [==============================] - 1s 7ms/step - loss: 2842538240.0000 - val_loss: 3562226944.0000\n",
      "Epoch 94/200\n",
      "69/69 [==============================] - 1s 7ms/step - loss: 2819752960.0000 - val_loss: 3300854272.0000\n",
      "Epoch 95/200\n",
      "69/69 [==============================] - 1s 7ms/step - loss: 2839272448.0000 - val_loss: 3469515520.0000\n",
      "Epoch 96/200\n",
      "69/69 [==============================] - 1s 8ms/step - loss: 2810076672.0000 - val_loss: 3439787264.0000\n",
      "Epoch 97/200\n",
      "69/69 [==============================] - 1s 8ms/step - loss: 2842700032.0000 - val_loss: 3288302848.0000\n",
      "Epoch 98/200\n",
      "69/69 [==============================] - 1s 8ms/step - loss: 2818101504.0000 - val_loss: 3559078912.0000\n",
      "Epoch 99/200\n",
      "69/69 [==============================] - 1s 8ms/step - loss: 2805461760.0000 - val_loss: 3322368768.0000\n",
      "Epoch 100/200\n",
      "69/69 [==============================] - 1s 8ms/step - loss: 2786630144.0000 - val_loss: 3438670848.0000\n",
      "Epoch 101/200\n",
      "69/69 [==============================] - 0s 7ms/step - loss: 2786798592.0000 - val_loss: 3461264384.0000\n",
      "Epoch 102/200\n",
      "69/69 [==============================] - 0s 7ms/step - loss: 2800960000.0000 - val_loss: 3343312896.0000\n",
      "Epoch 103/200\n",
      "69/69 [==============================] - 0s 7ms/step - loss: 2772410880.0000 - val_loss: 3637674496.0000\n",
      "Epoch 104/200\n",
      "69/69 [==============================] - 0s 6ms/step - loss: 2797572352.0000 - val_loss: 3343031296.0000\n",
      "Epoch 105/200\n",
      "69/69 [==============================] - 0s 7ms/step - loss: 2763118592.0000 - val_loss: 3343459840.0000\n",
      "Epoch 106/200\n",
      "69/69 [==============================] - 0s 6ms/step - loss: 2756599296.0000 - val_loss: 3406653440.0000\n",
      "Epoch 107/200\n",
      "69/69 [==============================] - 0s 6ms/step - loss: 2762932992.0000 - val_loss: 3392331264.0000\n",
      "Epoch 108/200\n",
      "69/69 [==============================] - 0s 7ms/step - loss: 2761476864.0000 - val_loss: 3486672384.0000\n",
      "Epoch 109/200\n",
      "69/69 [==============================] - 0s 7ms/step - loss: 2748817152.0000 - val_loss: 3518588928.0000\n",
      "Epoch 110/200\n",
      "69/69 [==============================] - 0s 6ms/step - loss: 2717924352.0000 - val_loss: 3428228864.0000\n",
      "Epoch 111/200\n",
      "69/69 [==============================] - 0s 6ms/step - loss: 2720667904.0000 - val_loss: 3533967104.0000\n",
      "Epoch 112/200\n",
      "69/69 [==============================] - 0s 7ms/step - loss: 2727660032.0000 - val_loss: 3484870144.0000\n",
      "Epoch 113/200\n",
      "69/69 [==============================] - 0s 7ms/step - loss: 2730411264.0000 - val_loss: 3551516160.0000\n",
      "Epoch 114/200\n",
      "69/69 [==============================] - 0s 6ms/step - loss: 2733009664.0000 - val_loss: 3443178752.0000\n",
      "Epoch 115/200\n",
      "69/69 [==============================] - 0s 6ms/step - loss: 2718042880.0000 - val_loss: 3528489472.0000\n",
      "Epoch 116/200\n",
      "69/69 [==============================] - 0s 6ms/step - loss: 2733780224.0000 - val_loss: 3330267648.0000\n",
      "Epoch 117/200\n",
      "69/69 [==============================] - 0s 6ms/step - loss: 2707853056.0000 - val_loss: 3315394560.0000\n",
      "Epoch 118/200\n",
      "69/69 [==============================] - 0s 7ms/step - loss: 2721211904.0000 - val_loss: 3315692800.0000\n",
      "Epoch 119/200\n",
      "69/69 [==============================] - 0s 6ms/step - loss: 2697426432.0000 - val_loss: 3816131584.0000\n",
      "Epoch 120/200\n",
      "69/69 [==============================] - 0s 6ms/step - loss: 2705380608.0000 - val_loss: 3373965312.0000\n",
      "Epoch 121/200\n",
      "69/69 [==============================] - 0s 7ms/step - loss: 2698801664.0000 - val_loss: 3461127168.0000\n",
      "Epoch 122/200\n",
      "69/69 [==============================] - 0s 7ms/step - loss: 2700960000.0000 - val_loss: 3354328576.0000\n",
      "Epoch 123/200\n",
      "69/69 [==============================] - 0s 7ms/step - loss: 2706149888.0000 - val_loss: 3291101696.0000\n",
      "Epoch 124/200\n",
      "69/69 [==============================] - 1s 7ms/step - loss: 2695068672.0000 - val_loss: 3461492992.0000\n",
      "Epoch 125/200\n",
      "69/69 [==============================] - 1s 8ms/step - loss: 2688239104.0000 - val_loss: 3410660864.0000\n",
      "Epoch 126/200\n",
      "69/69 [==============================] - 1s 8ms/step - loss: 2663017216.0000 - val_loss: 3366367232.0000\n",
      "Epoch 127/200\n",
      "69/69 [==============================] - 1s 7ms/step - loss: 2661873408.0000 - val_loss: 3327787776.0000\n",
      "Epoch 128/200\n",
      "69/69 [==============================] - 1s 8ms/step - loss: 2655194112.0000 - val_loss: 3317689088.0000\n",
      "Epoch 129/200\n",
      "69/69 [==============================] - 1s 7ms/step - loss: 2675619584.0000 - val_loss: 3360925952.0000\n",
      "Epoch 130/200\n",
      "69/69 [==============================] - 1s 7ms/step - loss: 2635517440.0000 - val_loss: 3459109376.0000\n",
      "Epoch 131/200\n",
      "69/69 [==============================] - 1s 7ms/step - loss: 2644374528.0000 - val_loss: 3401625344.0000\n",
      "Epoch 132/200\n",
      "69/69 [==============================] - 0s 7ms/step - loss: 2642582528.0000 - val_loss: 3370076160.0000\n",
      "Epoch 133/200\n",
      "69/69 [==============================] - 1s 8ms/step - loss: 2635157248.0000 - val_loss: 3413326848.0000\n",
      "Epoch 134/200\n",
      "69/69 [==============================] - 1s 8ms/step - loss: 2653928704.0000 - val_loss: 3284986624.0000\n",
      "Epoch 135/200\n",
      "69/69 [==============================] - 1s 7ms/step - loss: 2605030656.0000 - val_loss: 3337159168.0000\n",
      "Epoch 136/200\n",
      "69/69 [==============================] - 1s 8ms/step - loss: 2629766400.0000 - val_loss: 3562467840.0000\n",
      "Epoch 137/200\n",
      "69/69 [==============================] - 1s 9ms/step - loss: 2601401600.0000 - val_loss: 3306974976.0000\n",
      "Epoch 138/200\n",
      "69/69 [==============================] - 1s 8ms/step - loss: 2627343104.0000 - val_loss: 3341094400.0000\n",
      "Epoch 139/200\n",
      "69/69 [==============================] - 1s 8ms/step - loss: 2643388160.0000 - val_loss: 3224581632.0000\n",
      "Epoch 140/200\n",
      "69/69 [==============================] - 1s 7ms/step - loss: 2611962624.0000 - val_loss: 3383848448.0000\n",
      "Epoch 141/200\n",
      "69/69 [==============================] - 1s 8ms/step - loss: 2630842624.0000 - val_loss: 3466510848.0000\n",
      "Epoch 142/200\n",
      "69/69 [==============================] - 1s 8ms/step - loss: 2609945344.0000 - val_loss: 3285978112.0000\n",
      "Epoch 143/200\n",
      "69/69 [==============================] - 0s 7ms/step - loss: 2607685888.0000 - val_loss: 3333245184.0000\n",
      "Epoch 144/200\n",
      "69/69 [==============================] - 0s 7ms/step - loss: 2586784256.0000 - val_loss: 3445384448.0000\n",
      "Epoch 145/200\n",
      "69/69 [==============================] - 1s 8ms/step - loss: 2586976512.0000 - val_loss: 3552962048.0000\n",
      "Epoch 146/200\n",
      "69/69 [==============================] - 1s 8ms/step - loss: 2588643328.0000 - val_loss: 3277421568.0000\n",
      "Epoch 147/200\n",
      "69/69 [==============================] - 1s 8ms/step - loss: 2576921088.0000 - val_loss: 3369676800.0000\n",
      "Epoch 148/200\n",
      "69/69 [==============================] - 1s 8ms/step - loss: 2584485376.0000 - val_loss: 3432176128.0000\n",
      "Epoch 149/200\n",
      "69/69 [==============================] - 1s 8ms/step - loss: 2561089792.0000 - val_loss: 3228909824.0000\n",
      "Epoch 150/200\n",
      "69/69 [==============================] - 1s 7ms/step - loss: 2585630976.0000 - val_loss: 3366236928.0000\n",
      "Epoch 151/200\n",
      "69/69 [==============================] - 1s 7ms/step - loss: 2573628160.0000 - val_loss: 3417602048.0000\n",
      "Epoch 152/200\n",
      "69/69 [==============================] - 0s 7ms/step - loss: 2552060160.0000 - val_loss: 3419343104.0000\n",
      "Epoch 153/200\n",
      "69/69 [==============================] - 1s 8ms/step - loss: 2548049920.0000 - val_loss: 3388464896.0000\n",
      "Epoch 154/200\n",
      "69/69 [==============================] - 1s 8ms/step - loss: 2577325568.0000 - val_loss: 3582429696.0000\n",
      "Epoch 155/200\n",
      "69/69 [==============================] - 1s 8ms/step - loss: 2575671808.0000 - val_loss: 3437199616.0000\n",
      "Epoch 156/200\n",
      "69/69 [==============================] - 1s 7ms/step - loss: 2575517952.0000 - val_loss: 3336957184.0000\n",
      "Epoch 157/200\n",
      "69/69 [==============================] - 1s 8ms/step - loss: 2577730816.0000 - val_loss: 3462576128.0000\n",
      "Epoch 158/200\n",
      "69/69 [==============================] - 0s 7ms/step - loss: 2596854016.0000 - val_loss: 3248753664.0000\n",
      "Epoch 159/200\n",
      "69/69 [==============================] - 0s 7ms/step - loss: 2529661440.0000 - val_loss: 3262043648.0000\n",
      "Epoch 160/200\n",
      "69/69 [==============================] - 0s 7ms/step - loss: 2532990208.0000 - val_loss: 3566985216.0000\n",
      "Epoch 161/200\n",
      "69/69 [==============================] - 0s 7ms/step - loss: 2541191936.0000 - val_loss: 3342693376.0000\n",
      "Epoch 162/200\n",
      "69/69 [==============================] - 1s 8ms/step - loss: 2538558720.0000 - val_loss: 3325758720.0000\n",
      "Epoch 163/200\n",
      "69/69 [==============================] - 1s 7ms/step - loss: 2526347008.0000 - val_loss: 3298072832.0000\n",
      "Epoch 164/200\n",
      "69/69 [==============================] - 0s 7ms/step - loss: 2545262336.0000 - val_loss: 3380610304.0000\n",
      "Epoch 165/200\n",
      "69/69 [==============================] - 0s 7ms/step - loss: 2534263040.0000 - val_loss: 3338032896.0000\n",
      "Epoch 166/200\n",
      "69/69 [==============================] - 0s 7ms/step - loss: 2508418048.0000 - val_loss: 3369805312.0000\n",
      "Epoch 167/200\n",
      "69/69 [==============================] - 0s 6ms/step - loss: 2503917056.0000 - val_loss: 3355496448.0000\n",
      "Epoch 168/200\n",
      "69/69 [==============================] - 0s 7ms/step - loss: 2515099136.0000 - val_loss: 3340326144.0000\n",
      "Epoch 169/200\n",
      "69/69 [==============================] - 0s 7ms/step - loss: 2518870784.0000 - val_loss: 3551127040.0000\n",
      "Epoch 170/200\n",
      "69/69 [==============================] - 0s 7ms/step - loss: 2514795776.0000 - val_loss: 3390839296.0000\n",
      "Epoch 171/200\n",
      "69/69 [==============================] - 0s 7ms/step - loss: 2533209600.0000 - val_loss: 3362899968.0000\n",
      "Epoch 172/200\n",
      "69/69 [==============================] - 0s 7ms/step - loss: 2508624896.0000 - val_loss: 3309574656.0000\n",
      "Epoch 173/200\n",
      "69/69 [==============================] - 0s 7ms/step - loss: 2510175232.0000 - val_loss: 3480108288.0000\n",
      "Epoch 174/200\n",
      "69/69 [==============================] - 0s 7ms/step - loss: 2500525312.0000 - val_loss: 3459428608.0000\n",
      "Epoch 175/200\n",
      "69/69 [==============================] - 0s 7ms/step - loss: 2515723264.0000 - val_loss: 3584887552.0000\n",
      "Epoch 176/200\n",
      "69/69 [==============================] - 1s 8ms/step - loss: 2499086592.0000 - val_loss: 3452450816.0000\n",
      "Epoch 177/200\n",
      "69/69 [==============================] - 0s 7ms/step - loss: 2486887936.0000 - val_loss: 3555714048.0000\n",
      "Epoch 178/200\n",
      "69/69 [==============================] - 0s 7ms/step - loss: 2496632320.0000 - val_loss: 3374317312.0000\n",
      "Epoch 179/200\n",
      "69/69 [==============================] - 0s 7ms/step - loss: 2477797120.0000 - val_loss: 3302344192.0000\n",
      "Epoch 180/200\n",
      "69/69 [==============================] - 0s 7ms/step - loss: 2486749440.0000 - val_loss: 3508564992.0000\n",
      "Epoch 181/200\n",
      "69/69 [==============================] - 0s 7ms/step - loss: 2495668992.0000 - val_loss: 3429215744.0000\n",
      "Epoch 182/200\n",
      "69/69 [==============================] - 0s 7ms/step - loss: 2477440256.0000 - val_loss: 3383517952.0000\n",
      "Epoch 183/200\n",
      "69/69 [==============================] - 0s 7ms/step - loss: 2504847360.0000 - val_loss: 3436762624.0000\n",
      "Epoch 184/200\n",
      "69/69 [==============================] - 0s 7ms/step - loss: 2469259776.0000 - val_loss: 3327598080.0000\n",
      "Epoch 185/200\n",
      "69/69 [==============================] - 0s 7ms/step - loss: 2457454848.0000 - val_loss: 3299786496.0000\n",
      "Epoch 186/200\n",
      "69/69 [==============================] - 0s 7ms/step - loss: 2495741184.0000 - val_loss: 3428953344.0000\n",
      "Epoch 187/200\n",
      "69/69 [==============================] - 0s 7ms/step - loss: 2460481536.0000 - val_loss: 3459395072.0000\n",
      "Epoch 188/200\n",
      "69/69 [==============================] - 1s 8ms/step - loss: 2459687424.0000 - val_loss: 3284289792.0000\n",
      "Epoch 189/200\n",
      "69/69 [==============================] - 0s 7ms/step - loss: 2446039296.0000 - val_loss: 3439651584.0000\n",
      "Epoch 190/200\n",
      "69/69 [==============================] - 0s 7ms/step - loss: 2440664832.0000 - val_loss: 3262424064.0000\n",
      "Epoch 191/200\n",
      "69/69 [==============================] - 0s 6ms/step - loss: 2467644928.0000 - val_loss: 3480718336.0000\n",
      "Epoch 192/200\n",
      "69/69 [==============================] - 0s 6ms/step - loss: 2454639104.0000 - val_loss: 3525797632.0000\n",
      "Epoch 193/200\n",
      "69/69 [==============================] - 0s 7ms/step - loss: 2484573184.0000 - val_loss: 3745284864.0000\n",
      "Epoch 194/200\n",
      "69/69 [==============================] - 0s 7ms/step - loss: 2456157696.0000 - val_loss: 3378980352.0000\n",
      "Epoch 195/200\n",
      "69/69 [==============================] - 0s 7ms/step - loss: 2455282944.0000 - val_loss: 3441424384.0000\n",
      "Epoch 196/200\n",
      "69/69 [==============================] - 0s 7ms/step - loss: 2433453824.0000 - val_loss: 3344413440.0000\n",
      "Epoch 197/200\n",
      "69/69 [==============================] - 0s 7ms/step - loss: 2458964480.0000 - val_loss: 3426950912.0000\n",
      "Epoch 198/200\n",
      "69/69 [==============================] - 0s 7ms/step - loss: 2427097856.0000 - val_loss: 3483218688.0000\n",
      "Epoch 199/200\n",
      "69/69 [==============================] - 0s 7ms/step - loss: 2429364224.0000 - val_loss: 3481115904.0000\n",
      "Epoch 200/200\n",
      "69/69 [==============================] - 0s 7ms/step - loss: 2417536512.0000 - val_loss: 3777271552.0000\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_87\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_834 (Dense)           (None, 64)                896       \n",
      "                                                                 \n",
      " dense_835 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_836 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_837 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_838 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_839 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_840 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_841 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_842 (Dense)           (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 30,081\n",
      "Trainable params: 30,081\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "35/35 [==============================] - 1s 15ms/step - loss: 56031477760.0000 - val_loss: 56393728000.0000\n",
      "Epoch 2/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 54107475968.0000 - val_loss: 47517687808.0000\n",
      "Epoch 3/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 18652055552.0000 - val_loss: 7373117440.0000\n",
      "Epoch 4/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 6486204416.0000 - val_loss: 4150904832.0000\n",
      "Epoch 5/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 5406972928.0000 - val_loss: 3772954880.0000\n",
      "Epoch 6/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 5022636544.0000 - val_loss: 3656320512.0000\n",
      "Epoch 7/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 4845382656.0000 - val_loss: 3629366016.0000\n",
      "Epoch 8/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 4739098112.0000 - val_loss: 3617606144.0000\n",
      "Epoch 9/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 4666104832.0000 - val_loss: 3591273216.0000\n",
      "Epoch 10/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 4600904192.0000 - val_loss: 3545285632.0000\n",
      "Epoch 11/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 4540943872.0000 - val_loss: 3526232064.0000\n",
      "Epoch 12/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 4479005696.0000 - val_loss: 3493537536.0000\n",
      "Epoch 13/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 4431847424.0000 - val_loss: 3447367936.0000\n",
      "Epoch 14/200\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 4363682304.0000 - val_loss: 3435054592.0000\n",
      "Epoch 15/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 4297710080.0000 - val_loss: 3417799680.0000\n",
      "Epoch 16/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 4251744768.0000 - val_loss: 3419208448.0000\n",
      "Epoch 17/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 4197842176.0000 - val_loss: 3388961024.0000\n",
      "Epoch 18/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 4162424064.0000 - val_loss: 3375329792.0000\n",
      "Epoch 19/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 4100340480.0000 - val_loss: 3366174208.0000\n",
      "Epoch 20/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 4054755840.0000 - val_loss: 3371247104.0000\n",
      "Epoch 21/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 4013102080.0000 - val_loss: 3320234496.0000\n",
      "Epoch 22/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 3975321600.0000 - val_loss: 3363958528.0000\n",
      "Epoch 23/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 3923056384.0000 - val_loss: 3311595264.0000\n",
      "Epoch 24/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 3887605248.0000 - val_loss: 3331087616.0000\n",
      "Epoch 25/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 3848126976.0000 - val_loss: 3301239296.0000\n",
      "Epoch 26/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 3822002688.0000 - val_loss: 3348844800.0000\n",
      "Epoch 27/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 3807452672.0000 - val_loss: 3357324288.0000\n",
      "Epoch 28/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 3768484352.0000 - val_loss: 3317764608.0000\n",
      "Epoch 29/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 3743188480.0000 - val_loss: 3307290880.0000\n",
      "Epoch 30/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 3730237184.0000 - val_loss: 3308736256.0000\n",
      "Epoch 31/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 3692899840.0000 - val_loss: 3323173376.0000\n",
      "Epoch 32/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 3680142592.0000 - val_loss: 3373627392.0000\n",
      "Epoch 33/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 3680823296.0000 - val_loss: 3324043264.0000\n",
      "Epoch 34/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 3678212864.0000 - val_loss: 3328934400.0000\n",
      "Epoch 35/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 3633323776.0000 - val_loss: 3376702976.0000\n",
      "Epoch 36/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 3622663936.0000 - val_loss: 3352734720.0000\n",
      "Epoch 37/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 3609028608.0000 - val_loss: 3348056320.0000\n",
      "Epoch 38/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 3587135744.0000 - val_loss: 3308397568.0000\n",
      "Epoch 39/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 3597163520.0000 - val_loss: 3317744640.0000\n",
      "Epoch 40/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 3570156544.0000 - val_loss: 3324243712.0000\n",
      "Epoch 41/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 3578487296.0000 - val_loss: 3398754304.0000\n",
      "Epoch 42/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 3554657280.0000 - val_loss: 3310473984.0000\n",
      "Epoch 43/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 3521778688.0000 - val_loss: 3480703744.0000\n",
      "Epoch 44/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 3541755136.0000 - val_loss: 3338816256.0000\n",
      "Epoch 45/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 3503965696.0000 - val_loss: 3395796736.0000\n",
      "Epoch 46/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 3521681152.0000 - val_loss: 3373790208.0000\n",
      "Epoch 47/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 3490377728.0000 - val_loss: 3394255104.0000\n",
      "Epoch 48/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 3473761024.0000 - val_loss: 3393478912.0000\n",
      "Epoch 49/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 3468589824.0000 - val_loss: 3471986944.0000\n",
      "Epoch 50/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 3455739392.0000 - val_loss: 3362933248.0000\n",
      "Epoch 51/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 3455265792.0000 - val_loss: 3439054848.0000\n",
      "Epoch 52/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 3455050496.0000 - val_loss: 3517666560.0000\n",
      "Epoch 53/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 3451437568.0000 - val_loss: 3494347776.0000\n",
      "Epoch 54/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 3447854080.0000 - val_loss: 3501057792.0000\n",
      "Epoch 55/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 3413981184.0000 - val_loss: 3429654272.0000\n",
      "Epoch 56/200\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 3387654912.0000 - val_loss: 3465547520.0000\n",
      "Epoch 57/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 3400480000.0000 - val_loss: 3449242368.0000\n",
      "Epoch 58/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 3385569280.0000 - val_loss: 3483765760.0000\n",
      "Epoch 59/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 3372493568.0000 - val_loss: 3462080512.0000\n",
      "Epoch 60/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 3387282432.0000 - val_loss: 3467545088.0000\n",
      "Epoch 61/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 3355853824.0000 - val_loss: 3541718272.0000\n",
      "Epoch 62/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 3371722752.0000 - val_loss: 3476179200.0000\n",
      "Epoch 63/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 3364036864.0000 - val_loss: 3525169920.0000\n",
      "Epoch 64/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 3345995264.0000 - val_loss: 3603341056.0000\n",
      "Epoch 65/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 3331991808.0000 - val_loss: 3545362432.0000\n",
      "Epoch 66/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 3311289344.0000 - val_loss: 3472464384.0000\n",
      "Epoch 67/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 3302961408.0000 - val_loss: 3565857792.0000\n",
      "Epoch 68/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 3320184064.0000 - val_loss: 3667835648.0000\n",
      "Epoch 69/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 3299754240.0000 - val_loss: 3581568512.0000\n",
      "Epoch 70/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 3270726400.0000 - val_loss: 3550136576.0000\n",
      "Epoch 71/200\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 3273469440.0000 - val_loss: 3493029120.0000\n",
      "Epoch 72/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 3256159232.0000 - val_loss: 3457139200.0000\n",
      "Epoch 73/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 3256977664.0000 - val_loss: 3592267520.0000\n",
      "Epoch 74/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 3252239616.0000 - val_loss: 3721895424.0000\n",
      "Epoch 75/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 3263732992.0000 - val_loss: 3642808832.0000\n",
      "Epoch 76/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 3228234752.0000 - val_loss: 3557674496.0000\n",
      "Epoch 77/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 3235898880.0000 - val_loss: 3680736512.0000\n",
      "Epoch 78/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 3211451136.0000 - val_loss: 3518043392.0000\n",
      "Epoch 79/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 3232861184.0000 - val_loss: 3569093632.0000\n",
      "Epoch 80/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 3194952448.0000 - val_loss: 3579562752.0000\n",
      "Epoch 81/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 3193089280.0000 - val_loss: 3596491776.0000\n",
      "Epoch 82/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 3183454464.0000 - val_loss: 3613188352.0000\n",
      "Epoch 83/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 3177867520.0000 - val_loss: 3653782784.0000\n",
      "Epoch 84/200\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 3180973824.0000 - val_loss: 3655536384.0000\n",
      "Epoch 85/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 3164636160.0000 - val_loss: 3575805952.0000\n",
      "Epoch 86/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 3156653056.0000 - val_loss: 3536626688.0000\n",
      "Epoch 87/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 3159249664.0000 - val_loss: 3611482368.0000\n",
      "Epoch 88/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 3137488384.0000 - val_loss: 3590019840.0000\n",
      "Epoch 89/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 3138292224.0000 - val_loss: 3678304256.0000\n",
      "Epoch 90/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 3127233024.0000 - val_loss: 3661643264.0000\n",
      "Epoch 91/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 3113307136.0000 - val_loss: 3583251200.0000\n",
      "Epoch 92/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 3108388864.0000 - val_loss: 3763660288.0000\n",
      "Epoch 93/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 3113372928.0000 - val_loss: 3577624832.0000\n",
      "Epoch 94/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 3095589888.0000 - val_loss: 3562930432.0000\n",
      "Epoch 95/200\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 3096513792.0000 - val_loss: 3699718144.0000\n",
      "Epoch 96/200\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 3108038912.0000 - val_loss: 3650315008.0000\n",
      "Epoch 97/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 3088009728.0000 - val_loss: 3645253632.0000\n",
      "Epoch 98/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 3075523584.0000 - val_loss: 3610098176.0000\n",
      "Epoch 99/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 3055195904.0000 - val_loss: 3701431552.0000\n",
      "Epoch 100/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 3061619456.0000 - val_loss: 3793999616.0000\n",
      "Epoch 101/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 3058688512.0000 - val_loss: 3593601024.0000\n",
      "Epoch 102/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 3058432000.0000 - val_loss: 3553614848.0000\n",
      "Epoch 103/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 3035803904.0000 - val_loss: 3652335104.0000\n",
      "Epoch 104/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 3039720704.0000 - val_loss: 3708673024.0000\n",
      "Epoch 105/200\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 3033899776.0000 - val_loss: 3522550784.0000\n",
      "Epoch 106/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 3015040000.0000 - val_loss: 3605510912.0000\n",
      "Epoch 107/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 2990049280.0000 - val_loss: 3772647168.0000\n",
      "Epoch 108/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 2999447040.0000 - val_loss: 3686593024.0000\n",
      "Epoch 109/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 2994626816.0000 - val_loss: 3718725888.0000\n",
      "Epoch 110/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 3001156864.0000 - val_loss: 3710288640.0000\n",
      "Epoch 111/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 2970950144.0000 - val_loss: 3546462976.0000\n",
      "Epoch 112/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 2980238080.0000 - val_loss: 3725239296.0000\n",
      "Epoch 113/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 2994394112.0000 - val_loss: 3592671232.0000\n",
      "Epoch 114/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 2975878400.0000 - val_loss: 3661638656.0000\n",
      "Epoch 115/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 2990295552.0000 - val_loss: 3516772864.0000\n",
      "Epoch 116/200\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 2962428160.0000 - val_loss: 3625983488.0000\n",
      "Epoch 117/200\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 2950727680.0000 - val_loss: 3533709824.0000\n",
      "Epoch 118/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 2985681408.0000 - val_loss: 3638340864.0000\n",
      "Epoch 119/200\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 2923014144.0000 - val_loss: 3746724352.0000\n",
      "Epoch 120/200\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 2939293696.0000 - val_loss: 3534142720.0000\n",
      "Epoch 121/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 2930663680.0000 - val_loss: 3469686528.0000\n",
      "Epoch 122/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 2905348608.0000 - val_loss: 3521476608.0000\n",
      "Epoch 123/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 2916495616.0000 - val_loss: 3512981248.0000\n",
      "Epoch 124/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 2941878528.0000 - val_loss: 3482137600.0000\n",
      "Epoch 125/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 2899808256.0000 - val_loss: 3466290176.0000\n",
      "Epoch 126/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 2897711360.0000 - val_loss: 3504281344.0000\n",
      "Epoch 127/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 2888573184.0000 - val_loss: 3500871936.0000\n",
      "Epoch 128/200\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 2898791424.0000 - val_loss: 3558455808.0000\n",
      "Epoch 129/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 2886731520.0000 - val_loss: 3707008000.0000\n",
      "Epoch 130/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 2873045760.0000 - val_loss: 3519739136.0000\n",
      "Epoch 131/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 2864820736.0000 - val_loss: 3503112960.0000\n",
      "Epoch 132/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 2863438592.0000 - val_loss: 3511490304.0000\n",
      "Epoch 133/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 2863602432.0000 - val_loss: 3711046656.0000\n",
      "Epoch 134/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 2848633856.0000 - val_loss: 3528778752.0000\n",
      "Epoch 135/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 2821426688.0000 - val_loss: 3593990400.0000\n",
      "Epoch 136/200\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 2829229824.0000 - val_loss: 3657527296.0000\n",
      "Epoch 137/200\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 2857057536.0000 - val_loss: 3461249536.0000\n",
      "Epoch 138/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 2857252608.0000 - val_loss: 3516297728.0000\n",
      "Epoch 139/200\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 2839047168.0000 - val_loss: 3487707648.0000\n",
      "Epoch 140/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 2810659584.0000 - val_loss: 3457739008.0000\n",
      "Epoch 141/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 2834061312.0000 - val_loss: 3567922688.0000\n",
      "Epoch 142/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 2848426752.0000 - val_loss: 3451076864.0000\n",
      "Epoch 143/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 2808980480.0000 - val_loss: 3628901120.0000\n",
      "Epoch 144/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 2842318592.0000 - val_loss: 3631661056.0000\n",
      "Epoch 145/200\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 2798619904.0000 - val_loss: 3549492736.0000\n",
      "Epoch 146/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 2790894848.0000 - val_loss: 3276037632.0000\n",
      "Epoch 147/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 2813117696.0000 - val_loss: 3368103936.0000\n",
      "Epoch 148/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 2783492096.0000 - val_loss: 3595722496.0000\n",
      "Epoch 149/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 2800028672.0000 - val_loss: 3563620864.0000\n",
      "Epoch 150/200\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 2779657472.0000 - val_loss: 3719712000.0000\n",
      "Epoch 151/200\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 2780951552.0000 - val_loss: 3431220992.0000\n",
      "Epoch 152/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 2749696768.0000 - val_loss: 3554607872.0000\n",
      "Epoch 153/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 2756339712.0000 - val_loss: 3446855680.0000\n",
      "Epoch 154/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 2770213888.0000 - val_loss: 3658287872.0000\n",
      "Epoch 155/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 2772764672.0000 - val_loss: 3611312384.0000\n",
      "Epoch 156/200\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 2744125952.0000 - val_loss: 3778006784.0000\n",
      "Epoch 157/200\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 2754264576.0000 - val_loss: 3494627840.0000\n",
      "Epoch 158/200\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 2755948800.0000 - val_loss: 3681517056.0000\n",
      "Epoch 159/200\n",
      "35/35 [==============================] - 0s 11ms/step - loss: 2753218304.0000 - val_loss: 3450394880.0000\n",
      "Epoch 160/200\n",
      "35/35 [==============================] - 0s 11ms/step - loss: 2782669056.0000 - val_loss: 3714797568.0000\n",
      "Epoch 161/200\n",
      "35/35 [==============================] - 0s 10ms/step - loss: 2734926592.0000 - val_loss: 3465816320.0000\n",
      "Epoch 162/200\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 2731764224.0000 - val_loss: 3398209536.0000\n",
      "Epoch 163/200\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 2710683136.0000 - val_loss: 3351026944.0000\n",
      "Epoch 164/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 2713355008.0000 - val_loss: 3409835776.0000\n",
      "Epoch 165/200\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 2722813696.0000 - val_loss: 3338594816.0000\n",
      "Epoch 166/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 2718745856.0000 - val_loss: 3494096128.0000\n",
      "Epoch 167/200\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 2704692992.0000 - val_loss: 3479519488.0000\n",
      "Epoch 168/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 2711210752.0000 - val_loss: 3546650880.0000\n",
      "Epoch 169/200\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 2709182720.0000 - val_loss: 3454817536.0000\n",
      "Epoch 170/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 2715041024.0000 - val_loss: 3472175872.0000\n",
      "Epoch 171/200\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 2714705152.0000 - val_loss: 3534179584.0000\n",
      "Epoch 172/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 2698938624.0000 - val_loss: 3391193344.0000\n",
      "Epoch 173/200\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 2695168512.0000 - val_loss: 3390920704.0000\n",
      "Epoch 174/200\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 2697198080.0000 - val_loss: 3473150976.0000\n",
      "Epoch 175/200\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 2687501824.0000 - val_loss: 3485859072.0000\n",
      "Epoch 176/200\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 2685084672.0000 - val_loss: 3459143424.0000\n",
      "Epoch 177/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 2685385728.0000 - val_loss: 3396621824.0000\n",
      "Epoch 178/200\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 2673136384.0000 - val_loss: 3457465088.0000\n",
      "Epoch 179/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 2675632640.0000 - val_loss: 3489671680.0000\n",
      "Epoch 180/200\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 2650707968.0000 - val_loss: 3546961152.0000\n",
      "Epoch 181/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 2657290496.0000 - val_loss: 3600042496.0000\n",
      "Epoch 182/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 2666531840.0000 - val_loss: 3545869056.0000\n",
      "Epoch 183/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 2684801792.0000 - val_loss: 3536771072.0000\n",
      "Epoch 184/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 2663252480.0000 - val_loss: 3480051712.0000\n",
      "Epoch 185/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 2629619456.0000 - val_loss: 3407389952.0000\n",
      "Epoch 186/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 2700565760.0000 - val_loss: 3530573056.0000\n",
      "Epoch 187/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 2662928640.0000 - val_loss: 3547962624.0000\n",
      "Epoch 188/200\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 2643616768.0000 - val_loss: 3403612416.0000\n",
      "Epoch 189/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 2637543680.0000 - val_loss: 3417403392.0000\n",
      "Epoch 190/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 2657616128.0000 - val_loss: 3505881600.0000\n",
      "Epoch 191/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 2670310656.0000 - val_loss: 3453246208.0000\n",
      "Epoch 192/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 2635259904.0000 - val_loss: 3449325568.0000\n",
      "Epoch 193/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 2632618496.0000 - val_loss: 3552348160.0000\n",
      "Epoch 194/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 2605971712.0000 - val_loss: 3490172928.0000\n",
      "Epoch 195/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 2636451840.0000 - val_loss: 3699700480.0000\n",
      "Epoch 196/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 2611415552.0000 - val_loss: 3539435520.0000\n",
      "Epoch 197/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 2653094400.0000 - val_loss: 3491813888.0000\n",
      "Epoch 198/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 2609442816.0000 - val_loss: 3473317632.0000\n",
      "Epoch 199/200\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 2601703936.0000 - val_loss: 3410052352.0000\n",
      "Epoch 200/200\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 2623035136.0000 - val_loss: 3745726208.0000\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_88\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_843 (Dense)           (None, 64)                896       \n",
      "                                                                 \n",
      " dense_844 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_845 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_846 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_847 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_848 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_849 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_850 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_851 (Dense)           (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 30,081\n",
      "Trainable params: 30,081\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "18/18 [==============================] - 1s 22ms/step - loss: 56041332736.0000 - val_loss: 56449650688.0000\n",
      "Epoch 2/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 56019034112.0000 - val_loss: 56385601536.0000\n",
      "Epoch 3/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 55711899648.0000 - val_loss: 55492173824.0000\n",
      "Epoch 4/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 51509108736.0000 - val_loss: 45404930048.0000\n",
      "Epoch 5/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 24229226496.0000 - val_loss: 7371743744.0000\n",
      "Epoch 6/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 9327021056.0000 - val_loss: 6428758016.0000\n",
      "Epoch 7/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 6631712768.0000 - val_loss: 4354770432.0000\n",
      "Epoch 8/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 5853849088.0000 - val_loss: 4013381888.0000\n",
      "Epoch 9/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 5439366144.0000 - val_loss: 3831770112.0000\n",
      "Epoch 10/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 5190377984.0000 - val_loss: 3731453440.0000\n",
      "Epoch 11/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 5035831296.0000 - val_loss: 3668952576.0000\n",
      "Epoch 12/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 4924657152.0000 - val_loss: 3653610240.0000\n",
      "Epoch 13/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 4846279680.0000 - val_loss: 3628109312.0000\n",
      "Epoch 14/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 4791712256.0000 - val_loss: 3630755584.0000\n",
      "Epoch 15/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 4747902464.0000 - val_loss: 3612714240.0000\n",
      "Epoch 16/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 4711611392.0000 - val_loss: 3592683264.0000\n",
      "Epoch 17/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 4669481984.0000 - val_loss: 3585804544.0000\n",
      "Epoch 18/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 4629150208.0000 - val_loss: 3552355072.0000\n",
      "Epoch 19/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 4587838976.0000 - val_loss: 3547068160.0000\n",
      "Epoch 20/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 4557316608.0000 - val_loss: 3535804416.0000\n",
      "Epoch 21/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 4518468608.0000 - val_loss: 3519808768.0000\n",
      "Epoch 22/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 4483753984.0000 - val_loss: 3513730816.0000\n",
      "Epoch 23/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 4449708032.0000 - val_loss: 3496700928.0000\n",
      "Epoch 24/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 4424692224.0000 - val_loss: 3505111040.0000\n",
      "Epoch 25/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 4390986240.0000 - val_loss: 3456796160.0000\n",
      "Epoch 26/200\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 4355513344.0000 - val_loss: 3436877568.0000\n",
      "Epoch 27/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 4328528896.0000 - val_loss: 3435743232.0000\n",
      "Epoch 28/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 4299454464.0000 - val_loss: 3413948672.0000\n",
      "Epoch 29/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 4265536512.0000 - val_loss: 3413373952.0000\n",
      "Epoch 30/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 4230052096.0000 - val_loss: 3418395392.0000\n",
      "Epoch 31/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 4205582592.0000 - val_loss: 3401909504.0000\n",
      "Epoch 32/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 4173053696.0000 - val_loss: 3371186432.0000\n",
      "Epoch 33/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 4152431616.0000 - val_loss: 3399743232.0000\n",
      "Epoch 34/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 4117501184.0000 - val_loss: 3366104832.0000\n",
      "Epoch 35/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 4083390720.0000 - val_loss: 3363685376.0000\n",
      "Epoch 36/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 4058175232.0000 - val_loss: 3364940032.0000\n",
      "Epoch 37/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 4042661120.0000 - val_loss: 3374280960.0000\n",
      "Epoch 38/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 4005344000.0000 - val_loss: 3348792064.0000\n",
      "Epoch 39/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 3993320704.0000 - val_loss: 3316376320.0000\n",
      "Epoch 40/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 3959298304.0000 - val_loss: 3332320000.0000\n",
      "Epoch 41/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 3927215616.0000 - val_loss: 3336916224.0000\n",
      "Epoch 42/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 3896790784.0000 - val_loss: 3325984768.0000\n",
      "Epoch 43/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 3878684416.0000 - val_loss: 3327615232.0000\n",
      "Epoch 44/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 3866555136.0000 - val_loss: 3367435520.0000\n",
      "Epoch 45/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 3832368640.0000 - val_loss: 3355030784.0000\n",
      "Epoch 46/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 3816569088.0000 - val_loss: 3328195072.0000\n",
      "Epoch 47/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 3817606400.0000 - val_loss: 3350765312.0000\n",
      "Epoch 48/200\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 3774806784.0000 - val_loss: 3336248064.0000\n",
      "Epoch 49/200\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 3758961152.0000 - val_loss: 3338672640.0000\n",
      "Epoch 50/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 3739939840.0000 - val_loss: 3332645888.0000\n",
      "Epoch 51/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 3742861568.0000 - val_loss: 3328935936.0000\n",
      "Epoch 52/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 3740308480.0000 - val_loss: 3357867264.0000\n",
      "Epoch 53/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 3720276480.0000 - val_loss: 3316324096.0000\n",
      "Epoch 54/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 3698004224.0000 - val_loss: 3331247616.0000\n",
      "Epoch 55/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 3686235392.0000 - val_loss: 3292227840.0000\n",
      "Epoch 56/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 3669315328.0000 - val_loss: 3329500160.0000\n",
      "Epoch 57/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 3668032768.0000 - val_loss: 3329107456.0000\n",
      "Epoch 58/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 3647756544.0000 - val_loss: 3309545728.0000\n",
      "Epoch 59/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 3636130560.0000 - val_loss: 3332600320.0000\n",
      "Epoch 60/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 3647031552.0000 - val_loss: 3398855168.0000\n",
      "Epoch 61/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 3631057408.0000 - val_loss: 3283532544.0000\n",
      "Epoch 62/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 3621537280.0000 - val_loss: 3295567616.0000\n",
      "Epoch 63/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 3621178368.0000 - val_loss: 3292121600.0000\n",
      "Epoch 64/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 3601911040.0000 - val_loss: 3344329216.0000\n",
      "Epoch 65/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 3586266624.0000 - val_loss: 3328906496.0000\n",
      "Epoch 66/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 3578307584.0000 - val_loss: 3313357568.0000\n",
      "Epoch 67/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 3597297152.0000 - val_loss: 3323633408.0000\n",
      "Epoch 68/200\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 3578828544.0000 - val_loss: 3348529920.0000\n",
      "Epoch 69/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 3566674176.0000 - val_loss: 3310004480.0000\n",
      "Epoch 70/200\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 3558509312.0000 - val_loss: 3313462528.0000\n",
      "Epoch 71/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 3571108352.0000 - val_loss: 3296158464.0000\n",
      "Epoch 72/200\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 3546661888.0000 - val_loss: 3326433280.0000\n",
      "Epoch 73/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 3530122752.0000 - val_loss: 3313510912.0000\n",
      "Epoch 74/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 3528752128.0000 - val_loss: 3385240320.0000\n",
      "Epoch 75/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 3529873920.0000 - val_loss: 3375575552.0000\n",
      "Epoch 76/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 3530950912.0000 - val_loss: 3391613952.0000\n",
      "Epoch 77/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 3548507648.0000 - val_loss: 3325199104.0000\n",
      "Epoch 78/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 3497248000.0000 - val_loss: 3325315584.0000\n",
      "Epoch 79/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 3506475520.0000 - val_loss: 3350719488.0000\n",
      "Epoch 80/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 3488491264.0000 - val_loss: 3338947072.0000\n",
      "Epoch 81/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 3472900864.0000 - val_loss: 3364613376.0000\n",
      "Epoch 82/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 3472431360.0000 - val_loss: 3385523712.0000\n",
      "Epoch 83/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 3477153280.0000 - val_loss: 3403981824.0000\n",
      "Epoch 84/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 3476436224.0000 - val_loss: 3399339520.0000\n",
      "Epoch 85/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 3464182528.0000 - val_loss: 3365942528.0000\n",
      "Epoch 86/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 3459099648.0000 - val_loss: 3350157056.0000\n",
      "Epoch 87/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 3437269504.0000 - val_loss: 3401093888.0000\n",
      "Epoch 88/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 3479571968.0000 - val_loss: 3378766848.0000\n",
      "Epoch 89/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 3450796032.0000 - val_loss: 3439835904.0000\n",
      "Epoch 90/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 3441399808.0000 - val_loss: 3428480512.0000\n",
      "Epoch 91/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 3413071616.0000 - val_loss: 3357234432.0000\n",
      "Epoch 92/200\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 3402220032.0000 - val_loss: 3455294720.0000\n",
      "Epoch 93/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 3407708160.0000 - val_loss: 3379767296.0000\n",
      "Epoch 94/200\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 3395474944.0000 - val_loss: 3437899264.0000\n",
      "Epoch 95/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 3392378624.0000 - val_loss: 3449638144.0000\n",
      "Epoch 96/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 3392030976.0000 - val_loss: 3456434944.0000\n",
      "Epoch 97/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 3383819520.0000 - val_loss: 3426449920.0000\n",
      "Epoch 98/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 3387132160.0000 - val_loss: 3499325440.0000\n",
      "Epoch 99/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 3371065600.0000 - val_loss: 3503436032.0000\n",
      "Epoch 100/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 3361766912.0000 - val_loss: 3498805760.0000\n",
      "Epoch 101/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 3381086464.0000 - val_loss: 3547768576.0000\n",
      "Epoch 102/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 3363364096.0000 - val_loss: 3470363648.0000\n",
      "Epoch 103/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 3341004800.0000 - val_loss: 3482873600.0000\n",
      "Epoch 104/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 3346264320.0000 - val_loss: 3455551488.0000\n",
      "Epoch 105/200\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 3345919744.0000 - val_loss: 3421344000.0000\n",
      "Epoch 106/200\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 3333376768.0000 - val_loss: 3409022976.0000\n",
      "Epoch 107/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 3345245440.0000 - val_loss: 3424403968.0000\n",
      "Epoch 108/200\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 3318602496.0000 - val_loss: 3385891072.0000\n",
      "Epoch 109/200\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 3313498112.0000 - val_loss: 3434857984.0000\n",
      "Epoch 110/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 3313376256.0000 - val_loss: 3479495168.0000\n",
      "Epoch 111/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 3313209856.0000 - val_loss: 3464563968.0000\n",
      "Epoch 112/200\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 3316448768.0000 - val_loss: 3552015360.0000\n",
      "Epoch 113/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 3357361408.0000 - val_loss: 3460663552.0000\n",
      "Epoch 114/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 3336116480.0000 - val_loss: 3502409472.0000\n",
      "Epoch 115/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 3303297792.0000 - val_loss: 3507920640.0000\n",
      "Epoch 116/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 3287392000.0000 - val_loss: 3554299136.0000\n",
      "Epoch 117/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 3275544064.0000 - val_loss: 3502221568.0000\n",
      "Epoch 118/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 3276487168.0000 - val_loss: 3480793088.0000\n",
      "Epoch 119/200\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 3264760576.0000 - val_loss: 3563744768.0000\n",
      "Epoch 120/200\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 3258510592.0000 - val_loss: 3473213952.0000\n",
      "Epoch 121/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 3259568640.0000 - val_loss: 3430099712.0000\n",
      "Epoch 122/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 3249731072.0000 - val_loss: 3491713280.0000\n",
      "Epoch 123/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 3257545728.0000 - val_loss: 3475439872.0000\n",
      "Epoch 124/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 3250139392.0000 - val_loss: 3565064960.0000\n",
      "Epoch 125/200\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 3241769984.0000 - val_loss: 3475641600.0000\n",
      "Epoch 126/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 3237953024.0000 - val_loss: 3478715904.0000\n",
      "Epoch 127/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 3227130368.0000 - val_loss: 3490110464.0000\n",
      "Epoch 128/200\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 3236214016.0000 - val_loss: 3544179200.0000\n",
      "Epoch 129/200\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 3217660416.0000 - val_loss: 3547846400.0000\n",
      "Epoch 130/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 3213544192.0000 - val_loss: 3590477312.0000\n",
      "Epoch 131/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 3224522496.0000 - val_loss: 3468088320.0000\n",
      "Epoch 132/200\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 3219326720.0000 - val_loss: 3481917440.0000\n",
      "Epoch 133/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 3190877952.0000 - val_loss: 3530330624.0000\n",
      "Epoch 134/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 3196193536.0000 - val_loss: 3528310784.0000\n",
      "Epoch 135/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 3181882624.0000 - val_loss: 3602542080.0000\n",
      "Epoch 136/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 3200202240.0000 - val_loss: 3735307520.0000\n",
      "Epoch 137/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 3199401472.0000 - val_loss: 3574827264.0000\n",
      "Epoch 138/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 3177783296.0000 - val_loss: 3464890112.0000\n",
      "Epoch 139/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 3164706304.0000 - val_loss: 3593829888.0000\n",
      "Epoch 140/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 3156688384.0000 - val_loss: 3508716544.0000\n",
      "Epoch 141/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 3152744704.0000 - val_loss: 3592065024.0000\n",
      "Epoch 142/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 3155712768.0000 - val_loss: 3513571584.0000\n",
      "Epoch 143/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 3156047360.0000 - val_loss: 3535387904.0000\n",
      "Epoch 144/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 3167162880.0000 - val_loss: 3536369920.0000\n",
      "Epoch 145/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 3153780480.0000 - val_loss: 3558423552.0000\n",
      "Epoch 146/200\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 3142031104.0000 - val_loss: 3437731584.0000\n",
      "Epoch 147/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 3157983232.0000 - val_loss: 3577110016.0000\n",
      "Epoch 148/200\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 3128136704.0000 - val_loss: 3602475008.0000\n",
      "Epoch 149/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 3152548608.0000 - val_loss: 3636519424.0000\n",
      "Epoch 150/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 3127222016.0000 - val_loss: 3499395584.0000\n",
      "Epoch 151/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 3106383104.0000 - val_loss: 3527483904.0000\n",
      "Epoch 152/200\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 3132840448.0000 - val_loss: 3530501888.0000\n",
      "Epoch 153/200\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 3118590208.0000 - val_loss: 3635148032.0000\n",
      "Epoch 154/200\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 3104138240.0000 - val_loss: 3581130240.0000\n",
      "Epoch 155/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 3083642880.0000 - val_loss: 3504347648.0000\n",
      "Epoch 156/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 3077997056.0000 - val_loss: 3589001216.0000\n",
      "Epoch 157/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 3097426944.0000 - val_loss: 3541163520.0000\n",
      "Epoch 158/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 3079457792.0000 - val_loss: 3528034560.0000\n",
      "Epoch 159/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 3066200832.0000 - val_loss: 3502768384.0000\n",
      "Epoch 160/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 3058641664.0000 - val_loss: 3575998720.0000\n",
      "Epoch 161/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 3076021760.0000 - val_loss: 3576867840.0000\n",
      "Epoch 162/200\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 3063551232.0000 - val_loss: 3808358912.0000\n",
      "Epoch 163/200\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 3074462720.0000 - val_loss: 3661696512.0000\n",
      "Epoch 164/200\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 3048235264.0000 - val_loss: 3564440832.0000\n",
      "Epoch 165/200\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 3052821248.0000 - val_loss: 3590962688.0000\n",
      "Epoch 166/200\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 3031354880.0000 - val_loss: 3657917440.0000\n",
      "Epoch 167/200\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 3034333184.0000 - val_loss: 3683088640.0000\n",
      "Epoch 168/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 3052563456.0000 - val_loss: 3527464448.0000\n",
      "Epoch 169/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 3043679232.0000 - val_loss: 3538908672.0000\n",
      "Epoch 170/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 3035659008.0000 - val_loss: 3567850240.0000\n",
      "Epoch 171/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 3014061056.0000 - val_loss: 3497606912.0000\n",
      "Epoch 172/200\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 3030845184.0000 - val_loss: 3614288384.0000\n",
      "Epoch 173/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 3052455680.0000 - val_loss: 3582794752.0000\n",
      "Epoch 174/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 2989768192.0000 - val_loss: 3598649600.0000\n",
      "Epoch 175/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 3029834240.0000 - val_loss: 3578113536.0000\n",
      "Epoch 176/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 2989860864.0000 - val_loss: 3499496448.0000\n",
      "Epoch 177/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 2981576192.0000 - val_loss: 3552580352.0000\n",
      "Epoch 178/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 2985677312.0000 - val_loss: 3653202432.0000\n",
      "Epoch 179/200\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 3000539392.0000 - val_loss: 3510676480.0000\n",
      "Epoch 180/200\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 2961752576.0000 - val_loss: 3717153536.0000\n",
      "Epoch 181/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 2962059008.0000 - val_loss: 3600305664.0000\n",
      "Epoch 182/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 2962925056.0000 - val_loss: 3690359040.0000\n",
      "Epoch 183/200\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 2961214720.0000 - val_loss: 3674861568.0000\n",
      "Epoch 184/200\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 2965379328.0000 - val_loss: 3604291072.0000\n",
      "Epoch 185/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 2963817472.0000 - val_loss: 3455073280.0000\n",
      "Epoch 186/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 2948583168.0000 - val_loss: 3448338432.0000\n",
      "Epoch 187/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 2968715264.0000 - val_loss: 3512807168.0000\n",
      "Epoch 188/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 2939868928.0000 - val_loss: 3451259904.0000\n",
      "Epoch 189/200\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 2942935808.0000 - val_loss: 3474944000.0000\n",
      "Epoch 190/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 2929535232.0000 - val_loss: 3468527104.0000\n",
      "Epoch 191/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 2923035904.0000 - val_loss: 3508108544.0000\n",
      "Epoch 192/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 2934568960.0000 - val_loss: 3618580480.0000\n",
      "Epoch 193/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 2924981504.0000 - val_loss: 3776858880.0000\n",
      "Epoch 194/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 2914484224.0000 - val_loss: 3586832128.0000\n",
      "Epoch 195/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 2922049024.0000 - val_loss: 3560830208.0000\n",
      "Epoch 196/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 2910699520.0000 - val_loss: 3449829888.0000\n",
      "Epoch 197/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 2906959104.0000 - val_loss: 3442533376.0000\n",
      "Epoch 198/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 2892652800.0000 - val_loss: 3535376640.0000\n",
      "Epoch 199/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 2925944320.0000 - val_loss: 3500980480.0000\n",
      "Epoch 200/200\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 2905748992.0000 - val_loss: 3540587264.0000\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_89\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_852 (Dense)           (None, 64)                896       \n",
      "                                                                 \n",
      " dense_853 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_854 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_855 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_856 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_857 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_858 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_859 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_860 (Dense)           (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 30,081\n",
      "Trainable params: 30,081\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "9/9 [==============================] - 1s 41ms/step - loss: 56042557440.0000 - val_loss: 56454524928.0000\n",
      "Epoch 2/200\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 56039854080.0000 - val_loss: 56449638400.0000\n",
      "Epoch 3/200\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 56031318016.0000 - val_loss: 56435015680.0000\n",
      "Epoch 4/200\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 56003772416.0000 - val_loss: 56384872448.0000\n",
      "Epoch 5/200\n",
      "9/9 [==============================] - 0s 14ms/step - loss: 55897268224.0000 - val_loss: 56193626112.0000\n",
      "Epoch 6/200\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 55471599616.0000 - val_loss: 55473053696.0000\n",
      "Epoch 7/200\n",
      "9/9 [==============================] - 0s 14ms/step - loss: 53861625856.0000 - val_loss: 52949155840.0000\n",
      "Epoch 8/200\n",
      "9/9 [==============================] - 0s 14ms/step - loss: 48395337728.0000 - val_loss: 45054664704.0000\n",
      "Epoch 9/200\n",
      "9/9 [==============================] - 0s 14ms/step - loss: 33207183360.0000 - val_loss: 25264173056.0000\n",
      "Epoch 10/200\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 13357693952.0000 - val_loss: 7377115648.0000\n",
      "Epoch 11/200\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 10464911360.0000 - val_loss: 7340193280.0000\n",
      "Epoch 12/200\n",
      "9/9 [==============================] - 0s 14ms/step - loss: 7992610816.0000 - val_loss: 6396325888.0000\n",
      "Epoch 13/200\n",
      "9/9 [==============================] - 0s 14ms/step - loss: 6836013568.0000 - val_loss: 4649120768.0000\n",
      "Epoch 14/200\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 6403924992.0000 - val_loss: 4347772928.0000\n",
      "Epoch 15/200\n",
      "9/9 [==============================] - 0s 14ms/step - loss: 5951185920.0000 - val_loss: 4219983360.0000\n",
      "Epoch 16/200\n",
      "9/9 [==============================] - 0s 14ms/step - loss: 5682485248.0000 - val_loss: 4000987136.0000\n",
      "Epoch 17/200\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 5475888640.0000 - val_loss: 3885725696.0000\n",
      "Epoch 18/200\n",
      "9/9 [==============================] - 0s 14ms/step - loss: 5317228544.0000 - val_loss: 3814477312.0000\n",
      "Epoch 19/200\n",
      "9/9 [==============================] - 0s 14ms/step - loss: 5193288192.0000 - val_loss: 3751037440.0000\n",
      "Epoch 20/200\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 5093169152.0000 - val_loss: 3710224384.0000\n",
      "Epoch 21/200\n",
      "9/9 [==============================] - 0s 14ms/step - loss: 5013143552.0000 - val_loss: 3680898816.0000\n",
      "Epoch 22/200\n",
      "9/9 [==============================] - 0s 14ms/step - loss: 4947952640.0000 - val_loss: 3665285120.0000\n",
      "Epoch 23/200\n",
      "9/9 [==============================] - 0s 14ms/step - loss: 4893927424.0000 - val_loss: 3648237312.0000\n",
      "Epoch 24/200\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 4847905792.0000 - val_loss: 3636327680.0000\n",
      "Epoch 25/200\n",
      "9/9 [==============================] - 0s 14ms/step - loss: 4811598336.0000 - val_loss: 3638248448.0000\n",
      "Epoch 26/200\n",
      "9/9 [==============================] - 0s 14ms/step - loss: 4782138880.0000 - val_loss: 3628025600.0000\n",
      "Epoch 27/200\n",
      "9/9 [==============================] - 0s 14ms/step - loss: 4753629696.0000 - val_loss: 3628098816.0000\n",
      "Epoch 28/200\n",
      "9/9 [==============================] - 0s 14ms/step - loss: 4730658304.0000 - val_loss: 3620684032.0000\n",
      "Epoch 29/200\n",
      "9/9 [==============================] - 0s 14ms/step - loss: 4708187136.0000 - val_loss: 3622513664.0000\n",
      "Epoch 30/200\n",
      "9/9 [==============================] - 0s 14ms/step - loss: 4686848000.0000 - val_loss: 3615017984.0000\n",
      "Epoch 31/200\n",
      "9/9 [==============================] - 0s 14ms/step - loss: 4666263552.0000 - val_loss: 3606636032.0000\n",
      "Epoch 32/200\n",
      "9/9 [==============================] - 0s 14ms/step - loss: 4645228032.0000 - val_loss: 3589473024.0000\n",
      "Epoch 33/200\n",
      "9/9 [==============================] - 0s 14ms/step - loss: 4627109888.0000 - val_loss: 3579736064.0000\n",
      "Epoch 34/200\n",
      "9/9 [==============================] - 0s 14ms/step - loss: 4604846592.0000 - val_loss: 3582075904.0000\n",
      "Epoch 35/200\n",
      "9/9 [==============================] - 0s 14ms/step - loss: 4586934784.0000 - val_loss: 3572009984.0000\n",
      "Epoch 36/200\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 4565885440.0000 - val_loss: 3556981504.0000\n",
      "Epoch 37/200\n",
      "9/9 [==============================] - 0s 14ms/step - loss: 4545643520.0000 - val_loss: 3549237760.0000\n",
      "Epoch 38/200\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 4525336576.0000 - val_loss: 3540653568.0000\n",
      "Epoch 39/200\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 4506332160.0000 - val_loss: 3531112960.0000\n",
      "Epoch 40/200\n",
      "9/9 [==============================] - 0s 14ms/step - loss: 4486246400.0000 - val_loss: 3522808832.0000\n",
      "Epoch 41/200\n",
      "9/9 [==============================] - 0s 14ms/step - loss: 4468305408.0000 - val_loss: 3514497536.0000\n",
      "Epoch 42/200\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 4446949376.0000 - val_loss: 3502908416.0000\n",
      "Epoch 43/200\n",
      "9/9 [==============================] - 0s 14ms/step - loss: 4430336000.0000 - val_loss: 3499140864.0000\n",
      "Epoch 44/200\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 4410928640.0000 - val_loss: 3483264512.0000\n",
      "Epoch 45/200\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 4396667392.0000 - val_loss: 3481011200.0000\n",
      "Epoch 46/200\n",
      "9/9 [==============================] - 0s 14ms/step - loss: 4373440512.0000 - val_loss: 3471695360.0000\n",
      "Epoch 47/200\n",
      "9/9 [==============================] - 0s 14ms/step - loss: 4355267584.0000 - val_loss: 3471503872.0000\n",
      "Epoch 48/200\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 4335132672.0000 - val_loss: 3455502592.0000\n",
      "Epoch 49/200\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 4317536768.0000 - val_loss: 3440820480.0000\n",
      "Epoch 50/200\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 4299928576.0000 - val_loss: 3447482112.0000\n",
      "Epoch 51/200\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 4283577344.0000 - val_loss: 3441035008.0000\n",
      "Epoch 52/200\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 4264748032.0000 - val_loss: 3431216384.0000\n",
      "Epoch 53/200\n",
      "9/9 [==============================] - 0s 14ms/step - loss: 4246833152.0000 - val_loss: 3419137024.0000\n",
      "Epoch 54/200\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 4231970048.0000 - val_loss: 3415917056.0000\n",
      "Epoch 55/200\n",
      "9/9 [==============================] - 0s 14ms/step - loss: 4215506944.0000 - val_loss: 3427005184.0000\n",
      "Epoch 56/200\n",
      "9/9 [==============================] - 0s 14ms/step - loss: 4196097536.0000 - val_loss: 3408936192.0000\n",
      "Epoch 57/200\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 4178646016.0000 - val_loss: 3407162112.0000\n",
      "Epoch 58/200\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 4162013696.0000 - val_loss: 3407583488.0000\n",
      "Epoch 59/200\n",
      "9/9 [==============================] - 0s 14ms/step - loss: 4145369600.0000 - val_loss: 3401391104.0000\n",
      "Epoch 60/200\n",
      "9/9 [==============================] - 0s 14ms/step - loss: 4128249088.0000 - val_loss: 3407113216.0000\n",
      "Epoch 61/200\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 4111420160.0000 - val_loss: 3395167744.0000\n",
      "Epoch 62/200\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 4099586816.0000 - val_loss: 3397926400.0000\n",
      "Epoch 63/200\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 4083287296.0000 - val_loss: 3388687360.0000\n",
      "Epoch 64/200\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 4074709504.0000 - val_loss: 3390421504.0000\n",
      "Epoch 65/200\n",
      "9/9 [==============================] - 0s 16ms/step - loss: 4061194496.0000 - val_loss: 3389521664.0000\n",
      "Epoch 66/200\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 4036062720.0000 - val_loss: 3373394688.0000\n",
      "Epoch 67/200\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 4023311872.0000 - val_loss: 3376641024.0000\n",
      "Epoch 68/200\n",
      "9/9 [==============================] - 0s 14ms/step - loss: 4004008192.0000 - val_loss: 3371810560.0000\n",
      "Epoch 69/200\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 3989005312.0000 - val_loss: 3368927744.0000\n",
      "Epoch 70/200\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 3977926400.0000 - val_loss: 3366570752.0000\n",
      "Epoch 71/200\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 3957561344.0000 - val_loss: 3359448576.0000\n",
      "Epoch 72/200\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 3942819840.0000 - val_loss: 3361263104.0000\n",
      "Epoch 73/200\n",
      "9/9 [==============================] - 0s 14ms/step - loss: 3929346048.0000 - val_loss: 3347633152.0000\n",
      "Epoch 74/200\n",
      "9/9 [==============================] - 0s 14ms/step - loss: 3916775424.0000 - val_loss: 3348662272.0000\n",
      "Epoch 75/200\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 3901092096.0000 - val_loss: 3369701888.0000\n",
      "Epoch 76/200\n",
      "9/9 [==============================] - 0s 14ms/step - loss: 3887293184.0000 - val_loss: 3349162240.0000\n",
      "Epoch 77/200\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 3882163968.0000 - val_loss: 3354616832.0000\n",
      "Epoch 78/200\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 3864184320.0000 - val_loss: 3341475840.0000\n",
      "Epoch 79/200\n",
      "9/9 [==============================] - 0s 14ms/step - loss: 3849975296.0000 - val_loss: 3322301952.0000\n",
      "Epoch 80/200\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 3832623104.0000 - val_loss: 3367239936.0000\n",
      "Epoch 81/200\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 3815745024.0000 - val_loss: 3311621888.0000\n",
      "Epoch 82/200\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 3801783040.0000 - val_loss: 3343245824.0000\n",
      "Epoch 83/200\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 3790605056.0000 - val_loss: 3325775616.0000\n",
      "Epoch 84/200\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 3780781056.0000 - val_loss: 3327819776.0000\n",
      "Epoch 85/200\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 3768550912.0000 - val_loss: 3326742272.0000\n",
      "Epoch 86/200\n",
      "9/9 [==============================] - 0s 16ms/step - loss: 3759450880.0000 - val_loss: 3315785728.0000\n",
      "Epoch 87/200\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 3746667776.0000 - val_loss: 3319777024.0000\n",
      "Epoch 88/200\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 3739352320.0000 - val_loss: 3338763264.0000\n",
      "Epoch 89/200\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 3735916032.0000 - val_loss: 3325111296.0000\n",
      "Epoch 90/200\n",
      "9/9 [==============================] - 0s 16ms/step - loss: 3721843200.0000 - val_loss: 3313166336.0000\n",
      "Epoch 91/200\n",
      "9/9 [==============================] - 0s 16ms/step - loss: 3714860800.0000 - val_loss: 3296306944.0000\n",
      "Epoch 92/200\n",
      "9/9 [==============================] - 0s 16ms/step - loss: 3706234624.0000 - val_loss: 3314977280.0000\n",
      "Epoch 93/200\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 3696153344.0000 - val_loss: 3297554176.0000\n",
      "Epoch 94/200\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 3690128896.0000 - val_loss: 3318448128.0000\n",
      "Epoch 95/200\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 3678711552.0000 - val_loss: 3291992576.0000\n",
      "Epoch 96/200\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 3675289856.0000 - val_loss: 3290944256.0000\n",
      "Epoch 97/200\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 3674904576.0000 - val_loss: 3287803392.0000\n",
      "Epoch 98/200\n",
      "9/9 [==============================] - 0s 14ms/step - loss: 3661895168.0000 - val_loss: 3296604416.0000\n",
      "Epoch 99/200\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 3647483136.0000 - val_loss: 3298512128.0000\n",
      "Epoch 100/200\n",
      "9/9 [==============================] - 0s 16ms/step - loss: 3641468160.0000 - val_loss: 3292220416.0000\n",
      "Epoch 101/200\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 3636067328.0000 - val_loss: 3283961600.0000\n",
      "Epoch 102/200\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 3630746368.0000 - val_loss: 3298958336.0000\n",
      "Epoch 103/200\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 3627939584.0000 - val_loss: 3292738560.0000\n",
      "Epoch 104/200\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 3627954176.0000 - val_loss: 3285875456.0000\n",
      "Epoch 105/200\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 3613598720.0000 - val_loss: 3312868864.0000\n",
      "Epoch 106/200\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 3611499264.0000 - val_loss: 3292808448.0000\n",
      "Epoch 107/200\n",
      "9/9 [==============================] - 0s 14ms/step - loss: 3600233216.0000 - val_loss: 3323171328.0000\n",
      "Epoch 108/200\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 3595736576.0000 - val_loss: 3289797120.0000\n",
      "Epoch 109/200\n",
      "9/9 [==============================] - 0s 14ms/step - loss: 3585682432.0000 - val_loss: 3271745024.0000\n",
      "Epoch 110/200\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 3583908864.0000 - val_loss: 3340394752.0000\n",
      "Epoch 111/200\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 3593010432.0000 - val_loss: 3330772224.0000\n",
      "Epoch 112/200\n",
      "9/9 [==============================] - 0s 14ms/step - loss: 3589508608.0000 - val_loss: 3349354496.0000\n",
      "Epoch 113/200\n",
      "9/9 [==============================] - 0s 14ms/step - loss: 3579253760.0000 - val_loss: 3295425792.0000\n",
      "Epoch 114/200\n",
      "9/9 [==============================] - 0s 16ms/step - loss: 3560986368.0000 - val_loss: 3296111872.0000\n",
      "Epoch 115/200\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 3554872576.0000 - val_loss: 3304895488.0000\n",
      "Epoch 116/200\n",
      "9/9 [==============================] - 0s 14ms/step - loss: 3550434816.0000 - val_loss: 3293224960.0000\n",
      "Epoch 117/200\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 3546602752.0000 - val_loss: 3317244928.0000\n",
      "Epoch 118/200\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 3542242304.0000 - val_loss: 3312974848.0000\n",
      "Epoch 119/200\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 3536689920.0000 - val_loss: 3341989632.0000\n",
      "Epoch 120/200\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 3536995072.0000 - val_loss: 3301036544.0000\n",
      "Epoch 121/200\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 3529801472.0000 - val_loss: 3299658752.0000\n",
      "Epoch 122/200\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 3519633152.0000 - val_loss: 3283289856.0000\n",
      "Epoch 123/200\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 3524211456.0000 - val_loss: 3308599040.0000\n",
      "Epoch 124/200\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 3517201408.0000 - val_loss: 3311023616.0000\n",
      "Epoch 125/200\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 3515928320.0000 - val_loss: 3338452224.0000\n",
      "Epoch 126/200\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 3505515008.0000 - val_loss: 3332552448.0000\n",
      "Epoch 127/200\n",
      "9/9 [==============================] - 0s 16ms/step - loss: 3502667520.0000 - val_loss: 3302723840.0000\n",
      "Epoch 128/200\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 3497194496.0000 - val_loss: 3351024640.0000\n",
      "Epoch 129/200\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 3490237440.0000 - val_loss: 3324805120.0000\n",
      "Epoch 130/200\n",
      "9/9 [==============================] - 0s 14ms/step - loss: 3480281088.0000 - val_loss: 3319500032.0000\n",
      "Epoch 131/200\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 3476845568.0000 - val_loss: 3301210624.0000\n",
      "Epoch 132/200\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 3471664896.0000 - val_loss: 3334080256.0000\n",
      "Epoch 133/200\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 3466571520.0000 - val_loss: 3328773632.0000\n",
      "Epoch 134/200\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 3467639040.0000 - val_loss: 3330258432.0000\n",
      "Epoch 135/200\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 3461843200.0000 - val_loss: 3337850368.0000\n",
      "Epoch 136/200\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 3460438272.0000 - val_loss: 3355520512.0000\n",
      "Epoch 137/200\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 3468745984.0000 - val_loss: 3327184384.0000\n",
      "Epoch 138/200\n",
      "9/9 [==============================] - 0s 14ms/step - loss: 3455148800.0000 - val_loss: 3340958976.0000\n",
      "Epoch 139/200\n",
      "9/9 [==============================] - 0s 14ms/step - loss: 3445469952.0000 - val_loss: 3376516608.0000\n",
      "Epoch 140/200\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 3456436480.0000 - val_loss: 3329106176.0000\n",
      "Epoch 141/200\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 3443313664.0000 - val_loss: 3343895552.0000\n",
      "Epoch 142/200\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 3441138176.0000 - val_loss: 3367314432.0000\n",
      "Epoch 143/200\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 3436820736.0000 - val_loss: 3321544704.0000\n",
      "Epoch 144/200\n",
      "9/9 [==============================] - 0s 14ms/step - loss: 3429272832.0000 - val_loss: 3371019776.0000\n",
      "Epoch 145/200\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 3417637632.0000 - val_loss: 3380882432.0000\n",
      "Epoch 146/200\n",
      "9/9 [==============================] - 0s 14ms/step - loss: 3420195072.0000 - val_loss: 3359779840.0000\n",
      "Epoch 147/200\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 3414352128.0000 - val_loss: 3343047168.0000\n",
      "Epoch 148/200\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 3400646656.0000 - val_loss: 3383019520.0000\n",
      "Epoch 149/200\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 3402704640.0000 - val_loss: 3372039680.0000\n",
      "Epoch 150/200\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 3412884224.0000 - val_loss: 3358598144.0000\n",
      "Epoch 151/200\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 3396143360.0000 - val_loss: 3396105216.0000\n",
      "Epoch 152/200\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 3385939456.0000 - val_loss: 3361349632.0000\n",
      "Epoch 153/200\n",
      "9/9 [==============================] - 0s 14ms/step - loss: 3391122176.0000 - val_loss: 3384214784.0000\n",
      "Epoch 154/200\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 3378207232.0000 - val_loss: 3401878528.0000\n",
      "Epoch 155/200\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 3380570112.0000 - val_loss: 3364100608.0000\n",
      "Epoch 156/200\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 3370920960.0000 - val_loss: 3412930560.0000\n",
      "Epoch 157/200\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 3375137792.0000 - val_loss: 3383096320.0000\n",
      "Epoch 158/200\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 3369891072.0000 - val_loss: 3380072960.0000\n",
      "Epoch 159/200\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 3368735488.0000 - val_loss: 3382692608.0000\n",
      "Epoch 160/200\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 3364193536.0000 - val_loss: 3430769664.0000\n",
      "Epoch 161/200\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 3359002368.0000 - val_loss: 3348528896.0000\n",
      "Epoch 162/200\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 3359033088.0000 - val_loss: 3410521856.0000\n",
      "Epoch 163/200\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 3341952512.0000 - val_loss: 3373984000.0000\n",
      "Epoch 164/200\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 3347235072.0000 - val_loss: 3394496512.0000\n",
      "Epoch 165/200\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 3342522112.0000 - val_loss: 3421534976.0000\n",
      "Epoch 166/200\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 3333240320.0000 - val_loss: 3421371392.0000\n",
      "Epoch 167/200\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 3342573312.0000 - val_loss: 3358804992.0000\n",
      "Epoch 168/200\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 3326925056.0000 - val_loss: 3392425728.0000\n",
      "Epoch 169/200\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 3330149120.0000 - val_loss: 3442925568.0000\n",
      "Epoch 170/200\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 3319106304.0000 - val_loss: 3413603840.0000\n",
      "Epoch 171/200\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 3314915840.0000 - val_loss: 3410920448.0000\n",
      "Epoch 172/200\n",
      "9/9 [==============================] - 0s 14ms/step - loss: 3315492352.0000 - val_loss: 3394910464.0000\n",
      "Epoch 173/200\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 3308312064.0000 - val_loss: 3452054016.0000\n",
      "Epoch 174/200\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 3301344256.0000 - val_loss: 3443870976.0000\n",
      "Epoch 175/200\n",
      "9/9 [==============================] - 0s 16ms/step - loss: 3304812288.0000 - val_loss: 3408746496.0000\n",
      "Epoch 176/200\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 3295458304.0000 - val_loss: 3434177536.0000\n",
      "Epoch 177/200\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 3292241664.0000 - val_loss: 3438640640.0000\n",
      "Epoch 178/200\n",
      "9/9 [==============================] - 0s 14ms/step - loss: 3287207936.0000 - val_loss: 3408438016.0000\n",
      "Epoch 179/200\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 3295789312.0000 - val_loss: 3462261760.0000\n",
      "Epoch 180/200\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 3296043008.0000 - val_loss: 3455631360.0000\n",
      "Epoch 181/200\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 3284164096.0000 - val_loss: 3400958720.0000\n",
      "Epoch 182/200\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 3273966592.0000 - val_loss: 3428782592.0000\n",
      "Epoch 183/200\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 3267400704.0000 - val_loss: 3498426112.0000\n",
      "Epoch 184/200\n",
      "9/9 [==============================] - 0s 16ms/step - loss: 3276016640.0000 - val_loss: 3454496256.0000\n",
      "Epoch 185/200\n",
      "9/9 [==============================] - 0s 16ms/step - loss: 3277082624.0000 - val_loss: 3471476480.0000\n",
      "Epoch 186/200\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 3290489856.0000 - val_loss: 3465380096.0000\n",
      "Epoch 187/200\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 3264668160.0000 - val_loss: 3465695488.0000\n",
      "Epoch 188/200\n",
      "9/9 [==============================] - 0s 16ms/step - loss: 3253686528.0000 - val_loss: 3454991104.0000\n",
      "Epoch 189/200\n",
      "9/9 [==============================] - 0s 16ms/step - loss: 3249541120.0000 - val_loss: 3450659072.0000\n",
      "Epoch 190/200\n",
      "9/9 [==============================] - 0s 16ms/step - loss: 3241283840.0000 - val_loss: 3456683520.0000\n",
      "Epoch 191/200\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 3250339840.0000 - val_loss: 3441502976.0000\n",
      "Epoch 192/200\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 3246672896.0000 - val_loss: 3462981888.0000\n",
      "Epoch 193/200\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 3237135104.0000 - val_loss: 3478100480.0000\n",
      "Epoch 194/200\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 3227665664.0000 - val_loss: 3466956800.0000\n",
      "Epoch 195/200\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 3234725376.0000 - val_loss: 3457797888.0000\n",
      "Epoch 196/200\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 3229896448.0000 - val_loss: 3488639488.0000\n",
      "Epoch 197/200\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 3223118080.0000 - val_loss: 3462117632.0000\n",
      "Epoch 198/200\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 3216945664.0000 - val_loss: 3455740928.0000\n",
      "Epoch 199/200\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 3210342400.0000 - val_loss: 3491709952.0000\n",
      "Epoch 200/200\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 3205415424.0000 - val_loss: 3538180864.0000\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAEYCAYAAABY7FHWAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAA2aklEQVR4nO3dd5xU9bn48c8zszPb2KWDKAqigqEuCIgVlARrlMQCxsQWr4kaSxKNmNyrRpOoN+ZqqiUx0RgVFSMxyTWxovJDr4AioSiKbpS+IGUXtszOPr8/vmeWmdkpu7BTlnner9d5zWlzznPKnOd8v6eMqCrGGGNMhC/XARhjjMkvlhiMMcbEsMRgjDEmhiUGY4wxMSwxGGOMiWGJwRhjTAxLDGmIiIrIobmOI5+JyH0i8l+5jiNbRGSeiFzqtZ8vIs+3Z9y9mF/CeYjIYSLyrogM2otpF4vIChHZb29i7EwicpyIbBaR80TkfhEZuofT2et13875XCQi8zM9n70lImeIyOz2jJuRxCAi1SJSLyJ1Uc2vMjGvfVFX2dEiVPWbqnrb3k5HRKaIyJrOiClbVPVRVZ2W7XmISHfgt8DZqvrvvZj8ZcBrqrpBRJ6L+r2GRKQpqvu+vVmGDjoOOAP4AtAX+CCL8wZARB4SkR9lYLrFIvKgiPxbRGpF5B0ROSVunKki8p6I7BKRVxIlfhEJeuOsietfJSKvi8h2EVkjIjdFhqnqs8BIERmdLs6ivVjGdL6oqi+mG0lEilS1Oa6fX1XD7Z1RR8ffFxTiMpvdVHU7MKUTJvUNr0FVWw9QIvIQsEZV/7MT5tEhqvoTr3VBtuedBUXAp8Bk4BPgVOBJERmlqtUi0gf4M3Ap8FfgNuAJYFLcdK4HNgHd4vo/BjyD2zcGA/NFZImXFAAex50MfCtllKra6Q1QDXw+ybCLgP8H3A18BvwIeAi4F/hfYCfweeBzwDxgG7AcOCNqGonG3x94GqgBPgaujhp/IrAI2AFsBP4nRezXA+uBdcAlgAKHesOKgbtwG3QjcB9QmmJalwArga3AP4FBUcMU+CbubGgr8GtAvOVuAMJAHbBtD5f5FuBJ4I9ArbcOx0cNnwWs9oatAL6UZBttAz4Cjvb6f4rbIS+M2x4/iuo+HVjifXcBMDpu37gOWApsx+30JUA5UA+0eMtd5y1fMXCPtz3Wee3FCdZ1MW5/GhXVr583zb4Jxt0GjIzq19cbtx/QE/ibt163eu0Do8adB1wata7mRw37AvCet2y/Al6NGvcQ4GVgC7AZeBToEfXdA3EHhRpvnF8lmcfRwEJvHguBo+Niu83bfrXA80CfJPvnQd4yFyUY1rpN27k+fuRt6zrcAa23t3w7vBgHR43/c9x+tANYDBzXgf026XEhwTLMA24H3vLW1V+AXlHDnwI2eMNeA0Z4/S8DQkBTZHnas31wx4atuN/iKR04Xi4Fzoqa94KoYZHfxeFR/Q7GHVdOwSXv6GntAobHLeONUd3HAB+njam9wXekIX1iaAauwmXPUm8n3O4F7QMqgA+B7wNB4ERvJxkWtdNGj1/m7WA3eeMPwR3MTvLGfwP4mtfeDZiUJLaTcQf8kd4GeYzYxHAP8CzQy4vxr8DtSaY13VuGz3nL+Z9xG1xxP7AeuB9oDXByogPBHi7zLbgEcyrgx/1A3oya3jm4A68PmIFLNgPittHF3nd/hEuGv8YdVKd526NbgoPIOFziONL77oXe/lActW+85c27F24H/6Y3bAptd/RbgTdxB+y+uIPPbUnW+W+AO6O6r8H7UScY9/fAj6O6rwT+4bX3Bs7y1nEF7sc1N+6A0yYxAH1wB7uzgQDwbW89RsY9FJc4ir1leQ24xxvmB97FJeNyXLI8NsE8euEOPl/D7Vfned29o2JbDQzF/bbmAXckWQenAcuTDIvepu1ZHx/iEl933InGKtzJSxHuIP+HqPG/6k2zCPgu7uBckm6/9dZp0uNCgmWYB6xl9+/5aeBPUcMv8ZYncvKxJNHyt3P7hID/8Ma7HHcSI+04Vvb3lvdwr/vnwL1x4yzDSxxe99+AL5H49/IT4A5vXQ0D1gAToob3wh17KlPG1dGDfnsa3I+/DpfVI81/RK3ETxLshH+M+sFu9Va0L2qcx4FboscHjgfexv34auKm+RTuQPoB8D7wQ5KcOcUdLO6I6h7qrcRDcWfzO4FDooYfRZLsCzwHfD2q24fL5oO8bo3sWF73k8Cs+ANBonXkdR+ZYD3eiPcDxP3AXowaNhyoT7HsS4Azo+b/QdSwUV68/aP6bQGqEhxE7iXuwO2t/8lR+8ZXo4b9N3Cf1z6Ftjv6auDUqO6TgOoky3Ak7kzU53UvAs5NMu7ngY+iuv8fcEGScauArVHd80icGC4gNvkK7od5aZLpTgfeidqXakh89h49j68Bb8UNfwO4KCq2/4wadgVewksw3fOj402wv/0oybBE6+MHUd0/A56L6v4iUQfdBNPbCoxJt9/irj1sIMlxIcF05xH7ex6OKwX4E4zbA7ePd0+0/O3YPh9GdZd509ov2TJ74wWAF4H7o/o9SFwi9/bNyPb9ErtPYKbQ9vdyNC55Nnsx/DDBPBU4KFVsmbwrabqq9ohqfhs17NME40f6PQTcCYRUtSVq+L+BA+LG/4Td1R69RGSb12zHneG8hatGKgVGAO+JyEIROT1JzPvHxRZ9Ua8v3ll6ZD7AP7z+iQwCfh417me4A0X0MmyIat9F2/rCeNGxDQL2j1rmbbgzqf4ppl8iIkUAInKBiCyJ+u5I3BlvxMao9noAVY3vlyjeQcB34+I6ELduk8WVarn3J3Y7/DtuWq1U9f9wyXuyiByOS+jPJhoXV6VTKiJHehf3qnB1s4hImXc3zL9FZAfuzL6HiPhTxBmJtXUbqfsltnaLSD8RmS0ia73p/ond6/xA4N8ad70tyTziLzbH/zbau3634s6YU2rn+ojfN5LuKyLyXRFZ6V0g3YYrZUTve8n22/2BT9McF+LF/54DQB8R8YvIHSKy2lumam+cPvET8KTbPq0xq+ourzXpfi0iPuARXKKKru+vAyrjRq8EakWkHHcidVWSafbCHZNuxZVoDgROEpErokaLbO9tyWKDzF58TkWT9VPV10SkPxAQEZ+qtojIIbizMUTkdVz97BpVrfb61QGbVHWA130eMEVVv+F1P4c7ezgX+DIwR0R6q+rOuBjW41ZmxEFR7ZtxO/gIVV3bjmX8FFdV8Wg7xo2XaP3E9/8UV1o5rKMT9w6EvwWmAm+oalhEluAS196KLPeP9+C7iZZ7HS7ZLPe6D/L6JfMwrqpiAzBHVRsSzsjtV0/iqmI2An9T1Vpv8HdxxfAj1d2tUwW8Q/r1E7P/iIgQuz/djlvG0aq6RUSm465DgFtvByW6GSNOZH1EOwh3QOiopcCQdsxzT9dHGyJyHHADbt9b7m2Hre2c1jrgwMhxwet3EK7aKpn433MI91v+CnAmruRYjUtO0XHE74vt3T5pefvFg7iTuFNVNRQ1eDmu+jUybjmuim45cBjugvLrbhIEge4isgF3cboPEFbVP3pfXyPu9tRTcdWs4Kq2q1V1R6oY8/U5hiW4i5DfE5EArpqlEleneB1tr9BvBnaJyA0iUgoMBBpFZII3vDcw1NuZtnn9Et3R8yRwkYgMF5Ey4ObIAO+7vwXuFpF+ACJygIiclGQZ7gNuFJER3rjdReScdi7/RmCgiARTjPMWsCOyzN4Z0MioZU6lHLfj13ixXYwrMXSG3wLf9M7ERUTKReQ0EUl7Zopb7t7ibsWMeBz4TxHp692xcRPuTDuZR3DF7a/iqhtTeQx3feV8rz2iAncSsM07C7s5wXcT+TswQkS+7J3hXg1EPx9QgVfFKiIH4G50iHgLl1ju8NZZiYgck2Ae/wsMFZGviEiRiMzAVZH8rZ0xtlLVNbiq1olpRt3T9ZFsWs141TLibqeMP0NOJlIi/J6IBERkCq6aKtW9+V+N+j3fijtZCHtxNOKqRMtwdfPRNuKu20W0d/u0x724A/QXVbU+btgzuFtKzxKREtz+vlRV38NdazgQV7qtwt25tNFr/xSXIMXbN3zink2Zgbs2EjEZV82dUiYTw18l9jmGZzrw3RCumugU3EF/LG5Hmg3cj6saiqa4Cy5VuDsCfog7OEQOMIcC13kli58DMxOdSarqc7iLUC/j6ulejhvlBq//m17x80XcmVQbqvoMrkpstjfuMm952uNl3BnCBhHZnGT6YdyPogq3zJuB37F7mZNS1RW4euA3cDvWKFx13F5T1UW4i3C/wp2BfYir7mvPd9/DJYKPvGqo/XEXvhfhzm7/hbumlPT+cu9g9zZun3g9zfwiB5r9if2x3IPbxzbjLny362xcVTfjLurfgTvgHEbsev0h7uL8dlwS+XPUdyPb81Dcvr8G96OOn8cW3F1f3/Xm8T3gdG/ee+J+3HWLVO5hD9ZHEv/EretVuKqdBhJXLbehqk245xsix4Xf4K4LvZfia4/gqqc34KpXrvb6/9Gb/1rcxfI34773IDDc2w/ntnf7pOOV1r+B+91uiDo+nu8tYw2uGvzHuN/PkcBMb1izqm6INLjq6RavO+yVAr6Mu+lhK+4Ee5k3rYjzcNs8dZzeBYm8IiKDcUX7kSJSCbwfqSZKMv5D3vhzvO74qqT7gXmq+njGgzc5JyK/B9ZpDu7B72pEpBhXLTRVVdfnOh6TOSLyRdzdmeemGzdfq5JaeVnw40g1jFc9MSbN1/4JTBORniLSE3d75T8zHKrJA95JxZdxZ3wmDVVtVNXhlhT2far61/YkBcjDxCAij+OqOIaJe6T767g64K+LyLu4KpYzvXEniHsk/BzgfhFZDqCqn+Ee8lnoNbd6/cw+TERuwxWdf6qqH+c6HmO6qrysSjLGGJM7eVdiMMYYk1u5eo4hoT59+ujgwYNzHYYxxnQZixcv3qyqyR603SN5lRgGDx7MokWLch2GMcZ0GSKyN69dT8iqkowxxsSwxGCMMSaGJQZjjDEx8uoagzFdXSgUYs2aNTQ0JHx3nzF7rKSkhIEDBxIIBDI+L0sMxnSiNWvWUFFRweDBg/HegGnMXlNVtmzZwpo1azj44IMzPj+rSjKmEzU0NNC7d29LCqZTiQi9e/fOWknUEoMxncySgsmEbO5X+0ZV0m23gSoEg67p2xeGDYMJE8B+pMYY0yH7RmK4807YGf9nbMDYsfCLX8Cxx2Y/JmNyxO/3M2rUqNbumTNnMmvWrIzPt7q6mtNPP51ly5ZlbB5z585l6NChDB8+PGPz6Ij77ruPsrIyLrjggg5/t7q6mgULFvCVr3wlA5HtnX0iMfQoqqMpGKbU30R5USMHlWxiWslrfOvD26g4awaB1e9Dt3R/p2zMvqG0tJQlS5akHCccDuP3+5N2t/d72TZ37lxOP/30hImhubmZoqLsHtK++c1v7vF3q6ureeyxx/IyMewT1xiuvRau/rafS64s5cuX9ODwM4by+rBLOatpNoFN61h37X/nOkRjcm7w4MHceuutHHvssTz11FNtuh9//HFGjRrFyJEjueGGG1q/161bN2666SaOPPJI3njjjZhpLl68mDFjxnDUUUfx61//urV/OBzm+uuvZ8KECYwePZr770/8p2F/+tOfmDhxIlVVVXzjG98gHA63zvMHP/gBY8aMYdKkSWzcuJEFCxbw7LPPcv3111NVVcXq1auZMmUK3//+95k8eTI///nPWbx4MZMnT+aII47gpJNOYv169zcTU6ZM4YYbbmDixIkMHTqU1193f+5XXV3Ncccdx7hx4xg3bhwLFiwAYN68eUyePJlzzz2XoUOHMmvWLB599FEmTpzIqFGjWL16NQC33HILd911FwCrV6/m5JNP5ogjjuC4447jvffcH8tddNFFXH311Rx99NEMGTKEOXPmADBr1ixef/11qqqquPvuu2loaODiiy9m1KhRjB07lldeeWXvNvjeUNW8aY444gjtTJ98ojq3/DzdRYlu+teGTp22MYmsWLGitf2aa1QnT+7c5ppr0sfg8/l0zJgxrc3s2bNVVXXQoEF65513to4X3b127Vo98MADddOmTRoKhfSEE07QZ555RlVVAX3iiScSzmvUqFE6b948VVW97rrrdMSIEaqqev/99+ttt92mqqoNDQ16xBFH6EcffdRmXZ1++una1NSkqqqXX365Pvzww63zfPbZZ1VV9frrr2+d1oUXXqhPPfVU6zQmT56sl19+uaqqNjU16VFHHaWbNm1SVdXZs2frxRdf3Dred77zHVVV/fvf/65Tp05VVdWdO3dqfX29qqquWrVKI8egV155Rbt3767r1q3ThoYG3X///fWmm25SVdV77rlHr/E2xM0336w//elPVVX1xBNP1FWrVqmq6ptvvqknnHBCa8xnn322hsNhXb58uR5yyCGt8zjttNNal+Wuu+7Siy66SFVVV65cqQceeGBrbNHrLB6wSDv5WLxPVCUlc+CBUPfzb1F66eMseHAhU+8+PdchGZNxqaqSZsyYkbB74cKFTJkyhb593Us6zz//fF577TWmT5+O3+/nrLPOajOt7du3s23bNiZPngzA1772NZ57zv119vPPP8/SpUtbz463b9/OBx98EHMP/ksvvcTixYuZMGECAPX19fTr1w+AYDDI6ae73+sRRxzBCy+8kHR5I8vw/vvvs2zZMr7whS8ArtQyYMDufwT+8pe/3Dq96upqwD2Q+K1vfYslS5bg9/tZtWpV6/gTJkxo/f4hhxzCtGnTABg1alSbs/m6ujoWLFjAOeec09qvsbGxtX369On4fD6GDx/Oxo0bEy7H/PnzueqqqwA4/PDDGTRoEKtWrWL06NFJlz1T9unEADDsS8PhUtj8+grcf6gbkx333JPrCNoqLy9P2K0p/rCrpKQk4XUFVU16C6Wq8stf/pKTTjop6XRVlQsvvJDbb7+9zbBAINA6bb/fT3Nzc9LpRC/DiBEj2lR3RRQXF7eZ3t13303//v159913aWlpoaSkpM34AD6fr7Xb5/O1iaelpYUePXokTcjR00q2rlNtg2zbJ64xpOLr1YMtJfvjW7mCPFrvxuSVI488kldffZXNmzcTDod5/PHHW0sCyfTo0YPu3bszf/58AB599NHWYSeddBL33nsvoVAIgFWrVrEz7s7BqVOnMmfOHDZt2gTAZ599xr//nfoN0hUVFdTW1iYcNmzYMGpqaloTQygUYvny5Smnt337dgYMGIDP5+ORRx5pvcbRUZWVlRx88ME89dRTgDvIv/vuuym/E78sxx9/fOs6XLVqFZ988gnDhg3bo3j21j6fGAB2DR7OoF0r+OijXEdiTObV19dTVVXV2rTnVtUBAwZw++23c8IJJzBmzBjGjRvHmWeemfZ7f/jDH7jyyis56qijKC0tbe1/6aWXMnz4cMaNG8fIkSP5xje+0eYse/jw4fzoRz9i2rRpjB49mi984QutF4uTmTlzJj/96U8ZO3Zs6wXgiGAwyJw5c7jhhhsYM2YMVVVVrReTk7niiit4+OGHmTRpEqtWrWpTouqIRx99lAcffJAxY8YwYsQI/vKXv6Qcf/To0RQVFTFmzBjuvvturrjiCsLhMKNGjWLGjBk89NBDMSWNbMqr/3weP368ZuKPej776tUEHv0DTzywg0v/wx54M5mzcuVKPve5z+U6DLOPSrR/ichiVR3fmfMpiBJDz2OGU0Ed7/59Ta5DMcaYvFcQiUFGuIdhilatyHEkxhiT/woiMeA9JdlvsyUGY4xJpzASQ58+1Jb0YeAOSwzGGJNOYSQGYGvvQxnQ+DEtLbmOxBhj8lvBJIaWiu5UUMu2bbmOxBjTVYTDYX79618X3F+1FkxikIoKKqhl8+ZcR2JMZvn9/pjnGO64446szLe6upqRI0dmdB7dvLckr1u3jrPPPjvhOFOmTGFPbntftGgRV199dUy/6667js997nMxT0QXgn3+lRgR/h4VVLKDT7fkOhJjMmtffu12xP7779/6HqbOMn78eMaPj30c4O677+7UeXQVBVNiKOpVSQW1bLHEYApUvr12+4YbbuA3v/lNa/ctt9zCz372M+rq6pg6dSrjxo1j1KhRCZ8gji6d1NfXM3PmTEaPHs2MGTOor69vHe/yyy9n/PjxjBgxgptvvrm1/8KFCzn66KMZM2YMEydOpLa2lnnz5rW+uO+zzz5j+vTpjB49mkmTJrF06dLWGC+55BKmTJnCkCFD+MUvftGhbdBVFEyJIdjHq0qqUcCefjZZcO21kObMvcOqqtK+nS/ySoyIG2+8sfUNpCUlJa3vNpo1a1Zr97p165g0aRKLFy+mZ8+eTJs2jblz5zJ9+nR27tzJyJEjufXWW9vM6+KLL+aXv/wlkydP5vrrr2/t/+CDD9K9e3cWLlxIY2MjxxxzDNOmTYt5u+rMmTO59tprueKKKwB48skn+cc//kFJSQnPPPMMlZWVbN68mUmTJnHGGWckfWHfvffeS1lZGUuXLmXp0qWMGzeuddiPf/xjevXqRTgcZurUqSxdupTDDz+cGTNm8MQTTzBhwgR27NgR8zoPgJtvvpmxY8cyd+5cXn75ZS644ILWUth7773HK6+8Qm1tLcOGDePyyy8nEAik3CZdTcEkhtL+lfhQdqzfCdi/uZl9V1d57fbYsWPZtGkT69ato6amhp49e3LQQQcRCoX4/ve/z2uvvYbP52Pt2rVs3LiR/fbbL+Eyvfbaa63XBkaPHh3zmuonn3ySBx54gObmZtavX8+KFSsQEQYMGND6uu/Kyso205w/fz5PP/00ACeeeCJbtmxh+/btAJx22mkUFxdTXFxMv3792LhxIwMHDkwYW1dVMImhpE8FADs31GKJwWRFHr53O59euw1w9tlnM2fOHDZs2MDMmTMB9zK6mpoaFi9eTCAQYPDgwWnvCkoUx8cff8xdd93FwoUL6dmzJxdddBENDQ0p446OP9k8ol9sl+6V4F1VRq8xiEi1iPxLRJaISOe/Ha8jsVS6xFC/cUcuwzAmL+XitdvgqpNmz57NnDlzWu8y2r59O/369SMQCPDKK6+kfRV39Ouqly1b1no9YMeOHZSXl9O9e3c2btzYWpo5/PDDWbduHQsXLgSgtra2zcE9eprz5s2jT58+CUsW+6pslBhOUNXc3yTqbdSGmsTvcjdmXxF/jeHkk09Oe8tq9Gu3VZVTTz213a/dvuSSSygrK4spHVx66aVUV1czbtw4VJW+ffsyd+7cNt8fMWIEtbW1HHDAAa3/lnb++efzxS9+kfHjx1NVVcXhhx+eMobLL7+ciy++mNGjR1NVVcXEiRMBGDNmDGPHjmXEiBEMGTKEY445BnCv537iiSe46qqrqK+vp7S0lBdffDFmmrfcckvrNMvKynj44YfTrot9SUZfuy0i1cD49iaGTL12G4B58+CEE7h29Mvc8+4JmZmHKXj22m2TSfvKa7cVeF5EFovIZYlGEJHLRGSRiCyqqanJXCQVriqpZZtVJRljTCqZTgzHqOo44BTgShE5Pn4EVX1AVcer6vjIHREZ4VUltWy3qiRjjEklo4lBVdd5n5uAZ4CJmZxfSl6Jgdpa++9nk1H59K+IZt+Rzf0qY4lBRMpFpCLSDkwDlmVqfml5iaG8ZQd1dTmLwuzjSkpK2LJliyUH06lUlS1btmTtnU2ZvCupP/CMd+9vEfCYqv4jg/NLrayMFvFRoe5FepEChDGdaeDAgaxZs4aMXi8zBamkpCRrD9JlLDGo6kfAmExNv8NECJdVULlzB1u2QNQDmMZ0mkAgEPN0rzFdUcG8RA8gXO5epOc92W6MMSaBgkoMLeXuRXqNjbmOxBhj8ldBJQYtd//J0NSU60iMMSZ/FVZiqKy0EoMxxqRRUImBCqtKMsaYdAoqMUiFq0qyxGCMMckVVmLoYVVJxhiTTkElBn93ryqpwZ5KNcaYZAorMfSspIgw4br69CMbY0yBKqzE0GP3i/SMMcYkVlCJIfL3nlJr/8lgjDHJFFRiiPwng2+nlRiMMSaZwkoM3itVLTEYY0xyhZUYgkEAWhrsnRjGGJNMYSWGQACAlsZQjgMxxpj8VViJwSsxaKOVGIwxJpnCSgxeiUGbrMRgjDHJWGIwxhgTo7ASg1eVZH/IYIwxyRVWYvBKDISsxGCMMclYYjDGGBOjsBJDpCopZFVJxhiTTGElBq/EIFZiMMaYpAoyMdBsicEYY5IpyMTgs6okY4xJqrASg89HWPxI2EoMxhiTTGElBqDFH8BnVUnGGJNUwSWGsD+ILxxC7W+fjTEmoYwnBhHxi8g7IvK3TM+rPVr8AQI00dyc60iMMSY/ZaPEcA2wMgvzaReXGEI0NuY6EmOMyU8ZTQwiMhA4DfhdJufTEWqJwRhjUsp0ieEe4HtAS7IRROQyEVkkIotqamoyHA60BIIEabL36BljTBIZSwwicjqwSVUXpxpPVR9Q1fGqOr5v376ZCmf3/IqsxGCMMalkssRwDHCGiFQDs4ETReRPGZxfu1hiMMaY1DKWGFT1RlUdqKqDgZnAy6r61UzNr928qiRLDMYYk1jBPcdAwEoMxhiTSlE2ZqKq84B52ZhXWpYYjDEmpcIrMRRbVZIxxqRScIlBrMRgjDEpFV5iCFpiMMaYVAovMZRYVZIxxqRScInBZyUGY4xJqeASgxRbYjDGmFTadbuqiPTDPcm8P1APLAMWqWrSdyDlK59VJRljTEopE4OInADMAnoB7wCbgBJgOnCIiMwBfqaqOzIcZ6fxW4nBGGNSSldiOBX4D1X9JH6AiBQBpwNfAJ7OQGwZ4SuxxGCMMamkTAyqen2KYc3A3M4OKNP8JfbabWOMSSXlxWcRuSeq/Zq4YQ9lJqTMsucYjDEmtXR3JR0f1X5h3LDRnRxLdgQCBAnR2KC5jsQYY/JSusQgSdq7rmAQgFB9c44DMcaY/JTu4rNPRHriEkikPZIg/BmNLFMCAQDCDSEgkNtYjDEmD6VLDN2BxexOBm9HDeuadTFeYmiuD+U4EGOMyU/p7koanKU4sserSmppsNuSjDEmkXR3JQ0Ske5R3SeIyM9F5NsiEsx8eBkQU5VkjDEmXrqLz08C5QAiUgU8BXwCVAG/yWRgGWOJwRhjUkp3jaFUVdd57V8Ffq+qPxMRH7Ako5FlSqQqqdESgzHGJNKR21VPBF4C6Iovz2vllRjsGoMxxiSWrsTwsog8CawHegIvA4jIAKBrHlm9xEDISgzGGJNIusRwLTADGAAcq6qRo+l+wA8yGFfmeFVJlhiMMSaxdLerKjA7Qf93MhZRpkVKDPYWPWOMSSjd/zHUEvsgm3jdgssblRmMLTO8xCDNVmIwxphE0lUlvYSrNvozMDvR/zJ0OZGqJEsMxhiTUMq7klR1OnASUAP8VkReFZErRKRXNoLLiEiJIWRVScYYk0i621VR1e2q+gfgFOA+4FbgogzHlTleYvBZicEYYxJKV5WEiBwNnAccB8wHvqSqr2c6sIyxawzGGJNSuovP1cA23J1JlwHNXv9xAKr6dorvlgCvAcXefOao6s2dEfRe8a4x+JqtKskYYxJJV2Koxt2FdBIwjdgnoRX3NHQyjcCJqlonIgFgvog8p6pv7kW8e89KDMYYk1K65xim7OmEvWcg6rzOgNfk/j8cvMTg1xAtLeBLe5XFGGMKS7rXbh+bZniliIxMMdwvIkuATcALqvp/Cca5TEQWiciimpqadoa9F7yqpCBN9vCzMcYkkO58+SwRWSAiN4nIaSIyUUSOF5FLROQR4G9AabIvq2pYVauAgcDERElEVR9Q1fGqOr5v3757syzt45UYAoQsMRhjTALpqpK+7f3P89nAObh3JtUDK4H7VXV+e2aiqttEZB5wMrBsryLeW5YYjDEmpbS3q6rqVuC3XtNuItIXCHlJoRT4PHDnHkXZmaKqkux1ScYY01baxLAXBgAPi4gfV2X1pKr+LYPzax8rMRhjTEoZSwyquhQYm6np7zGfjxbxEVBLDMYYk0jamzVFxOc9/bzPaCkKWlWSMcYk0Z53JbUAP8tCLFmjRQGrSjLGmCTa+3jX8yJylohI+lHzn/otMRhjTDLtvcbwHaAcCItIPV35j3qAlkDQHnAzxpgk2pUYVLUi04FkU6Qqya4xGGNMW+2+K0lEzgCO9zrn5cWtp3sqYFVJxhiTTLuuMYjIHcA1wAqvucbr1yWpVSUZY0xS7S0xnApUeXcoISIPA+8AszIVWEYFrCrJGGOS6chLp3tEtXfv5Diyy6qSjDEmqfaWGH4CvCMir+DuSDoeuDFjUWVaMEiAELssMRhjTBvt+c9nH9ACTAIm4BLDDaq6IcOxZYwEAvbkszHGJNGet6u2iMi3VPVJ4NksxJR5wQABGq0qyRhjEmjvNYYXROQ6ETlQRHpFmoxGlkEStLuSjDEmmfZeY7jE+7wyqp8CQzo3nOyQkqDdlWSMMUm09xrDLFV9IgvxZIUUW4nBGGOSae/bVa9MN15X4rPEYIwxSRXkNQZfif0fgzHGJFOY1xisxGCMMUm19+2qB2c6kGyyxGCMMcmlrEoSke9FtZ8TN+wnmQoq4+wBN2OMSSrdNYaZUe3xr8A4uZNjyR57jsEYY5JKlxgkSXui7q4jGKSYJkJNmutIjDEm76RLDJqkPVF31xEMAtDcGM5xIMYYk3/SXXweIyI7cKWDUq8dr7sko5FlkpcYtLGJDvyJnTHGFISUR0VV9WcrkKyKSQxluY3FGGPyTEf+qGff4SWGlga7LckYY+IVdGKw+1WNMaYtSwzGGGNiZCwxeO9VekVEVorIchG5JlPz6rCYawzGGGOiZfKWnGbgu6r6tohUAItF5AVVXZHBebZPIOA+rcRgjDFtZKzEoKrrVfVtr70WWAkckKn5dYhXYpCQJQZjjImXlWsMIjIYGAv8Xzbml5ZdYzDGmKQynhhEpBvwNHCtqu5IMPwyEVkkIotqamoyHY4TSQz2siRjjGkjo4lBRAK4pPCoqv450Tiq+oCqjlfV8X379s1kOLtZVZIxxiSVybuSBHgQWKmq/5Op+ewRSwzGGJNUJksMxwBfA04UkSVec2oG59d+XmLwNVtiMMaYeBm7XVVV55Ovr+a2xGCMMUkV9JPPlhiMMaYtSwzGGGNiFHRi8IctMRhjTLzCTAzeKzEsMRhjTFuFmRgiJYaWJrTr/kGpMcZkREEnhiBN9vCzMcbEKczE4FUlBQhZYjDGmDiFmRhECPsDBGmy9+gZY0ycwkwMQIs/aFVJxhiTQMEmhnCRJQZjjEmkYBODeonBqpKMMSZWwSaGloCVGIwxJpHCTQxWlWSMMQkVbGKgyO5KMsaYRAo2MahVJRljTEKFmxiClhiMMSaRgk0MBIIECFlVkjHGxCncxGAlBmOMScgSgyUGY4yJUbCJwV/qEsPWrbmOxBhj8kvBJobS7i4xrF6d60iMMSa/FGxi8JcGKStq4oMPch2JMcbkl4JNDARdYvjww1wHYowx+aVwE0MgQKnPEoMxxsQr3MQQDBKUJrZswS5AG2NMlIJODAF1T7dZqcEYY3Yr6MTgb3EPMdgFaGOM2a2gE4OEmhCxEoMxxkQr7MQQDjNoYNgSgzHGRMlYYhCR34vIJhFZlql57JVgEIBhQ0JWlWSMMVEyWWJ4CDg5g9PfO15iGH5oEytXgmqO4zHGmDyRscSgqq8Bn2Vq+nvNSwwjDmti+3ZYvz7H8RhjTJ7I+TUGEblMRBaJyKKamprszdhLDIcPcbesLl+evVkbY0w+y3liUNUHVHW8qo7v27dv9mYcucZwsCUGY4yJlvPEkDOBAAB9Kpvo0wdWrMhxPMYYkycKNzF4JQaamhg+3EoMxhgTkcnbVR8H3gCGicgaEfl6pua1R6ISw4gRLjHYnUnGGANFmZqwqp6XqWl3ikhiCIUYMQK2b4d16+CAA3IbljHG5JpVJXlVSWDVScYYA5YYoKmJqiooKoIXX8xpRMYYkxcsMSxZQk//Dk4+GR57DFpachuWMcbkWuEmhsgzE9ddB5Mm8dXzwqxdC6++mtuwjDEm1wo3MQwZAqtXw113wcqVTPf/lW7d4NFHcx2YMcbkVuEmBnDJ4Zpr4KCDKL73Hs45Bx55BF56KdeBGWNM7hR2YgB31fmqq+DVV7n7wiUMHQpnnglvvZXrwIwxJjcsMQBcfDGI0P21v/L889C/P5xyit2+aowpTJYYAHr3hpEjYf58BgyAF16A4mKYMgXmzMl1cMYYk12WGCKOOw4WLIDmZoYMgVdegUGD4JxzYOpUd7eSvTLDGFMILDFEHHss1NXB0qUADBsGb74J99zjqpSmTIHhw+G//gtefx1CoZxGa4wxGWOJIeK449zn/PmtvYqK3E1LH30EDz7oHn34yU/g+OOhVy848UQ3/He/cyWK1auhsTFH8RtjTCcRzaP6kfHjx+uiRYtyF8DgwTBhAjz1VNJRtm1z1UwvvACLF8OyZbBrV+w4ffu6l/H16gXdu0OPHm0/KyuhW7fETWkpiGRuMY0x+w4RWayq4ztzmhl7u2qXNGUKPPGEKx5ccknCo3OPHvClL7kG3Cs0Pv7YNZ9+CmvWuGbtWpdEVq1yb27dvh1qa9sXhsjuJFFenjyBpGoSfa/ItrYxph2sxBBt40b4ylfg5ZfhkENgxgz4znfcXUudIByGHTt2J4qdO91ljY420d9ramr//IuLYxNHeTmUlbkm0p6oX7r20lLXFBdbSceYbMtEicESQ7xwGP74R3jySXj+eXcUPfdcd2vSyJFw2GHuCJgnmpr2PMHs2rW72bkz9jO+eqw9RHYnibKy9J+R9khTUrL7M1V7dHdRkSUjU9gsMWTb8uXw4x/D3//uTvUB/H53LWK//aBfP1eaKClxySIYdJ/R7UVF7jt+P/h8sZ9+vxseDLr/oI5u4vsl687QUVEV6uvbJo5E7ZHx6uuTt6fqtzdvtPX5dpdW4ldhUVHbfu0d3t7vFhXtbgKB5Js5vl9nD/PZbSRdV3MzfPaZO57sAbvGkG0jRrh3cYdC7irzypWwYgV88AHU1LgLCFu2uFuRGhvd6Xtzc3Zj9PsTJ45UySRyZIk/0kT1E7+fMq9pM06ibp8PinzQww+9fLv7JTqiRX1PfX6a1U+oxU9T2E9Ts899hv00NrvP8I6dyKaN1JX1o7aoB76tnxEKQUNzEQ3hAI0hH40hH81hoTkshMK+2M9mIdTko7leaGqO6hf2EWoWGsOuf6h592ez16+pWWjBhyIou9sT9Ytv789GDuVDAEIEYppmimK6i2mkjF2tjaBspD8NlCAoPlrw0dLaXs5OKqilnlJ2Uk4TQQ6Q9VT4dtLgK6PeV06Dr4wmXwk9fduplFp8om6TiOL3KaXUE/Q1U1fUg4A00123UaE7aCjqxtZAP0qlgXKtI0gTW4v3o0jC9AutpVgbwOejKVBOQ6AbAZrp0bQJ/D7C/mJaioJ0C22luKWenaV98BOmtLmWknAdoWA3QsFyeuxci58woWAZ4WAZ6vMTaGmke91amoNlfNb/c1TUriMQrqexvDd+bSYQricQ2kUwtIuWQJC6focQCO2ifPs6yravB5/QEiwlXFKGlpQhKOWbPsYfakCLAmgggAYivwn3KQLBTZ9StHMHGgzS3H8gWlFJ0bbNBKvfx79jG+H+A6AoAOFmtDmMT1vw+RSfKKItoIqourOpFtdNe7pbWlwNRU0NDBjgLk7mCUsM7REIwNixrkknHHaJJJIoQiHXL7ITxLc3N7txIuNGmnTde/qdxsbd846PpT3difpFdvI9IEDAa8r2aAqmlQJhr8kDzfgpigpmJ2WUUo8PZQcVNBH0EmE9AC0I6xlABbVUUkuIIhoooYI6AOop8cYupYxd9GIrAJvoy3oGoAhl7KKcnZRSj6BUM5g6uhGkjgAhgjTFfPpo4WMGspWelNDAgbxJN+rYSC8+4DC20JsBG9bjo4UwfsL4E54cgNAiPleC9xrFBz43DBE0frjPh4qPLT33o65sEDdkewOlYImhs0XOhktKch1J9rW0tG3am1xSJaWSEvcCq40bXZVer16uBBJJdpGzsPjPvem3t9NoaYE+fWDoULc/xCfoUGj3SUFzs6sLi1x4KStz09mwwSX3SMlLZPdn5LazhgZXn9fY6M46y8tj6wDr69290ZWVuw9KsPuCkN/vbp8rKtp9H3VtLWza5OLo1s2dGK1f7+Y9cKCbRzi8++KWz+e2D7h4mpqgspKiYNDdZVFUBOXllPt8blnr66msqIjdb1QRhP3w0dLcQsPaDYR79wNfEdt2NdLiDxBWHy0t4AvDzjDUbd1Gc7CMlqIgJd4u09QC9eHYXa8oDKEwNCbYHaPHqwvDprhhhGGt1w/crhh9Xa+5ue100n0m6ldZmcXfaTvYNQZjjOnCMnGNwS5ZGWOMiWGJwRhjTAxLDMYYY2JYYjDGGBPDEoMxxpgYlhiMMcbEsMRgjDEmhiUGY4wxMfLqATcRqQH+3cGv9QE2ZyCczpCvsVlcHWNxdVy+xrYvxjVIVft2ZjB5lRj2hIgs6uyn/jpLvsZmcXWMxdVx+RqbxdU+VpVkjDEmhiUGY4wxMfaFxPBArgNIIV9js7g6xuLquHyNzeJqhy5/jcEYY0zn2hdKDMYYYzqRJQZjjDExunRiEJGTReR9EflQRGblMI4DReQVEVkpIstF5Bqv/y0islZElnjNqTmIrVpE/uXNf5HXr5eIvCAiH3ifPbMc07CodbJERHaIyLW5Wl8i8nsR2SQiy6L6JV1HInKjt8+9LyInZTmun4rIeyKyVESeEZEeXv/BIlIfte7uy3JcSbddjtfXE1ExVYvIEq9/NtdXsuNDzvexpFS1SzaAH1gNDAGCwLvA8BzFMgAY57VXAKuA4cAtwHU5Xk/VQJ+4fv8NzPLaZwF35ng7bgAG5Wp9AccD44Bl6daRt13fBYqBg7190J/FuKYBRV77nVFxDY4eLwfrK+G2y/X6ihv+M+CmHKyvZMeHnO9jyZquXGKYCHyoqh+pahMwGzgzF4Go6npVfdtrrwVWAgfkIpZ2OhN42Gt/GJieu1CYCqxW1Y4+8d5pVPU14LO43snW0ZnAbFVtVNWPgQ9x+2JW4lLV51W12et8ExiYiXl3NK4Ucrq+IkREgHOBxzMx71RSHB9yvo8l05UTwwHAp1Hda8iDg7GIDAbGAv/n9fqWV+z/fbarbDwKPC8ii0XkMq9ff1VdD26nBfrlIK6ImcT+WHO9viKSraN82u8uAZ6L6j5YRN4RkVdF5LgcxJNo2+XL+joO2KiqH0T1y/r6ijs+5O0+1pUTgyTol9N7b0WkG/A0cK2q7gDuBQ4BqoD1uKJsth2jquOAU4ArReT4HMSQkIgEgTOAp7xe+bC+0smL/U5EfgA0A496vdYDB6nqWOA7wGMiUpnFkJJtu7xYX8B5xJ6AZH19JTg+JB01Qb+srrOunBjWAAdGdQ8E1uUoFkQkgNvoj6rqnwFUdaOqhlW1BfgtWS4OejGs8z43Ac94MWwUkQFe3AOATdmOy3MK8LaqbvRizPn6ipJsHeV8vxORC4HTgfPVq5T2qh22eO2LcfXSQ7MVU4ptlw/rqwj4MvBEpF+211ei4wN5vI915cSwEDhMRA72zjxnAs/mIhCv/vJBYKWq/k9U/wFRo30JWBb/3QzHVS4iFZF23IXLZbj1dKE32oXAX7IZV5SYs7hcr684ydbRs8BMESkWkYOBw4C3shWUiJwM3ACcoaq7ovr3FRG/1z7Ei+ujLMaVbNvldH15Pg+8p6prIj2yub6SHR/I030M6Lp3JXknSqfirvCvBn6QwziOxRX1lgJLvOZU4BHgX17/Z4EBWY5rCO7uhneB5ZF1BPQGXgI+8D575WCdlQFbgO5R/XKyvnDJaT0Qwp2tfT3VOgJ+4O1z7wOnZDmuD3H1z5H97D5v3LO8bfwu8DbwxSzHlXTb5XJ9ef0fAr4ZN24211ey40PO97Fkjb0SwxhjTIyuXJVkjDEmAywxGGOMiWGJwRhjTAxLDMYYY2JYYjDGGBPDEoMpCCLiE5F/ishBuY7FmHxnt6uagiAihwADVfXVXMdiTL6zxGD2eSISxj18FTFbVe/IVTzG5DtLDGafJyJ1qtot13EY01XYNQZTsLx/9LpTRN7ymkO9/oNE5CXvFdIvRa5LiEh/cf+a9q7XHO31n+u91nx55NXmIuIXkYdEZJm4f9D7du6W1JiOKcp1AMZkQWnkLx09t6tq5E2bO1R1oohcANyDe2vpr4A/qurDInIJ8Avcn6j8AnhVVb/kvYAtUgq5RFU/E5FSYKGIPI37h7ADVHUkgHh/wWlMV2BVSWafl6wqSUSqgRNV9SPvtcgbVLW3iGzGvQQu5PVfr6p9RKQGdwG7MW46t+DeKAouIZyEe/nZIuB/gb8Dz6t7JbUxec+qkkyh0yTtycaJISJTcK91PkpVxwDvACWquhUYA8wDrgR+1wmxGpMVlhhMoZsR9fmG174A9/8eAOcD8732l4DLofUaQiXQHdiqqrtE5HBgkje8D+BT1aeB/8L9Sb0xXYJVJZl9XoLbVf+hqrO8qqQ/4N6N7wPOU9UPvf/l/T3QB6gBLlbVT0SkP/AA7n8uwrgk8TYwF/efvO8DfYFbgK3etCMnXzeqavT/MxuTtywxmILlJYbxqro517EYk0+sKskYY0wMKzEYY4yJYSUGY4wxMSwxGGOMiWGJwRhjTAxLDMYYY2JYYjDGGBPj/wPJSVVC2Mh8pAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>configuracion</th>\n",
       "      <th>error_test</th>\n",
       "      <th>error_train</th>\n",
       "      <th>segundos_entrenamiento</th>\n",
       "      <th>epoca</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>ANN_capas=[64; 64; 64; 64; 64; 64; 64; 64; 1]_activaciones=['elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1</td>\n",
       "      <td>53376.532371</td>\n",
       "      <td>51400.826141</td>\n",
       "      <td>67.506442</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ANN_capas=[64; 64; 64; 64; 64; 64; 64; 64; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1</td>\n",
       "      <td>54231.385747</td>\n",
       "      <td>51958.811996</td>\n",
       "      <td>63.929625</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>ANN_capas=[32; 50; 64; 80; 80; 64; 50; 32; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1</td>\n",
       "      <td>54464.082256</td>\n",
       "      <td>52447.311771</td>\n",
       "      <td>65.858972</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>ANN_capas=[64; 64; 64; 64; 64; 64; 64; 64; 1]_activaciones=['selu'; 'selu'; 'selu'; 'selu'; 'selu'; 'selu'; 'selu'; 'selu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1</td>\n",
       "      <td>55036.204520</td>\n",
       "      <td>50085.477017</td>\n",
       "      <td>63.393345</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>ANN_capas=[50; 50; 50; 50; 50; 50; 50; 50; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1</td>\n",
       "      <td>55458.095748</td>\n",
       "      <td>52839.030423</td>\n",
       "      <td>58.103801</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>ANN_capas=[80; 80; 64; 64; 50; 50; 32; 32; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1</td>\n",
       "      <td>55472.535331</td>\n",
       "      <td>51841.128383</td>\n",
       "      <td>63.146074</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>ANN_capas=[64; 64; 64; 64; 64; 64; 64; 64; 1]_activaciones=['elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=nadam_paciencia=-1</td>\n",
       "      <td>55548.626626</td>\n",
       "      <td>51087.213273</td>\n",
       "      <td>73.071667</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>ANN_capas=[64; 64; 64; 64; 64; 64; 64; 64; 1]_activaciones=['elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=rmsprop_paciencia=-1</td>\n",
       "      <td>55743.102174</td>\n",
       "      <td>51816.125367</td>\n",
       "      <td>60.515572</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>ANN_capas=[64; 64; 64; 64; 64; 64; 64; 64; 1]_activaciones=['elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'linear']_inicializadorKernel=glorot_normal_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1</td>\n",
       "      <td>55989.743061</td>\n",
       "      <td>51369.158062</td>\n",
       "      <td>66.137771</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>ANN_capas=[64; 64; 64; 64; 64; 64; 64; 64; 1]_activaciones=['elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'linear']_inicializadorKernel=random_normal_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1</td>\n",
       "      <td>56144.034198</td>\n",
       "      <td>54215.765678</td>\n",
       "      <td>66.496849</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>ANN_capas=[64; 64; 64; 64; 64; 64; 64; 64; 1]_activaciones=['elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_porcentajesDropOut=[0.1; 0.1; 0.1; 0.1; 0.1; 0.1; 0.1; 0.1]_batchSize=32_optimizador=adam_paciencia=-1</td>\n",
       "      <td>56164.539987</td>\n",
       "      <td>60837.986028</td>\n",
       "      <td>83.210063</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ANN_capas=[8; 8; 8; 8; 8; 8; 8; 8; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=10</td>\n",
       "      <td>56168.076912</td>\n",
       "      <td>65505.463589</td>\n",
       "      <td>52.175863</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ANN_capas=[80; 80; 80; 80; 80; 80; 80; 80; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1</td>\n",
       "      <td>56252.450400</td>\n",
       "      <td>50596.551345</td>\n",
       "      <td>68.058731</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ANN_capas=[10; 10; 10; 10; 10; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=10</td>\n",
       "      <td>56579.769105</td>\n",
       "      <td>65503.054219</td>\n",
       "      <td>46.783819</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>ANN_capas=[50; 50; 50; 64; 64; 64; 80; 80; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1</td>\n",
       "      <td>56641.712968</td>\n",
       "      <td>50598.160283</td>\n",
       "      <td>71.024920</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                       configuracion  \\\n",
       "33                                                              ANN_capas=[64; 64; 64; 64; 64; 64; 64; 64; 1]_activaciones=['elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1   \n",
       "14                                                      ANN_capas=[64; 64; 64; 64; 64; 64; 64; 64; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1   \n",
       "24                                                      ANN_capas=[32; 50; 64; 80; 80; 64; 50; 32; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1   \n",
       "32                                                      ANN_capas=[64; 64; 64; 64; 64; 64; 64; 64; 1]_activaciones=['selu'; 'selu'; 'selu'; 'selu'; 'selu'; 'selu'; 'selu'; 'selu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1   \n",
       "18                                                      ANN_capas=[50; 50; 50; 50; 50; 50; 50; 50; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1   \n",
       "22                                                      ANN_capas=[80; 80; 64; 64; 50; 50; 32; 32; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1   \n",
       "38                                                             ANN_capas=[64; 64; 64; 64; 64; 64; 64; 64; 1]_activaciones=['elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=nadam_paciencia=-1   \n",
       "34                                                           ANN_capas=[64; 64; 64; 64; 64; 64; 64; 64; 1]_activaciones=['elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=rmsprop_paciencia=-1   \n",
       "46                                                               ANN_capas=[64; 64; 64; 64; 64; 64; 64; 64; 1]_activaciones=['elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'linear']_inicializadorKernel=glorot_normal_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1   \n",
       "47                                                               ANN_capas=[64; 64; 64; 64; 64; 64; 64; 64; 1]_activaciones=['elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'linear']_inicializadorKernel=random_normal_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1   \n",
       "40  ANN_capas=[64; 64; 64; 64; 64; 64; 64; 64; 1]_activaciones=['elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_porcentajesDropOut=[0.1; 0.1; 0.1; 0.1; 0.1; 0.1; 0.1; 0.1]_batchSize=32_optimizador=adam_paciencia=-1   \n",
       "7                                                               ANN_capas=[8; 8; 8; 8; 8; 8; 8; 8; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=10   \n",
       "15                                                      ANN_capas=[80; 80; 80; 80; 80; 80; 80; 80; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1   \n",
       "3                                                                                           ANN_capas=[10; 10; 10; 10; 10; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=10   \n",
       "20                                                      ANN_capas=[50; 50; 50; 64; 64; 64; 80; 80; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1   \n",
       "\n",
       "      error_test   error_train  segundos_entrenamiento  epoca  \n",
       "33  53376.532371  51400.826141               67.506442   50.0  \n",
       "14  54231.385747  51958.811996               63.929625   50.0  \n",
       "24  54464.082256  52447.311771               65.858972   50.0  \n",
       "32  55036.204520  50085.477017               63.393345   50.0  \n",
       "18  55458.095748  52839.030423               58.103801   50.0  \n",
       "22  55472.535331  51841.128383               63.146074   50.0  \n",
       "38  55548.626626  51087.213273               73.071667   50.0  \n",
       "34  55743.102174  51816.125367               60.515572   50.0  \n",
       "46  55989.743061  51369.158062               66.137771   50.0  \n",
       "47  56144.034198  54215.765678               66.496849   50.0  \n",
       "40  56164.539987  60837.986028               83.210063   50.0  \n",
       "7   56168.076912  65505.463589               52.175863   50.0  \n",
       "15  56252.450400  50596.551345               68.058731   50.0  \n",
       "3   56579.769105  65503.054219               46.783819   50.0  \n",
       "20  56641.712968  50598.160283               71.024920   50.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Experimentos para tamaños de batch\n",
    "experimentosBatchSizes = [8, 16, 32, 64, 128, 256, 512, 1024, 2048]\n",
    "listaCapas = [64, 64, 64, 64, 64, 64, 64, 64, 1]\n",
    "listaActivaciones = ['elu', 'elu', 'elu', 'elu', 'elu', 'elu', 'elu', 'elu', 'linear']\n",
    "for experimento in experimentosBatchSizes:\n",
    "    redAnn, configuracion = construirANN(caracteristicasEstandarizadas.shape[1], listaCapas, listaActivaciones)\n",
    "    redAnn.summary()\n",
    "    datosEntrenamiento = entrenarANN(redAnn, configuracion, caracteristicasEstandarizadas, etiquetas, 'mean_squared_error', \\\n",
    "                tamanioBatch=experimento, epocas=200, paciencia = -1, forzarEntrenamiento=1)\n",
    "    graficarResultados(datosEntrenamiento, 'Errores de entrenamiento y validación (Tamaño batch '+ str(experimento) +')')\n",
    "\n",
    "mostrarResultadosBitacora('ANN')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incoming-pocket",
   "metadata": {},
   "source": [
    "Después de analizar el historial y gráficas del entrenamiento con los diferentes tamaños de batches se puede observar que las mejores métricas se obtuvieron con un batch de tamaño __32__. Por el momento el mejor modelo se ha logrado con 50 épocas, se extenderán el número de épocas a 200 y se observará hasta que punto el modelo sigue mejorando."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "trying-bernard",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_95\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_906 (Dense)           (None, 64)                896       \n",
      "                                                                 \n",
      " dense_907 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_908 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_909 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_910 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_911 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_912 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_913 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_914 (Dense)           (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 30,081\n",
      "Trainable params: 30,081\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "549/549 [==============================] - 2s 2ms/step - loss: 12557889536.0000 - val_loss: 3697672448.0000\n",
      "Epoch 2/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4528478208.0000 - val_loss: 3389339136.0000\n",
      "Epoch 3/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 4202723584.0000 - val_loss: 3308553216.0000\n",
      "Epoch 4/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3957412608.0000 - val_loss: 3379048960.0000\n",
      "Epoch 5/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3825446144.0000 - val_loss: 3280989184.0000\n",
      "Epoch 6/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3713457664.0000 - val_loss: 3574827264.0000\n",
      "Epoch 7/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3695789824.0000 - val_loss: 3529154304.0000\n",
      "Epoch 8/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3623845888.0000 - val_loss: 3312336384.0000\n",
      "Epoch 9/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3575429632.0000 - val_loss: 3480711424.0000\n",
      "Epoch 10/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3550134272.0000 - val_loss: 3778212096.0000\n",
      "Epoch 11/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3520719872.0000 - val_loss: 3481009920.0000\n",
      "Epoch 12/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3482146304.0000 - val_loss: 3619852288.0000\n",
      "Epoch 13/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3413114624.0000 - val_loss: 3480320000.0000\n",
      "Epoch 14/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3393126656.0000 - val_loss: 3459404032.0000\n",
      "Epoch 15/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3349471744.0000 - val_loss: 3527977728.0000\n",
      "Epoch 16/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3308579072.0000 - val_loss: 3463248128.0000\n",
      "Epoch 17/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3265932288.0000 - val_loss: 3478746368.0000\n",
      "Epoch 18/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3211909632.0000 - val_loss: 3580603648.0000\n",
      "Epoch 19/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3202082560.0000 - val_loss: 3527870208.0000\n",
      "Epoch 20/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3170595072.0000 - val_loss: 3410961152.0000\n",
      "Epoch 21/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3147824128.0000 - val_loss: 3269108736.0000\n",
      "Epoch 22/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3124166912.0000 - val_loss: 3204875264.0000\n",
      "Epoch 23/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3070687232.0000 - val_loss: 3542271232.0000\n",
      "Epoch 24/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3052813312.0000 - val_loss: 3269476864.0000\n",
      "Epoch 25/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 3030784256.0000 - val_loss: 3222132224.0000\n",
      "Epoch 26/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2981434112.0000 - val_loss: 3330184960.0000\n",
      "Epoch 27/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2959887616.0000 - val_loss: 3408983552.0000\n",
      "Epoch 28/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2964387072.0000 - val_loss: 3100001792.0000\n",
      "Epoch 29/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2920799744.0000 - val_loss: 3083581952.0000\n",
      "Epoch 30/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2910818560.0000 - val_loss: 3068209152.0000\n",
      "Epoch 31/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2892419072.0000 - val_loss: 2887830784.0000\n",
      "Epoch 32/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2840213504.0000 - val_loss: 2969822208.0000\n",
      "Epoch 33/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2871948800.0000 - val_loss: 3027530240.0000\n",
      "Epoch 34/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2826061312.0000 - val_loss: 3088948736.0000\n",
      "Epoch 35/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2805245440.0000 - val_loss: 2921540608.0000\n",
      "Epoch 36/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2823396352.0000 - val_loss: 2935742976.0000\n",
      "Epoch 37/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2776606464.0000 - val_loss: 3234834944.0000\n",
      "Epoch 38/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2771848448.0000 - val_loss: 3037154304.0000\n",
      "Epoch 39/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2761400064.0000 - val_loss: 3040059904.0000\n",
      "Epoch 40/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2755213056.0000 - val_loss: 2953137408.0000\n",
      "Epoch 41/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2722328576.0000 - val_loss: 2941158400.0000\n",
      "Epoch 42/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2717822208.0000 - val_loss: 3119179776.0000\n",
      "Epoch 43/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2711713280.0000 - val_loss: 2889962496.0000\n",
      "Epoch 44/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2708334336.0000 - val_loss: 2967217152.0000\n",
      "Epoch 45/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2689121536.0000 - val_loss: 2965677312.0000\n",
      "Epoch 46/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2675070208.0000 - val_loss: 2961047808.0000\n",
      "Epoch 47/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2660881152.0000 - val_loss: 3022682112.0000\n",
      "Epoch 48/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2658064128.0000 - val_loss: 3161844992.0000\n",
      "Epoch 49/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2619944192.0000 - val_loss: 2986365440.0000\n",
      "Epoch 50/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2642044928.0000 - val_loss: 2849054208.0000\n",
      "Epoch 51/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2595761408.0000 - val_loss: 2934782464.0000\n",
      "Epoch 52/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2590974720.0000 - val_loss: 3452187136.0000\n",
      "Epoch 53/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2592978688.0000 - val_loss: 2989207296.0000\n",
      "Epoch 54/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2562789632.0000 - val_loss: 3075402496.0000\n",
      "Epoch 55/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2564840704.0000 - val_loss: 2990322688.0000\n",
      "Epoch 56/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2541359616.0000 - val_loss: 3027899648.0000\n",
      "Epoch 57/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2569622784.0000 - val_loss: 3598797568.0000\n",
      "Epoch 58/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2522608128.0000 - val_loss: 3442548736.0000\n",
      "Epoch 59/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2516607232.0000 - val_loss: 3083143424.0000\n",
      "Epoch 60/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2495257344.0000 - val_loss: 3068043520.0000\n",
      "Epoch 61/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2508360704.0000 - val_loss: 3094775552.0000\n",
      "Epoch 62/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2488688128.0000 - val_loss: 3313600000.0000\n",
      "Epoch 63/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2480350976.0000 - val_loss: 3092152320.0000\n",
      "Epoch 64/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2476441856.0000 - val_loss: 3164744960.0000\n",
      "Epoch 65/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2455801344.0000 - val_loss: 3313672448.0000\n",
      "Epoch 66/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2452394752.0000 - val_loss: 3305232384.0000\n",
      "Epoch 67/200\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 2440843264.0000 - val_loss: 3332317696.0000\n",
      "Epoch 68/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2425108480.0000 - val_loss: 3389885696.0000\n",
      "Epoch 69/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2409332736.0000 - val_loss: 3370629888.0000\n",
      "Epoch 70/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2399324160.0000 - val_loss: 3190681344.0000\n",
      "Epoch 71/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2390216448.0000 - val_loss: 3212211712.0000\n",
      "Epoch 72/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2387124480.0000 - val_loss: 3392129536.0000\n",
      "Epoch 73/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2391259648.0000 - val_loss: 3337833728.0000\n",
      "Epoch 74/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2362926848.0000 - val_loss: 3550403840.0000\n",
      "Epoch 75/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2342524160.0000 - val_loss: 3465712896.0000\n",
      "Epoch 76/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2348219904.0000 - val_loss: 3370072576.0000\n",
      "Epoch 77/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2335972352.0000 - val_loss: 3330234624.0000\n",
      "Epoch 78/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2331812096.0000 - val_loss: 3410998528.0000\n",
      "Epoch 79/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2311167488.0000 - val_loss: 3398198016.0000\n",
      "Epoch 80/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2311912448.0000 - val_loss: 3564569856.0000\n",
      "Epoch 81/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2299071744.0000 - val_loss: 3513283328.0000\n",
      "Epoch 82/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2307017472.0000 - val_loss: 3467737344.0000\n",
      "Epoch 83/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2283046144.0000 - val_loss: 3566281728.0000\n",
      "Epoch 84/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2300840448.0000 - val_loss: 3632257024.0000\n",
      "Epoch 85/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2268046336.0000 - val_loss: 3709735680.0000\n",
      "Epoch 86/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2239699968.0000 - val_loss: 3691076608.0000\n",
      "Epoch 87/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2265406976.0000 - val_loss: 3486144000.0000\n",
      "Epoch 88/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2242221824.0000 - val_loss: 3702917120.0000\n",
      "Epoch 89/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2252496128.0000 - val_loss: 3514842368.0000\n",
      "Epoch 90/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2229152000.0000 - val_loss: 3739795200.0000\n",
      "Epoch 91/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2216169216.0000 - val_loss: 3721592576.0000\n",
      "Epoch 92/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2215618304.0000 - val_loss: 3544918528.0000\n",
      "Epoch 93/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2183311872.0000 - val_loss: 3616348928.0000\n",
      "Epoch 94/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2216349440.0000 - val_loss: 3475507968.0000\n",
      "Epoch 95/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2173542912.0000 - val_loss: 3701992704.0000\n",
      "Epoch 96/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2192183040.0000 - val_loss: 3647087104.0000\n",
      "Epoch 97/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2158454528.0000 - val_loss: 3730697472.0000\n",
      "Epoch 98/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2157568256.0000 - val_loss: 3827514112.0000\n",
      "Epoch 99/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2165456384.0000 - val_loss: 3567911680.0000\n",
      "Epoch 100/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2147580160.0000 - val_loss: 3961690112.0000\n",
      "Epoch 101/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2135174016.0000 - val_loss: 3679836672.0000\n",
      "Epoch 102/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2147014144.0000 - val_loss: 3759040256.0000\n",
      "Epoch 103/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2154826240.0000 - val_loss: 3673779712.0000\n",
      "Epoch 104/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2113141504.0000 - val_loss: 3679947008.0000\n",
      "Epoch 105/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2127327360.0000 - val_loss: 3634022656.0000\n",
      "Epoch 106/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2108701696.0000 - val_loss: 3713202688.0000\n",
      "Epoch 107/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2114350208.0000 - val_loss: 3892719616.0000\n",
      "Epoch 108/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2106370688.0000 - val_loss: 3862200064.0000\n",
      "Epoch 109/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2106104576.0000 - val_loss: 3797202944.0000\n",
      "Epoch 110/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2069725440.0000 - val_loss: 3708553984.0000\n",
      "Epoch 111/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2106882944.0000 - val_loss: 3806601728.0000\n",
      "Epoch 112/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2058854528.0000 - val_loss: 3894857984.0000\n",
      "Epoch 113/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2047385472.0000 - val_loss: 3815201024.0000\n",
      "Epoch 114/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2052138496.0000 - val_loss: 3813967616.0000\n",
      "Epoch 115/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2071297920.0000 - val_loss: 3860882176.0000\n",
      "Epoch 116/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2030714752.0000 - val_loss: 3870597888.0000\n",
      "Epoch 117/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2028324736.0000 - val_loss: 3937687808.0000\n",
      "Epoch 118/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2026955264.0000 - val_loss: 4148482560.0000\n",
      "Epoch 119/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2027376768.0000 - val_loss: 3875259392.0000\n",
      "Epoch 120/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2018776064.0000 - val_loss: 3950618112.0000\n",
      "Epoch 121/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2028892416.0000 - val_loss: 3992250880.0000\n",
      "Epoch 122/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2006405760.0000 - val_loss: 3957033216.0000\n",
      "Epoch 123/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 2003786240.0000 - val_loss: 4002367232.0000\n",
      "Epoch 124/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 1986915584.0000 - val_loss: 3969132032.0000\n",
      "Epoch 125/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 1993177344.0000 - val_loss: 4148318976.0000\n",
      "Epoch 126/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 1959723136.0000 - val_loss: 3904024320.0000\n",
      "Epoch 127/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 1980925696.0000 - val_loss: 3967666176.0000\n",
      "Epoch 128/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 1976905216.0000 - val_loss: 4118478592.0000\n",
      "Epoch 129/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 1953000576.0000 - val_loss: 3816011008.0000\n",
      "Epoch 130/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 1955548416.0000 - val_loss: 4088450048.0000\n",
      "Epoch 131/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 1938694656.0000 - val_loss: 4200057344.0000\n",
      "Epoch 132/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 1934834560.0000 - val_loss: 4245642240.0000\n",
      "Epoch 133/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 1963648768.0000 - val_loss: 4227531264.0000\n",
      "Epoch 134/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 1938212992.0000 - val_loss: 4424140800.0000\n",
      "Epoch 135/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 1925982848.0000 - val_loss: 4311006720.0000\n",
      "Epoch 136/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 1916384512.0000 - val_loss: 3907759616.0000\n",
      "Epoch 137/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 1911165184.0000 - val_loss: 4078919424.0000\n",
      "Epoch 138/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 1909518080.0000 - val_loss: 4085957632.0000\n",
      "Epoch 139/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 1899742464.0000 - val_loss: 4456064000.0000\n",
      "Epoch 140/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 1893602048.0000 - val_loss: 4334051840.0000\n",
      "Epoch 141/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 1891527936.0000 - val_loss: 4204912128.0000\n",
      "Epoch 142/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 1862169856.0000 - val_loss: 4558758400.0000\n",
      "Epoch 143/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 1889376768.0000 - val_loss: 4205191680.0000\n",
      "Epoch 144/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 1889962880.0000 - val_loss: 4094251008.0000\n",
      "Epoch 145/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 1888615552.0000 - val_loss: 4239885824.0000\n",
      "Epoch 146/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 1858991744.0000 - val_loss: 4049866752.0000\n",
      "Epoch 147/200\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 1834270592.0000 - val_loss: 4294926080.0000\n",
      "Epoch 148/200\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 1844684672.0000 - val_loss: 4502565888.0000\n",
      "Epoch 149/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 1856658944.0000 - val_loss: 4336555520.0000\n",
      "Epoch 150/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 1827392768.0000 - val_loss: 4110480128.0000\n",
      "Epoch 151/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 1845481600.0000 - val_loss: 4173353216.0000\n",
      "Epoch 152/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 1839011968.0000 - val_loss: 4534751232.0000\n",
      "Epoch 153/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 1805343488.0000 - val_loss: 4203455744.0000\n",
      "Epoch 154/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 1818669056.0000 - val_loss: 4509511168.0000\n",
      "Epoch 155/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 1793113088.0000 - val_loss: 4278294784.0000\n",
      "Epoch 156/200\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 1807445376.0000 - val_loss: 4207405824.0000\n",
      "Epoch 157/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 1809669248.0000 - val_loss: 4438532096.0000\n",
      "Epoch 158/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 1797059840.0000 - val_loss: 4325066240.0000\n",
      "Epoch 159/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 1791350528.0000 - val_loss: 4550291968.0000\n",
      "Epoch 160/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 1785366784.0000 - val_loss: 4260824320.0000\n",
      "Epoch 161/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 1789601536.0000 - val_loss: 4527750656.0000\n",
      "Epoch 162/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 1776301312.0000 - val_loss: 4430913536.0000\n",
      "Epoch 163/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 1763097344.0000 - val_loss: 4505331200.0000\n",
      "Epoch 164/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 1760470400.0000 - val_loss: 4218198272.0000\n",
      "Epoch 165/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 1763599488.0000 - val_loss: 4346312704.0000\n",
      "Epoch 166/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 1755094144.0000 - val_loss: 4327051264.0000\n",
      "Epoch 167/200\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 1743578496.0000 - val_loss: 4668689920.0000\n",
      "Epoch 168/200\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 1751541888.0000 - val_loss: 4546734080.0000\n",
      "Epoch 169/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 1707758464.0000 - val_loss: 4727660544.0000\n",
      "Epoch 170/200\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 1734634112.0000 - val_loss: 4338921472.0000\n",
      "Epoch 171/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 1718438016.0000 - val_loss: 4498554880.0000\n",
      "Epoch 172/200\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 1706833536.0000 - val_loss: 4449438720.0000\n",
      "Epoch 173/200\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 1701111424.0000 - val_loss: 4472685568.0000\n",
      "Epoch 174/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 1698669568.0000 - val_loss: 4529839104.0000\n",
      "Epoch 175/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 1709073408.0000 - val_loss: 4858887168.0000\n",
      "Epoch 176/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 1713868544.0000 - val_loss: 4978370048.0000\n",
      "Epoch 177/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 1684145536.0000 - val_loss: 4710363136.0000\n",
      "Epoch 178/200\n",
      "549/549 [==============================] - 1s 3ms/step - loss: 1693947904.0000 - val_loss: 4481059328.0000\n",
      "Epoch 179/200\n",
      "549/549 [==============================] - 2s 3ms/step - loss: 1680705536.0000 - val_loss: 4488629760.0000\n",
      "Epoch 180/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 1662182016.0000 - val_loss: 4520615936.0000\n",
      "Epoch 181/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 1691053056.0000 - val_loss: 5025316864.0000\n",
      "Epoch 182/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 1649421440.0000 - val_loss: 4393196544.0000\n",
      "Epoch 183/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 1660840064.0000 - val_loss: 4516317184.0000\n",
      "Epoch 184/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 1632927104.0000 - val_loss: 4506494464.0000\n",
      "Epoch 185/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 1680139904.0000 - val_loss: 4417315840.0000\n",
      "Epoch 186/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 1638262528.0000 - val_loss: 4526948352.0000\n",
      "Epoch 187/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 1618931584.0000 - val_loss: 4989288448.0000\n",
      "Epoch 188/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 1628230784.0000 - val_loss: 4657225728.0000\n",
      "Epoch 189/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 1631529216.0000 - val_loss: 4948566528.0000\n",
      "Epoch 190/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 1609085952.0000 - val_loss: 4485550592.0000\n",
      "Epoch 191/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 1604845568.0000 - val_loss: 4679791104.0000\n",
      "Epoch 192/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 1631238912.0000 - val_loss: 4616130048.0000\n",
      "Epoch 193/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 1621225856.0000 - val_loss: 4666958336.0000\n",
      "Epoch 194/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 1592238464.0000 - val_loss: 4414174208.0000\n",
      "Epoch 195/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 1586756736.0000 - val_loss: 4842669568.0000\n",
      "Epoch 196/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 1582785280.0000 - val_loss: 5033111552.0000\n",
      "Epoch 197/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 1577583360.0000 - val_loss: 4740923904.0000\n",
      "Epoch 198/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 1571816192.0000 - val_loss: 4899633152.0000\n",
      "Epoch 199/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 1552064768.0000 - val_loss: 4803577856.0000\n",
      "Epoch 200/200\n",
      "549/549 [==============================] - 1s 2ms/step - loss: 1587153280.0000 - val_loss: 4745487360.0000\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>configuracion</th>\n",
       "      <th>error_test</th>\n",
       "      <th>error_train</th>\n",
       "      <th>segundos_entrenamiento</th>\n",
       "      <th>epoca</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>ANN_capas=[64; 64; 64; 64; 64; 64; 64; 64; 1]_activaciones=['elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1</td>\n",
       "      <td>53376.532371</td>\n",
       "      <td>51400.826141</td>\n",
       "      <td>67.506442</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ANN_capas=[64; 64; 64; 64; 64; 64; 64; 64; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1</td>\n",
       "      <td>54231.385747</td>\n",
       "      <td>51958.811996</td>\n",
       "      <td>63.929625</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>ANN_capas=[32; 50; 64; 80; 80; 64; 50; 32; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1</td>\n",
       "      <td>54464.082256</td>\n",
       "      <td>52447.311771</td>\n",
       "      <td>65.858972</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>ANN_capas=[64; 64; 64; 64; 64; 64; 64; 64; 1]_activaciones=['elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_porcentajesDropOut=[0.05; 0.05; 0.05; 0.05; 0.05; 0.05; 0.05; 0.05]_batchSize=32_optimizador=adam_paciencia=-1_epocas=90</td>\n",
       "      <td>54796.879327</td>\n",
       "      <td>53966.714482</td>\n",
       "      <td>148.886218</td>\n",
       "      <td>90.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>ANN_capas=[64; 64; 64; 64; 64; 64; 64; 64; 1]_activaciones=['selu'; 'selu'; 'selu'; 'selu'; 'selu'; 'selu'; 'selu'; 'selu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1</td>\n",
       "      <td>55036.204520</td>\n",
       "      <td>50085.477017</td>\n",
       "      <td>63.393345</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>ANN_capas=[50; 50; 50; 50; 50; 50; 50; 50; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1</td>\n",
       "      <td>55458.095748</td>\n",
       "      <td>52839.030423</td>\n",
       "      <td>58.103801</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>ANN_capas=[80; 80; 64; 64; 50; 50; 32; 32; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1</td>\n",
       "      <td>55472.535331</td>\n",
       "      <td>51841.128383</td>\n",
       "      <td>63.146074</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>ANN_capas=[64; 64; 64; 64; 64; 64; 64; 64; 1]_activaciones=['elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=nadam_paciencia=-1</td>\n",
       "      <td>55548.626626</td>\n",
       "      <td>51087.213273</td>\n",
       "      <td>73.071667</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>ANN_capas=[64; 64; 64; 64; 64; 64; 64; 64; 1]_activaciones=['elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=rmsprop_paciencia=-1</td>\n",
       "      <td>55743.102174</td>\n",
       "      <td>51816.125367</td>\n",
       "      <td>60.515572</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>ANN_capas=[64; 64; 64; 64; 64; 64; 64; 64; 1]_activaciones=['elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'linear']_inicializadorKernel=glorot_normal_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1</td>\n",
       "      <td>55989.743061</td>\n",
       "      <td>51369.158062</td>\n",
       "      <td>66.137771</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>ANN_capas=[64; 64; 64; 64; 64; 64; 64; 64; 1]_activaciones=['elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'linear']_inicializadorKernel=random_normal_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1</td>\n",
       "      <td>56144.034198</td>\n",
       "      <td>54215.765678</td>\n",
       "      <td>66.496849</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>ANN_capas=[64; 64; 64; 64; 64; 64; 64; 64; 1]_activaciones=['elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_porcentajesDropOut=[0.1; 0.1; 0.1; 0.1; 0.1; 0.1; 0.1; 0.1]_batchSize=32_optimizador=adam_paciencia=-1</td>\n",
       "      <td>56164.539987</td>\n",
       "      <td>60837.986028</td>\n",
       "      <td>83.210063</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ANN_capas=[8; 8; 8; 8; 8; 8; 8; 8; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=10</td>\n",
       "      <td>56168.076912</td>\n",
       "      <td>65505.463589</td>\n",
       "      <td>52.175863</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ANN_capas=[80; 80; 80; 80; 80; 80; 80; 80; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1</td>\n",
       "      <td>56252.450400</td>\n",
       "      <td>50596.551345</td>\n",
       "      <td>68.058731</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ANN_capas=[10; 10; 10; 10; 10; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=10</td>\n",
       "      <td>56579.769105</td>\n",
       "      <td>65503.054219</td>\n",
       "      <td>46.783819</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                         configuracion  \\\n",
       "33                                                                                ANN_capas=[64; 64; 64; 64; 64; 64; 64; 64; 1]_activaciones=['elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1   \n",
       "14                                                                        ANN_capas=[64; 64; 64; 64; 64; 64; 64; 64; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1   \n",
       "24                                                                        ANN_capas=[32; 50; 64; 80; 80; 64; 50; 32; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1   \n",
       "60  ANN_capas=[64; 64; 64; 64; 64; 64; 64; 64; 1]_activaciones=['elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_porcentajesDropOut=[0.05; 0.05; 0.05; 0.05; 0.05; 0.05; 0.05; 0.05]_batchSize=32_optimizador=adam_paciencia=-1_epocas=90   \n",
       "32                                                                        ANN_capas=[64; 64; 64; 64; 64; 64; 64; 64; 1]_activaciones=['selu'; 'selu'; 'selu'; 'selu'; 'selu'; 'selu'; 'selu'; 'selu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1   \n",
       "18                                                                        ANN_capas=[50; 50; 50; 50; 50; 50; 50; 50; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1   \n",
       "22                                                                        ANN_capas=[80; 80; 64; 64; 50; 50; 32; 32; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1   \n",
       "38                                                                               ANN_capas=[64; 64; 64; 64; 64; 64; 64; 64; 1]_activaciones=['elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=nadam_paciencia=-1   \n",
       "34                                                                             ANN_capas=[64; 64; 64; 64; 64; 64; 64; 64; 1]_activaciones=['elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=rmsprop_paciencia=-1   \n",
       "46                                                                                 ANN_capas=[64; 64; 64; 64; 64; 64; 64; 64; 1]_activaciones=['elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'linear']_inicializadorKernel=glorot_normal_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1   \n",
       "47                                                                                 ANN_capas=[64; 64; 64; 64; 64; 64; 64; 64; 1]_activaciones=['elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'linear']_inicializadorKernel=random_normal_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1   \n",
       "40                    ANN_capas=[64; 64; 64; 64; 64; 64; 64; 64; 1]_activaciones=['elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'elu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_porcentajesDropOut=[0.1; 0.1; 0.1; 0.1; 0.1; 0.1; 0.1; 0.1]_batchSize=32_optimizador=adam_paciencia=-1   \n",
       "7                                                                                 ANN_capas=[8; 8; 8; 8; 8; 8; 8; 8; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=10   \n",
       "15                                                                        ANN_capas=[80; 80; 80; 80; 80; 80; 80; 80; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=-1   \n",
       "3                                                                                                             ANN_capas=[10; 10; 10; 10; 10; 1]_activaciones=['relu'; 'relu'; 'relu'; 'relu'; 'relu'; 'linear']_inicializadorKernel=glorot_uniform_inicializadorSesgo=zeros_batchSize=32_optimizador=adam_paciencia=10   \n",
       "\n",
       "      error_test   error_train  segundos_entrenamiento  epoca  \n",
       "33  53376.532371  51400.826141               67.506442   50.0  \n",
       "14  54231.385747  51958.811996               63.929625   50.0  \n",
       "24  54464.082256  52447.311771               65.858972   50.0  \n",
       "60  54796.879327  53966.714482              148.886218   90.0  \n",
       "32  55036.204520  50085.477017               63.393345   50.0  \n",
       "18  55458.095748  52839.030423               58.103801   50.0  \n",
       "22  55472.535331  51841.128383               63.146074   50.0  \n",
       "38  55548.626626  51087.213273               73.071667   50.0  \n",
       "34  55743.102174  51816.125367               60.515572   50.0  \n",
       "46  55989.743061  51369.158062               66.137771   50.0  \n",
       "47  56144.034198  54215.765678               66.496849   50.0  \n",
       "40  56164.539987  60837.986028               83.210063   50.0  \n",
       "7   56168.076912  65505.463589               52.175863   50.0  \n",
       "15  56252.450400  50596.551345               68.058731   50.0  \n",
       "3   56579.769105  65503.054219               46.783819   50.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "listaCapas = [64, 64, 64, 64, 64, 64, 64, 64, 1]\n",
    "listaActivaciones = ['elu', 'elu', 'elu', 'elu', 'elu', 'elu', 'elu', 'elu', 'linear']\n",
    "redAnn, configuracion = construirANN(caracteristicasEstandarizadas.shape[1], listaCapas, listaActivaciones)\n",
    "redAnn.summary()\n",
    "datosEntrenamiento = entrenarANN(redAnn, configuracion, caracteristicasEstandarizadas, etiquetas, 'mean_squared_error', \\\n",
    "            tamanioBatch=32, epocas=200, paciencia = -1, forzarEntrenamiento=1)\n",
    "graficarResultados(datosEntrenamiento, 'Errores (Tamaño batch: 32)')\n",
    "\n",
    "mostrarResultadosBitacora('ANN')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorporated-future",
   "metadata": {},
   "source": [
    "### Elección de modelo y guardar por medio de puntos de revisión (checkpoints).\n",
    "Se puede observar a través del historial de entrenamiento y gráfica que las mejores métricas se obtienen después de 50 épocas, por lo que se obtendrá este modelo y se guardará en este directorio para que pueda ser cargado cuando sea necesario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cognitive-softball",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f67e074ca0>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nombreArchivo = 'mejorModeloAnn-{epoch:02d}-{val_loss:.2f}.h5'\n",
    "puntoRevision = ModelCheckpoint(nombreArchivo, monitor='val_loss', verbose=0, save_best_only=True, \\\n",
    "                                mode='min')\n",
    "listaCapas = [64, 64, 64, 64, 64, 64, 64, 64, 1]\n",
    "listaActivaciones = ['elu', 'elu', 'elu', 'elu', 'elu', 'elu', 'elu', 'elu', 'linear']\n",
    "redAnn, configuracion = construirANN(caracteristicasEstandarizadas.shape[1], listaCapas, listaActivaciones)\n",
    "entrenarANN(redAnn, configuracion, caracteristicasEstandarizadas, etiquetas, 'mean_squared_error', \\\n",
    "            tamanioBatch=32, epocas=200, paciencia = -1, forzarEntrenamiento=1, mostrarResultados=0, \\\n",
    "           puntoRevision = puntoRevision)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "younger-habitat",
   "metadata": {},
   "source": [
    "### Cargar mejor modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "assisted-auction",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Obtener nombre de mejor modelo\n",
    "import os\n",
    "archivos = [f for f in os.listdir('.') if os.path.isfile(f) and '.h5' in f]\n",
    "nombreMejorModelo = archivos[len(archivos)-1]\n",
    "## Cargar mejor modelo\n",
    "mejorModeloAnn = load_model(nombreMejorModelo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "turkish-formation",
   "metadata": {},
   "source": [
    "### Realizar predicciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "other-format",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 22ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[417628.03],\n",
       "       [429365.6 ],\n",
       "       [407632.78],\n",
       "       [322805.12],\n",
       "       [296242.03],\n",
       "       [259600.58],\n",
       "       [292020.06],\n",
       "       [313006.1 ],\n",
       "       [168541.25],\n",
       "       [307880.44]], dtype=float32)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mejorModeloAnn.predict(caracteristicasEstandarizadas[0:10,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "hired-adoption",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    452600.0\n",
       "1    358500.0\n",
       "2    352100.0\n",
       "3    341300.0\n",
       "4    342200.0\n",
       "5    269700.0\n",
       "6    299200.0\n",
       "7    241400.0\n",
       "8    226700.0\n",
       "9    261100.0\n",
       "Name: median_house_value, dtype: float64"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "etiquetas[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "endangered-insulin",
   "metadata": {},
   "source": [
    "## Conclusiones\n",
    "* El RMSE obtenido para los datos de entrenamiento fue de: 51,401 y para los de validación fue de 53,377. Estos valores son bastante aceptables e incluso mejores comparados a otros modelos obtenidos con regresión lineal donde se aplicaron distintas técnicas de ingeniería de características (interacción de variables, correlación entre variables, creación de nuevas características basadas en otras, descarte de características, etc.). Con una red neuronal feed-forward (convencional) aunque se agrega un grado alto de complejidad, no se debe tener un conocimiento del dominio del problema a realizar, ni de las relaciones que tienen las características entre sí para obtener modelos con gran poder de predicción.\n",
    "* A pesar que una red neuronal con gran número de parámetros puede estar propensa a sobre-ajuste, hay técnicas como DropOut o BatchNormalization que permiten atacar este problema.\n",
    "* A pesar que una red neuronal no necesita técnicas de ingeniería de características, siempre se deben aplicar etapas de pre-procesamiento de información como: manejo de información faltante, escalado de datos numéricos y conversión de información categórica a información numérica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "perceived-promotion",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
