{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "blond-seattle",
   "metadata": {},
   "source": [
    "# Proyecto Final - Deep Learning\n",
    "## Parte 2 - Convolutional Network\n",
    "\n",
    "**Curso:** Statistical Learning II\n",
    "\n",
    "**Catedrático:** Ing. Luis Leal\n",
    "\n",
    "**Estudiante:** Dany Rafael Díaz Lux (21000864)\n",
    "\n",
    "**Objetivo:** Entrenar una o varias redes neuronales convolucionales para llevar a cabo clasificación de imágenes con una exactitud mínima de 85%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "missing-referral",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "#from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import matplotlib.pylab as plt\n",
    "import datetime as dt\n",
    "#import math\n",
    "import numpy as np \n",
    "import os, shutil\n",
    "#import pandas as pd\n",
    "import random as python_random\n",
    "import sklearn.metrics as mts\n",
    "import tensorflow as tf\n",
    "#import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "formal-qualification",
   "metadata": {},
   "source": [
    "## Conjunto de datos: Imágenes de animales del mar\n",
    "Es un conjunto de imágenes de distintos animales marinos. Actualmente contiene 19 diferentes clases de animales, puede ser descargado aquí: https://www.kaggle.com/datasets/vencerlanz09/sea-animals-image-dataste\n",
    "\n",
    "### Crear directorios e imágenes en caso de ser necesario\n",
    "_(Ejecutar siguiente bloque de código sólo si se desea eliminar y crear de nuevo directorio de imágenes.)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "specialized-fleece",
   "metadata": {},
   "outputs": [],
   "source": [
    "directorioFuente = 'directorio_original'\n",
    "directorioPrincipal = 'sea_animals'\n",
    "directorioEntrenamiento = os.path.join(directorioPrincipal, 'train')\n",
    "directorioValidacion = os.path.join(directorioPrincipal, 'test')\n",
    "# Eliminar directorio de imágenes si existe\n",
    "if(os.path.isdir(directorioPrincipal)):\n",
    "    shutil.rmtree(directorioPrincipal)\n",
    "# Crear directorio\n",
    "os.mkdir(directorioPrincipal)\n",
    "# Crear sub-directorios para entrenamiento y validación\n",
    "os.mkdir(directorioEntrenamiento)\n",
    "os.mkdir(directorioValidacion)\n",
    "# Obtener todos los subdirectorios del directorio fuente\n",
    "directoriosAnimales = [d for d in os.listdir(directorioFuente) if os.path.isdir(os.path.join(directorioFuente, d))]\n",
    "# Crear subdirectorios de animales para entrenamiento y validación\n",
    "for animal in directoriosAnimales:\n",
    "    os.mkdir(os.path.join(directorioEntrenamiento, animal))\n",
    "    os.mkdir(os.path.join(directorioValidacion, animal))\n",
    "# Copiar imágenes a directorios de entrenamiento y prueba\n",
    "porcentajeTotalImagenesACopiar = 1\n",
    "porcentajeValidacion = 0.15\n",
    "# Para cada directorio de animales elegir el porcentaje a copiar y después el porcentaje que será de validación\n",
    "for animal in directoriosAnimales:\n",
    "    dirAnimalFuente = os.path.join(directorioFuente, animal)\n",
    "    dirAnimalEntrenamiento = os.path.join(directorioEntrenamiento, animal)\n",
    "    dirAnimalValidacion = os.path.join(directorioValidacion, animal)\n",
    "    # Obtener nombre de todos los archivos del directorio\n",
    "    nombreImagenes = [f for f in os.listdir(dirAnimalFuente) if os.path.isfile(os.path.join(dirAnimalFuente, f))]\n",
    "    python_random.seed(21000864)\n",
    "    imagenesACopiar = python_random.sample(nombreImagenes, int(len(nombreImagenes) * porcentajeTotalImagenesACopiar))\n",
    "    python_random.seed(21000864)\n",
    "    imagenesValidacion = python_random.sample(imagenesACopiar, int(len(imagenesACopiar) * porcentajeValidacion))\n",
    "    imagenesEntrenamiento = [i for i in imagenesACopiar if i not in imagenesValidacion]\n",
    "    for imagen in imagenesEntrenamiento:\n",
    "        shutil.copyfile(os.path.join(dirAnimalFuente, imagen), os.path.join(dirAnimalEntrenamiento, imagen))\n",
    "    for imagen in imagenesValidacion:\n",
    "        shutil.copyfile(os.path.join(dirAnimalFuente, imagen), os.path.join(dirAnimalValidacion, imagen))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dramatic-diversity",
   "metadata": {},
   "source": [
    "### Carga de imágenes con \"ImageDataGenerator\"\n",
    "Se realizarán diversos cambios a las imágenes originales con el objetivo de enriquecer las imágenes de entrenamiento y mejorar el rendimiento de la red neuronal convolucional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "gentle-writer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9990 images belonging to 19 classes.\n"
     ]
    }
   ],
   "source": [
    "# Parámetros que serán utilizados por generadores\n",
    "altura = 32\n",
    "ancho = 32\n",
    "tamanioBatch = 32\n",
    "# Se crearán generadores de datos para imágenes, se aplicarán diversos cambios a las imágenes de entrenamiento\n",
    "definicionGenEntrenamiento = ImageDataGenerator(\\\n",
    "      rescale=1./255,\\\n",
    "      rotation_range=30,\\\n",
    "      width_shift_range=0.2,\\\n",
    "      height_shift_range=0.2,\\\n",
    "      shear_range=0.1,\\\n",
    "      zoom_range=0.3,\\\n",
    "      horizontal_flip=True,\\\n",
    "      fill_mode='nearest')\n",
    "generadorEntrenamiento = definicionGenEntrenamiento.flow_from_directory(\\\n",
    "                            directorioEntrenamiento,\\\n",
    "                            batch_size=tamanioBatch,\\\n",
    "                            class_mode='categorical',\\\n",
    "                            target_size=(altura, ancho))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "abandoned-reform",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1752 images belonging to 19 classes.\n"
     ]
    }
   ],
   "source": [
    "# Generador para datos de validación\n",
    "definicionGenValidacion = ImageDataGenerator(rescale=1./255)\n",
    "generadorValidacion = definicionGenValidacion.flow_from_directory(\\\n",
    "                            directorioValidacion,\\\n",
    "                            batch_size=tamanioBatch,\\\n",
    "                            class_mode='categorical',\\\n",
    "                            target_size=(altura, ancho))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "institutional-clinton",
   "metadata": {},
   "source": [
    "## Arquitectura de Red Neuronal Convolucional\n",
    "A continuación se definirá una red neuronal convolucional específica para solucionar este problema de clasificación de imágenes. Las decisiones más importantes para esta arquitectura son:\n",
    "* Se aplicarán 3 capas convolucionales.\n",
    "* Se aplicarán 3 capas de de MaxPooling.\n",
    "* Se aplicarán 2 capas finales completamente conectadas (fully-connected).\n",
    "* La función de activación en las capas ocultas será 'ReLU'.\n",
    "* La función de activación en la capa final será 'softmax' por tratarse de un problema de clasificiación múltiple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "demonstrated-semester",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_136\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_756 (Conv2D)         (None, 30, 30, 32)        896       \n",
      "                                                                 \n",
      " max_pooling2d_426 (MaxPooli  (None, 15, 15, 32)       0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " conv2d_757 (Conv2D)         (None, 13, 13, 128)       36992     \n",
      "                                                                 \n",
      " max_pooling2d_427 (MaxPooli  (None, 6, 6, 128)        0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " flatten_136 (Flatten)       (None, 4608)              0         \n",
      "                                                                 \n",
      " dense_400 (Dense)           (None, 256)               1179904   \n",
      "                                                                 \n",
      " dense_401 (Dense)           (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_402 (Dense)           (None, 19)                4883      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,288,467\n",
      "Trainable params: 1,288,467\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Especificar seed para reproducir resultados\n",
    "np.random.seed(21000864)\n",
    "tf.random.set_seed(21000864)\n",
    "python_random.seed(21000864)\n",
    "modeloCNN = Sequential([\n",
    "    Conv2D(32, (3,3), activation='relu', input_shape=(altura, ancho, 3)),\n",
    "    MaxPooling2D(2,2),\n",
    "    Conv2D(128, (3,3), activation='relu'),\n",
    "    MaxPooling2D(2,2),\n",
    "    #Conv2D(128, (3,3), activation='relu'),\n",
    "    #MaxPooling2D(2,2),\n",
    "    Flatten(),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dense(19, activation='softmax')\n",
    "])\n",
    "modeloCNN.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "practical-heater",
   "metadata": {},
   "source": [
    "### Parámetros para entrenamiento\n",
    "* El modelo utilizará optimizador adam.\n",
    "* La función de costo será 'categorical_crossentropy' por ser un problema de clasificación múltiple.\n",
    "* Se tomará en cuenta la métrica de 'accuracy' (exactitud)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "touched-recorder",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Especificar seed para reproducir resultados\n",
    "np.random.seed(21000864)\n",
    "tf.random.set_seed(21000864)\n",
    "python_random.seed(21000864)\n",
    "modeloCNN.compile(optimizer='adam',\n",
    "                 loss='categorical_crossentropy',\n",
    "                 metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "psychological-thomas",
   "metadata": {},
   "source": [
    "### Entrenamiento de de modelo\n",
    "* Se realizará un entrenamiento de 100 épocas.\n",
    "* Se definirá puntos de revisión (checkpoints) para los modelos con mejor métrica en los datos de validación con respecto a la exactitud.\n",
    "* Se detendrá el entrenamiento si después de 10 épocas no se produce mejora en el error de los datos de validación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "animated-solid",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "313/313 [==============================] - 34s 106ms/step - loss: 2.4366 - accuracy: 0.2548 - val_loss: 2.2396 - val_accuracy: 0.2905\n",
      "Epoch 2/100\n",
      "313/313 [==============================] - 35s 112ms/step - loss: 2.2165 - accuracy: 0.3099 - val_loss: 2.1455 - val_accuracy: 0.3174\n",
      "Epoch 3/100\n",
      "313/313 [==============================] - 35s 111ms/step - loss: 2.1146 - accuracy: 0.3362 - val_loss: 2.0443 - val_accuracy: 0.3716\n",
      "Epoch 4/100\n",
      "313/313 [==============================] - 39s 124ms/step - loss: 2.0648 - accuracy: 0.3513 - val_loss: 2.1062 - val_accuracy: 0.3516\n",
      "Epoch 5/100\n",
      "313/313 [==============================] - 34s 107ms/step - loss: 2.0046 - accuracy: 0.3659 - val_loss: 1.9418 - val_accuracy: 0.3818\n",
      "Epoch 6/100\n",
      "313/313 [==============================] - 37s 118ms/step - loss: 1.9804 - accuracy: 0.3733 - val_loss: 1.8749 - val_accuracy: 0.3990\n",
      "Epoch 7/100\n",
      "313/313 [==============================] - 37s 119ms/step - loss: 1.9335 - accuracy: 0.3901 - val_loss: 1.8491 - val_accuracy: 0.4218\n",
      "Epoch 8/100\n",
      "313/313 [==============================] - 38s 120ms/step - loss: 1.9013 - accuracy: 0.3978 - val_loss: 1.9550 - val_accuracy: 0.4030\n",
      "Epoch 9/100\n",
      "313/313 [==============================] - 36s 116ms/step - loss: 1.8625 - accuracy: 0.4132 - val_loss: 1.9418 - val_accuracy: 0.3955\n",
      "Epoch 10/100\n",
      "313/313 [==============================] - 36s 114ms/step - loss: 1.8410 - accuracy: 0.4158 - val_loss: 1.9291 - val_accuracy: 0.4207\n",
      "Epoch 11/100\n",
      "313/313 [==============================] - 35s 113ms/step - loss: 1.8052 - accuracy: 0.4292 - val_loss: 1.9035 - val_accuracy: 0.4092\n",
      "Epoch 12/100\n",
      "313/313 [==============================] - 37s 117ms/step - loss: 1.7884 - accuracy: 0.4349 - val_loss: 1.9035 - val_accuracy: 0.4064\n",
      "Epoch 13/100\n",
      "313/313 [==============================] - 35s 113ms/step - loss: 1.7585 - accuracy: 0.4413 - val_loss: 1.7628 - val_accuracy: 0.4509\n",
      "Epoch 14/100\n",
      "313/313 [==============================] - 35s 112ms/step - loss: 1.7552 - accuracy: 0.4410 - val_loss: 1.7481 - val_accuracy: 0.4481\n",
      "Epoch 15/100\n",
      "313/313 [==============================] - 35s 112ms/step - loss: 1.7097 - accuracy: 0.4597 - val_loss: 1.8005 - val_accuracy: 0.4509\n",
      "Epoch 16/100\n",
      "313/313 [==============================] - 35s 112ms/step - loss: 1.7050 - accuracy: 0.4613 - val_loss: 1.7821 - val_accuracy: 0.4406\n",
      "Epoch 17/100\n",
      "313/313 [==============================] - 35s 113ms/step - loss: 1.6885 - accuracy: 0.4589 - val_loss: 1.9486 - val_accuracy: 0.4161\n",
      "Epoch 18/100\n",
      "313/313 [==============================] - 35s 111ms/step - loss: 1.6670 - accuracy: 0.4680 - val_loss: 1.8324 - val_accuracy: 0.4338\n",
      "Epoch 19/100\n",
      "313/313 [==============================] - 35s 113ms/step - loss: 1.6490 - accuracy: 0.4754 - val_loss: 1.7478 - val_accuracy: 0.4555\n",
      "Epoch 20/100\n",
      "313/313 [==============================] - 35s 112ms/step - loss: 1.6378 - accuracy: 0.4739 - val_loss: 1.9603 - val_accuracy: 0.4189\n",
      "Epoch 21/100\n",
      "313/313 [==============================] - 35s 112ms/step - loss: 1.6217 - accuracy: 0.4821 - val_loss: 2.0012 - val_accuracy: 0.4001\n",
      "Epoch 22/100\n",
      "313/313 [==============================] - 35s 112ms/step - loss: 1.5905 - accuracy: 0.4877 - val_loss: 1.8587 - val_accuracy: 0.4355\n",
      "Epoch 23/100\n",
      "313/313 [==============================] - 33s 106ms/step - loss: 1.5950 - accuracy: 0.4856 - val_loss: 1.8249 - val_accuracy: 0.4384\n",
      "Epoch 24/100\n",
      "313/313 [==============================] - 34s 108ms/step - loss: 1.5621 - accuracy: 0.4985 - val_loss: 1.7289 - val_accuracy: 0.4926\n",
      "Epoch 25/100\n",
      "313/313 [==============================] - 32s 103ms/step - loss: 1.5612 - accuracy: 0.4973 - val_loss: 1.9231 - val_accuracy: 0.4201\n",
      "Epoch 26/100\n",
      "313/313 [==============================] - 37s 119ms/step - loss: 1.5354 - accuracy: 0.5018 - val_loss: 1.8425 - val_accuracy: 0.4509\n",
      "Epoch 27/100\n",
      "313/313 [==============================] - 41s 130ms/step - loss: 1.5314 - accuracy: 0.5086 - val_loss: 1.9239 - val_accuracy: 0.4486\n",
      "Epoch 28/100\n",
      "313/313 [==============================] - 41s 130ms/step - loss: 1.5288 - accuracy: 0.5062 - val_loss: 1.9973 - val_accuracy: 0.4201\n",
      "Epoch 29/100\n",
      "313/313 [==============================] - 39s 125ms/step - loss: 1.5101 - accuracy: 0.5123 - val_loss: 1.9726 - val_accuracy: 0.4338\n",
      "Epoch 30/100\n",
      "313/313 [==============================] - 38s 122ms/step - loss: 1.4902 - accuracy: 0.5190 - val_loss: 1.9194 - val_accuracy: 0.4241\n",
      "Epoch 31/100\n",
      "313/313 [==============================] - 38s 123ms/step - loss: 1.4783 - accuracy: 0.5191 - val_loss: 1.8286 - val_accuracy: 0.4561\n",
      "Epoch 32/100\n",
      "313/313 [==============================] - 38s 123ms/step - loss: 1.4573 - accuracy: 0.5252 - val_loss: 1.8939 - val_accuracy: 0.4469\n",
      "Epoch 33/100\n",
      "313/313 [==============================] - 38s 122ms/step - loss: 1.4528 - accuracy: 0.5228 - val_loss: 1.9480 - val_accuracy: 0.4372\n",
      "Epoch 34/100\n",
      "313/313 [==============================] - 38s 122ms/step - loss: 1.4480 - accuracy: 0.5307 - val_loss: 1.7990 - val_accuracy: 0.4612\n"
     ]
    }
   ],
   "source": [
    "funcionesARevisar = []\n",
    "# Detener entrenamiento si no mejora el error en los datos de validación después de 10 épocas.\n",
    "funcionesARevisar.append(EarlyStopping(monitor='val_loss', patience=10))\n",
    "funcionesARevisar.append(ModelCheckpoint('mejorModeloCnn-{epoch:02d}-{val_accuracy:.2f}.h5', monitor='val_accuracy', save_best_only=True))\n",
    "\n",
    "# Especificar seed para reproducir resultados\n",
    "np.random.seed(21000864)\n",
    "tf.random.set_seed(21000864)\n",
    "python_random.seed(21000864)\n",
    "datosEntrenamiento = modeloCNN.fit(generadorEntrenamiento,\n",
    "                                   epochs=100,\n",
    "                                   validation_data=generadorValidacion,\n",
    "                                   callbacks=funcionesARevisar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "increasing-sentence",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "313/313 [==============================] - 36s 115ms/step - loss: 1.4313 - accuracy: 0.5342 - val_loss: 1.9617 - val_accuracy: 0.4412\n",
      "Epoch 2/100\n",
      "313/313 [==============================] - 38s 123ms/step - loss: 1.4157 - accuracy: 0.5394 - val_loss: 1.9944 - val_accuracy: 0.4515\n",
      "Epoch 3/100\n",
      "313/313 [==============================] - 40s 127ms/step - loss: 1.4123 - accuracy: 0.5371 - val_loss: 2.0303 - val_accuracy: 0.4463\n",
      "Epoch 4/100\n",
      "313/313 [==============================] - 36s 114ms/step - loss: 1.4062 - accuracy: 0.5407 - val_loss: 1.9357 - val_accuracy: 0.4652\n",
      "Epoch 5/100\n",
      "313/313 [==============================] - 39s 124ms/step - loss: 1.3815 - accuracy: 0.5476 - val_loss: 1.8509 - val_accuracy: 0.4789\n",
      "Epoch 6/100\n",
      "313/313 [==============================] - 36s 114ms/step - loss: 1.3817 - accuracy: 0.5457 - val_loss: 1.8792 - val_accuracy: 0.4686\n",
      "Epoch 7/100\n",
      "313/313 [==============================] - 36s 116ms/step - loss: 1.3588 - accuracy: 0.5577 - val_loss: 1.9270 - val_accuracy: 0.4566\n",
      "Epoch 8/100\n",
      "313/313 [==============================] - 36s 115ms/step - loss: 1.3655 - accuracy: 0.5478 - val_loss: 1.9537 - val_accuracy: 0.4589\n",
      "Epoch 9/100\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 1.3560 - accuracy: 0.5517 - val_loss: 1.8663 - val_accuracy: 0.4697\n",
      "Epoch 10/100\n",
      "313/313 [==============================] - 30s 94ms/step - loss: 1.3365 - accuracy: 0.5599 - val_loss: 1.9786 - val_accuracy: 0.4344\n",
      "Epoch 11/100\n",
      "313/313 [==============================] - 30s 95ms/step - loss: 1.3306 - accuracy: 0.5587 - val_loss: 1.9430 - val_accuracy: 0.4583\n",
      "Epoch 12/100\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 1.3171 - accuracy: 0.5658 - val_loss: 1.8957 - val_accuracy: 0.4812\n",
      "Epoch 13/100\n",
      "313/313 [==============================] - 30s 95ms/step - loss: 1.3137 - accuracy: 0.5636 - val_loss: 2.0220 - val_accuracy: 0.4424\n",
      "Epoch 14/100\n",
      "313/313 [==============================] - 32s 102ms/step - loss: 1.2847 - accuracy: 0.5737 - val_loss: 2.0112 - val_accuracy: 0.4697\n",
      "Epoch 15/100\n",
      "313/313 [==============================] - 37s 117ms/step - loss: 1.2930 - accuracy: 0.5682 - val_loss: 2.0965 - val_accuracy: 0.4355\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(21000864)\n",
    "tf.random.set_seed(21000864)\n",
    "python_random.seed(21000864)\n",
    "datosEntrenamiento = modeloCNN.fit(generadorEntrenamiento,\n",
    "                                   epochs=100,\n",
    "                                   validation_data=generadorValidacion,\n",
    "                                   callbacks=funcionesARevisar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "necessary-vietnam",
   "metadata": {},
   "source": [
    "### (No se continuo con esta segunda parte por limitaciones de tiempo y por no llegar a un 85% de exactitud.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loving-beach",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
