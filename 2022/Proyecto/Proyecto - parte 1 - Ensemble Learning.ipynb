{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "experienced-preparation",
   "metadata": {},
   "source": [
    "# Proyecto Final - Ensemble Learning (Aprendizaje por ensamblado)\n",
    "## Parte 1 - Entrenamieto, selección y validación.\n",
    "\n",
    "**Curso:** Statistical Learning\n",
    "\n",
    "**Catedrático:** Ing. Luis Leal\n",
    "\n",
    "**Estudiante:** Dany Rafael Díaz Lux (21000864)\n",
    "\n",
    "**Objetivo:** Hacer clasificación binaria para determinar si una persona sobrevive o no al hundimiento del Titanic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seasonal-principal",
   "metadata": {},
   "source": [
    "## Cargar información"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 654,
   "id": "convertible-operation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor flow version: 2.4.1\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import datetime as dt\n",
    "import joblib\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np \n",
    "import os.path\n",
    "import pandas as pd\n",
    "import scipy.stats as st\n",
    "import sklearn.metrics as mts\n",
    "import tensorflow as tf\n",
    "print('Tensor flow version: ' + tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "massive-antarctica",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Name</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>passenger_class</th>\n",
       "      <th>passenger_sex</th>\n",
       "      <th>passenger_survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>Lower</td>\n",
       "      <td>M</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "      <td>Upper</td>\n",
       "      <td>F</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>Lower</td>\n",
       "      <td>F</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "      <td>Upper</td>\n",
       "      <td>F</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>Lower</td>\n",
       "      <td>M</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>887</td>\n",
       "      <td>Montvila, Rev. Juozas</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>211536</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>Middle</td>\n",
       "      <td>M</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>888</td>\n",
       "      <td>Graham, Miss. Margaret Edith</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>112053</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>B42</td>\n",
       "      <td>S</td>\n",
       "      <td>Upper</td>\n",
       "      <td>F</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>889</td>\n",
       "      <td>Johnston, Miss. Catherine Helen \"Carrie\"</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>W./C. 6607</td>\n",
       "      <td>23.4500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>Lower</td>\n",
       "      <td>F</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>890</td>\n",
       "      <td>Behr, Mr. Karl Howell</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>111369</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>C148</td>\n",
       "      <td>C</td>\n",
       "      <td>Upper</td>\n",
       "      <td>M</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>891</td>\n",
       "      <td>Dooley, Mr. Patrick</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>370376</td>\n",
       "      <td>7.7500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "      <td>Lower</td>\n",
       "      <td>M</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>891 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     PassengerId                                               Name   Age  \\\n",
       "0              1                            Braund, Mr. Owen Harris  22.0   \n",
       "1              2  Cumings, Mrs. John Bradley (Florence Briggs Th...  38.0   \n",
       "2              3                             Heikkinen, Miss. Laina  26.0   \n",
       "3              4       Futrelle, Mrs. Jacques Heath (Lily May Peel)  35.0   \n",
       "4              5                           Allen, Mr. William Henry  35.0   \n",
       "..           ...                                                ...   ...   \n",
       "886          887                              Montvila, Rev. Juozas  27.0   \n",
       "887          888                       Graham, Miss. Margaret Edith  19.0   \n",
       "888          889           Johnston, Miss. Catherine Helen \"Carrie\"   NaN   \n",
       "889          890                              Behr, Mr. Karl Howell  26.0   \n",
       "890          891                                Dooley, Mr. Patrick  32.0   \n",
       "\n",
       "     SibSp  Parch            Ticket     Fare Cabin Embarked passenger_class  \\\n",
       "0        1      0         A/5 21171   7.2500   NaN        S           Lower   \n",
       "1        1      0          PC 17599  71.2833   C85        C           Upper   \n",
       "2        0      0  STON/O2. 3101282   7.9250   NaN        S           Lower   \n",
       "3        1      0            113803  53.1000  C123        S           Upper   \n",
       "4        0      0            373450   8.0500   NaN        S           Lower   \n",
       "..     ...    ...               ...      ...   ...      ...             ...   \n",
       "886      0      0            211536  13.0000   NaN        S          Middle   \n",
       "887      0      0            112053  30.0000   B42        S           Upper   \n",
       "888      1      2        W./C. 6607  23.4500   NaN        S           Lower   \n",
       "889      0      0            111369  30.0000  C148        C           Upper   \n",
       "890      0      0            370376   7.7500   NaN        Q           Lower   \n",
       "\n",
       "    passenger_sex passenger_survived  \n",
       "0               M                  N  \n",
       "1               F                  Y  \n",
       "2               F                  Y  \n",
       "3               F                  Y  \n",
       "4               M                  N  \n",
       "..            ...                ...  \n",
       "886             M                  N  \n",
       "887             F                  Y  \n",
       "888             F                  N  \n",
       "889             M                  Y  \n",
       "890             M                  N  \n",
       "\n",
       "[891 rows x 12 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data_titanic_proyecto.csv\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "structured-standing",
   "metadata": {},
   "source": [
    "## Pre-procesamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "freelance-nitrogen",
   "metadata": {},
   "source": [
    "### Ignorar columnas identificadoras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "polar-oriental",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>passenger_class</th>\n",
       "      <th>passenger_sex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>S</td>\n",
       "      <td>Lower</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C</td>\n",
       "      <td>Upper</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>S</td>\n",
       "      <td>Lower</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>S</td>\n",
       "      <td>Upper</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>S</td>\n",
       "      <td>Lower</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Age  SibSp  Parch     Fare Embarked passenger_class passenger_sex\n",
       "0  22.0      1      0   7.2500        S           Lower             M\n",
       "1  38.0      1      0  71.2833        C           Upper             F\n",
       "2  26.0      0      0   7.9250        S           Lower             F\n",
       "3  35.0      1      0  53.1000        S           Upper             F\n",
       "4  35.0      0      0   8.0500        S           Lower             M"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0    N\n",
       "1    Y\n",
       "2    Y\n",
       "3    Y\n",
       "4    N\n",
       "Name: passenger_survived, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Omitir columnas \"PassengerId\", \"Name\", \"Ticket\" y \"Cabin\" por ser columnas que tratan de distinguir a cada individuo y\n",
    "# no buscan indicar una característica general.\n",
    "caracteristicas = df.iloc[:,[2,3,4,6,8,9,10]]\n",
    "etiquetas = df.iloc[:,11]\n",
    "display(caracteristicas.head())\n",
    "display(etiquetas.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complicated-regression",
   "metadata": {},
   "source": [
    "### Detectar columnas con información faltante (NaN), determinar porcentaje, e imputación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "visible-nurse",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Porcentaje NaN en  Age : 19.87 %\n",
      "Porcentaje NaN en  Embarked : 0.22 %\n"
     ]
    }
   ],
   "source": [
    "# Listar columnas con valores NaN\n",
    "columnasConNaN = caracteristicas.columns[caracteristicas.isna().any()].tolist()\n",
    "for columna in columnasConNaN:\n",
    "    print('Porcentaje NaN en ', columna, ':', round(100 * \\\n",
    "            len(caracteristicas[caracteristicas[columna].isna()]) / len(caracteristicas), 2), '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "retired-tractor",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dany\\anaconda3\\envs\\py_galileo_2021\\lib\\site-packages\\pandas\\core\\series.py:4463: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().fillna(\n"
     ]
    }
   ],
   "source": [
    "# Se imputará la información faltante con la media de los datos en la columna \"Age\"\n",
    "caracteristicas['Age'].fillna(value=caracteristicas['Age'].mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dietary-salem",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se imputará la información faltante con la moda de los datos en la columna \"Embarked\" por ser categórica\n",
    "caracteristicas['Embarked'].fillna(value=caracteristicas['Embarked'].mode()[0], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "controversial-continuity",
   "metadata": {},
   "source": [
    "### One hot encoding en columnas categóricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "mature-scope",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked_C</th>\n",
       "      <th>Embarked_Q</th>\n",
       "      <th>Embarked_S</th>\n",
       "      <th>passenger_class_Lower</th>\n",
       "      <th>passenger_class_Middle</th>\n",
       "      <th>passenger_class_Upper</th>\n",
       "      <th>passenger_sex_M</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Age  SibSp  Parch     Fare  Embarked_C  Embarked_Q  Embarked_S  \\\n",
       "0  22.0      1      0   7.2500           0           0           1   \n",
       "1  38.0      1      0  71.2833           1           0           0   \n",
       "2  26.0      0      0   7.9250           0           0           1   \n",
       "3  35.0      1      0  53.1000           0           0           1   \n",
       "4  35.0      0      0   8.0500           0           0           1   \n",
       "\n",
       "   passenger_class_Lower  passenger_class_Middle  passenger_class_Upper  \\\n",
       "0                      1                       0                      0   \n",
       "1                      0                       0                      1   \n",
       "2                      1                       0                      0   \n",
       "3                      0                       0                      1   \n",
       "4                      1                       0                      0   \n",
       "\n",
       "   passenger_sex_M  \n",
       "0                1  \n",
       "1                0  \n",
       "2                0  \n",
       "3                0  \n",
       "4                1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>passenger_survived_Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   passenger_survived_Y\n",
       "0                     0\n",
       "1                     1\n",
       "2                     1\n",
       "3                     1\n",
       "4                     0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Realizar one hot encoding en columnas: \"Enbarked\", \"passenger_class\", \"passenger_sex\" y \"passenger_survived\"\n",
    "caracteristicasConOhe = caracteristicas.join(pd.get_dummies(caracteristicas.Embarked, prefix='Embarked'))\n",
    "caracteristicasConOhe = caracteristicasConOhe.join(pd.get_dummies(caracteristicasConOhe.passenger_class, prefix='passenger_class'))\n",
    "caracteristicasConOhe = caracteristicasConOhe.join(\\\n",
    "                                pd.get_dummies(caracteristicasConOhe.passenger_sex, prefix='passenger_sex', drop_first=True))\n",
    "caracteristicasConOhe = caracteristicasConOhe.loc[:, ~caracteristicasConOhe.columns.isin(['Embarked', 'passenger_class', 'passenger_sex'])]\n",
    "etiquetasConOhe = pd.get_dummies(etiquetas, prefix='passenger_survived', drop_first=True)\n",
    "display(caracteristicasConOhe.head())\n",
    "display(etiquetasConOhe.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parliamentary-characterization",
   "metadata": {},
   "source": [
    "## División entre datos de entrenamiento y datos de validación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "apparent-generator",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de datos de entrenamiento: 569\n",
      "Número de datos de pruebas: 143\n",
      "Número de datos de validación final: 179\n"
     ]
    }
   ],
   "source": [
    "# Primera división 80-20 entre el set de entrenamiento completo y datos de validación final.\n",
    "caracteristicasConOhe_completeTrain, caracteristicasConOhe_finalTest, \\\n",
    "    etiquetasConOhe_completeTrain, etiquetasConOhe_finalTest = \\\n",
    "    train_test_split(caracteristicasConOhe, etiquetasConOhe, test_size=0.2, random_state=2022)\n",
    "\n",
    "# Segunda división 80-20 entre set de entrenamiento y set de pruebas\n",
    "caracteristicasConOhe_train, caracteristicasConOhe_test, \\\n",
    "    etiquetasConOhe_train, etiquetasConOhe_test = \\\n",
    "    train_test_split(caracteristicasConOhe_completeTrain, etiquetasConOhe_completeTrain, test_size=0.2, random_state=2022)\n",
    "\n",
    "print('Número de datos de entrenamiento:', len(caracteristicasConOhe_train))\n",
    "print('Número de datos de pruebas:', len(caracteristicasConOhe_test))\n",
    "print('Número de datos de validación final:', len(caracteristicasConOhe_finalTest))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "linear-congo",
   "metadata": {},
   "source": [
    "### Escalar características numéricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "attractive-matrix",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se aplicará estandarización de datos como método de escalado\n",
    "def estandarizar(x, mediaEntrenamiento = None, desviacionEntrenamiento = None):\n",
    "    if(mediaEntrenamiento == None or desviacionEntrenamiento == None):\n",
    "        # Si no se reciben datos de entrenamiento, se calculan de los features \"x\" recibidos\n",
    "        media = np.mean(x)\n",
    "        desviacion = np.std(x)\n",
    "    else:\n",
    "        media = mediaEntrenamiento\n",
    "        desviacion = desviacionEntrenamiento\n",
    "    \n",
    "    return (x - media) / (desviacion), media, desviacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "middle-proportion",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked_C</th>\n",
       "      <th>Embarked_Q</th>\n",
       "      <th>Embarked_S</th>\n",
       "      <th>passenger_class_Lower</th>\n",
       "      <th>passenger_class_Middle</th>\n",
       "      <th>passenger_class_Upper</th>\n",
       "      <th>passenger_sex_M</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>357</th>\n",
       "      <td>0.689198</td>\n",
       "      <td>-0.474164</td>\n",
       "      <td>-0.485553</td>\n",
       "      <td>-0.392896</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>613</th>\n",
       "      <td>0.030567</td>\n",
       "      <td>-0.474164</td>\n",
       "      <td>-0.485553</td>\n",
       "      <td>-0.496634</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>868</th>\n",
       "      <td>0.030567</td>\n",
       "      <td>-0.474164</td>\n",
       "      <td>-0.485553</td>\n",
       "      <td>-0.462055</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>1.165266</td>\n",
       "      <td>-0.474164</td>\n",
       "      <td>-0.485553</td>\n",
       "      <td>-0.493176</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>863</th>\n",
       "      <td>0.030567</td>\n",
       "      <td>6.443769</td>\n",
       "      <td>2.003451</td>\n",
       "      <td>0.724512</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Age     SibSp     Parch      Fare  Embarked_C  Embarked_Q  \\\n",
       "357  0.689198 -0.474164 -0.485553 -0.392896           0           0   \n",
       "613  0.030567 -0.474164 -0.485553 -0.496634           0           1   \n",
       "868  0.030567 -0.474164 -0.485553 -0.462055           0           0   \n",
       "414  1.165266 -0.474164 -0.485553 -0.493176           0           0   \n",
       "863  0.030567  6.443769  2.003451  0.724512           0           0   \n",
       "\n",
       "     Embarked_S  passenger_class_Lower  passenger_class_Middle  \\\n",
       "357           1                      0                       1   \n",
       "613           0                      1                       0   \n",
       "868           1                      1                       0   \n",
       "414           1                      1                       0   \n",
       "863           1                      1                       0   \n",
       "\n",
       "     passenger_class_Upper  passenger_sex_M  \n",
       "357                      0                0  \n",
       "613                      0                1  \n",
       "868                      0                1  \n",
       "414                      0                1  \n",
       "863                      0                0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked_C</th>\n",
       "      <th>Embarked_Q</th>\n",
       "      <th>Embarked_S</th>\n",
       "      <th>passenger_class_Lower</th>\n",
       "      <th>passenger_class_Middle</th>\n",
       "      <th>passenger_class_Upper</th>\n",
       "      <th>passenger_sex_M</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>-0.342283</td>\n",
       "      <td>-0.474164</td>\n",
       "      <td>-0.485553</td>\n",
       "      <td>-0.498610</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>620</th>\n",
       "      <td>-0.183593</td>\n",
       "      <td>0.390578</td>\n",
       "      <td>-0.485553</td>\n",
       "      <td>-0.364161</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>562</th>\n",
       "      <td>-0.104249</td>\n",
       "      <td>-0.474164</td>\n",
       "      <td>-0.485553</td>\n",
       "      <td>-0.383016</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>1.244611</td>\n",
       "      <td>-0.474164</td>\n",
       "      <td>-0.485553</td>\n",
       "      <td>-0.511948</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>631</th>\n",
       "      <td>1.720679</td>\n",
       "      <td>-0.474164</td>\n",
       "      <td>-0.485553</td>\n",
       "      <td>-0.510383</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Age     SibSp     Parch      Fare  Embarked_C  Embarked_Q  \\\n",
       "75  -0.342283 -0.474164 -0.485553 -0.498610           0           0   \n",
       "620 -0.183593  0.390578 -0.485553 -0.364161           1           0   \n",
       "562 -0.104249 -0.474164 -0.485553 -0.383016           0           0   \n",
       "129  1.244611 -0.474164 -0.485553 -0.511948           0           0   \n",
       "631  1.720679 -0.474164 -0.485553 -0.510383           0           0   \n",
       "\n",
       "     Embarked_S  passenger_class_Lower  passenger_class_Middle  \\\n",
       "75            1                      1                       0   \n",
       "620           0                      1                       0   \n",
       "562           1                      0                       1   \n",
       "129           1                      1                       0   \n",
       "631           1                      1                       0   \n",
       "\n",
       "     passenger_class_Upper  passenger_sex_M  \n",
       "75                       0                1  \n",
       "620                      0                1  \n",
       "562                      0                1  \n",
       "129                      0                1  \n",
       "631                      0                1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Aplicar estandarización a columnas \"Age\", \"SibSp\", \"Parch\" y \"Fare\"\n",
    "caracteristicasConOheEstandarizadas = caracteristicasConOhe_train.join(pd.DataFrame())\n",
    "caracteristicasConOheEstandarizadas['Age'], mediaAge_train, desviacionAge_train = \\\n",
    "            estandarizar(caracteristicasConOheEstandarizadas['Age'])\n",
    "caracteristicasConOheEstandarizadas['SibSp'], mediaSibSp_train, desviacionSibSp_train = \\\n",
    "            estandarizar(caracteristicasConOheEstandarizadas['SibSp'])\n",
    "caracteristicasConOheEstandarizadas['Parch'], mediaParch_train, desviacionParch_train = \\\n",
    "            estandarizar(caracteristicasConOheEstandarizadas['Parch'])\n",
    "caracteristicasConOheEstandarizadas['Fare'], mediaFare_train, desviacionFare_train = \\\n",
    "            estandarizar(caracteristicasConOheEstandarizadas['Fare'])\n",
    "\n",
    "# Aplicar estandarización a datos de prueba\n",
    "caracteristicasConOheEstandarizadas_test = caracteristicasConOhe_test.join(pd.DataFrame())\n",
    "caracteristicasConOheEstandarizadas_test['Age'], _, _ = \\\n",
    "            estandarizar(caracteristicasConOheEstandarizadas_test['Age'], mediaAge_train, desviacionAge_train)\n",
    "caracteristicasConOheEstandarizadas_test['SibSp'], _, _ = \\\n",
    "            estandarizar(caracteristicasConOheEstandarizadas_test['SibSp'], mediaSibSp_train, desviacionSibSp_train)\n",
    "caracteristicasConOheEstandarizadas_test['Parch'], _, _ = \\\n",
    "            estandarizar(caracteristicasConOheEstandarizadas_test['Parch'], mediaParch_train, desviacionParch_train)\n",
    "caracteristicasConOheEstandarizadas_test['Fare'], _, _ = \\\n",
    "            estandarizar(caracteristicasConOheEstandarizadas_test['Fare'], mediaFare_train, desviacionFare_train)\n",
    "\n",
    "display(caracteristicasConOheEstandarizadas.head())\n",
    "display(caracteristicasConOheEstandarizadas_test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "helpful-hacker",
   "metadata": {},
   "source": [
    "## Guardar datos para validación final\n",
    "\n",
    "Los datos _\"caracteristicasConOhe_finalTest\"_ serán guardados en un archivo con nombre: \"datos_validacion.csv\" y serán usados en segundo notebook para validación de función de predicción final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 875,
   "id": "biological-spray",
   "metadata": {},
   "outputs": [],
   "source": [
    "datosDeValidacion = df.filter(items = caracteristicasConOhe_finalTest.index, axis=0)\n",
    "datosDeValidacion.to_csv('datos_validacion.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excellent-slovak",
   "metadata": {},
   "source": [
    "## Guardar media y desviación estándar de datos de entrenamiento\n",
    "\n",
    "Estos datos serán utilizados para estandarizar los datos de validación final en el segundo notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 877,
   "id": "pacific-georgia",
   "metadata": {},
   "outputs": [],
   "source": [
    "informacionEstadisticaEntrenamiento = dict()\n",
    "informacionEstadisticaEntrenamiento['mediaAge_train'] = mediaAge_train\n",
    "informacionEstadisticaEntrenamiento['desviacionAge_train'] = desviacionAge_train\n",
    "informacionEstadisticaEntrenamiento['mediaSibSp_train'] = mediaSibSp_train\n",
    "informacionEstadisticaEntrenamiento['desviacionSibSp_train'] = desviacionSibSp_train\n",
    "informacionEstadisticaEntrenamiento['mediaParch_train'] = mediaParch_train\n",
    "informacionEstadisticaEntrenamiento['desviacionParch_train'] = desviacionParch_train\n",
    "informacionEstadisticaEntrenamiento['mediaFare_train'] = mediaFare_train\n",
    "informacionEstadisticaEntrenamiento['desviacionFare_train'] = desviacionFare_train\n",
    "np.save('modelos/InformacionEstadistica', [informacionEstadisticaEntrenamiento])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southwest-christopher",
   "metadata": {},
   "source": [
    "## Árbol de decisión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "durable-crawford",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para crear un nuevo dataframe vacío con las columnas esperadas en bitácora\n",
    "def nuevoDataframeParaBitacora():\n",
    "    return pd.DataFrame({'tipomodelo': [], 'fecha': [], 'configuracion': [], 'error_train': [],\\\n",
    "                        'accuracy_train': [], 'precision_train': [], 'recall_train': [], 'f1_score_train': [],\\\n",
    "                        'accuracy_test': [], 'precision_test': [], 'recall_test': [], 'f1_score_test': []})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "id": "compound-novel",
   "metadata": {},
   "outputs": [],
   "source": [
    "nombreBitacora = 'bitacora_modelos.csv'\n",
    "# Función que determinará si una configuración de un experimento en particular ya se encuentra en la bitácora\n",
    "def existeConfiguracion(configuracion):\n",
    "    # Si existe bitácora, buscar si la configuración ya existe\n",
    "    if os.path.exists(nombreBitacora):\n",
    "        dfBitacora = pd.read_csv(nombreBitacora)\n",
    "        if(not dfBitacora['configuracion'].where(dfBitacora['configuracion'] == configuracion).isna().all()):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# Función que agregará una configuración de un experimento a la bitácora con sus métricas más importantes\n",
    "def manejarBitacora(tipoModelo, configuracion, y_train, y_pred_train, y_test, y_pred_test, errorEntrenamiento=''):\n",
    "    dfNewConfiguracion = nuevoDataframeParaBitacora()\n",
    "    dataConfiguracion = [tipoModelo, dt.datetime.now(), configuracion, errorEntrenamiento]\n",
    "    dataConfiguracion.append(mts.accuracy_score(y_train, y_pred_train))\n",
    "    dataConfiguracion.append(mts.precision_score(y_train, y_pred_train))\n",
    "    dataConfiguracion.append(mts.recall_score(y_train, y_pred_train))\n",
    "    dataConfiguracion.append(mts.f1_score(y_train, y_pred_train))\n",
    "    dataConfiguracion.append(mts.accuracy_score(y_test, y_pred_test))\n",
    "    dataConfiguracion.append(mts.precision_score(y_test, y_pred_test))\n",
    "    dataConfiguracion.append(mts.recall_score(y_test, y_pred_test))\n",
    "    dataConfiguracion.append(mts.f1_score(y_test, y_pred_test))\n",
    "    dfNewConfiguracion.loc[len(dfNewConfiguracion)] = dataConfiguracion\n",
    "\n",
    "    # Agregar configuración en bitácora\n",
    "    if os.path.exists(nombreBitacora):\n",
    "        dfNewConfiguracion.to_csv(nombreBitacora, mode='a', index=False, header=False)\n",
    "    else:\n",
    "        dfNewConfiguracion.to_csv(nombreBitacora, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "robust-acquisition",
   "metadata": {},
   "outputs": [],
   "source": [
    "tipoArbolDecision = 'ArbolDecision'\n",
    "# Función para la creación de modelo de árbol de decisión y registro de métricas en bitácora (en caso de no existir en la bitácora)\n",
    "def crearEvaluarArbolDecision(X, y, X_test, y_test, criterio='gini', profundidadMaxima=None):\n",
    "    # Crear modelo con parámetros enviados\n",
    "    modelo = DecisionTreeClassifier(criterion=criterio, max_depth=profundidadMaxima, random_state=2022)\n",
    "    modelo.fit(X, y)\n",
    "    # Crear cadena de configuración de árbol de decisión\n",
    "    configuracion = tipoArbolDecision + '_criterio=' + criterio + '_profundidadMaxima=' + str(profundidadMaxima)\n",
    "    configuracionYaExiste = existeConfiguracion(configuracion)\n",
    "        \n",
    "    # Si no existe configuración en bitácora, agregar configuración a bitácora y métricas    \n",
    "    if(not configuracionYaExiste):\n",
    "        # Calcular errores de predicción para entrenamiento y test\n",
    "        y_pred_train = modelo.predict(X)\n",
    "        y_pred_test = modelo.predict(X_test)\n",
    "        manejarBitacora(tipoArbolDecision, configuracion, y, y_pred_train, y_test, y_pred_test)\n",
    "\n",
    "    return modelo, configuracion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "resident-married",
   "metadata": {},
   "source": [
    "### Creación de diferentes árboles de decisión y análisis de métricas en bitácora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "anonymous-promotion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hiper-parámetros a experimentar\n",
    "criterios = ['gini', 'entropy']\n",
    "maxProfundidades = [None, 10, 50, 100]\n",
    "\n",
    "for criterio in criterios:\n",
    "    for maxProfundidad in maxProfundidades:\n",
    "        crearEvaluarArbolDecision(caracteristicasConOheEstandarizadas, etiquetasConOhe_train, \\\n",
    "                          caracteristicasConOheEstandarizadas_test, etiquetasConOhe_test, criterio, maxProfundidad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 835,
   "id": "static-jacksonville",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mostrarResultadosBitacora(tipoModelo):\n",
    "    dfBitacora = pd.read_csv(nombreBitacora)\n",
    "    dfBitacoraFiltrada = dfBitacora[dfBitacora['tipomodelo'] == tipoModelo]\n",
    "    pd.set_option(\"display.max_colwidth\", 1000)\n",
    "    display(dfBitacoraFiltrada[['configuracion', 'accuracy_train', 'accuracy_test', 'f1_score_train', 'f1_score_test']]\\\n",
    "            .sort_values(by='accuracy_test', ascending=False).head(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conscious-excess",
   "metadata": {},
   "source": [
    "La bitácora señala que los mejores resultados en __accuracy__ para los datos de prueba se obtuvieron con el criterio 'entropy' y con menor profundidad. Cómo __accuracy__ en los datos de pruebas aún es menor al solicitado (80%) se probarán más hiperparámetros hasta alcanzar al menos un __accuracy__ de 80%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "similar-riding",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>configuracion</th>\n",
       "      <th>accuracy_train</th>\n",
       "      <th>accuracy_test</th>\n",
       "      <th>f1_score_train</th>\n",
       "      <th>f1_score_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ArbolDecision_criterio=gini_profundidadMaxima=5</td>\n",
       "      <td>0.861160</td>\n",
       "      <td>0.825175</td>\n",
       "      <td>0.791557</td>\n",
       "      <td>0.778761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ArbolDecision_criterio=entropy_profundidadMaxima=3</td>\n",
       "      <td>0.833040</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.767726</td>\n",
       "      <td>0.790323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ArbolDecision_criterio=gini_profundidadMaxima=3</td>\n",
       "      <td>0.833040</td>\n",
       "      <td>0.804196</td>\n",
       "      <td>0.766585</td>\n",
       "      <td>0.770492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>ArbolDecision_criterio=entropy_profundidadMaxima=5</td>\n",
       "      <td>0.850615</td>\n",
       "      <td>0.797203</td>\n",
       "      <td>0.789082</td>\n",
       "      <td>0.760331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ArbolDecision_criterio=entropy_profundidadMaxima=9</td>\n",
       "      <td>0.894552</td>\n",
       "      <td>0.797203</td>\n",
       "      <td>0.839572</td>\n",
       "      <td>0.747826</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         configuracion  accuracy_train  \\\n",
       "9      ArbolDecision_criterio=gini_profundidadMaxima=5        0.861160   \n",
       "11  ArbolDecision_criterio=entropy_profundidadMaxima=3        0.833040   \n",
       "8      ArbolDecision_criterio=gini_profundidadMaxima=3        0.833040   \n",
       "12  ArbolDecision_criterio=entropy_profundidadMaxima=5        0.850615   \n",
       "13  ArbolDecision_criterio=entropy_profundidadMaxima=9        0.894552   \n",
       "\n",
       "    accuracy_test  f1_score_train  f1_score_test  \n",
       "9        0.825175        0.791557       0.778761  \n",
       "11       0.818182        0.767726       0.790323  \n",
       "8        0.804196        0.766585       0.770492  \n",
       "12       0.797203        0.789082       0.760331  \n",
       "13       0.797203        0.839572       0.747826  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Hiper-parámetros a experimentar\n",
    "criterios = ['gini', 'entropy']\n",
    "maxProfundidades = [3, 5, 9]\n",
    "\n",
    "for criterio in criterios:\n",
    "    for maxProfundidad in maxProfundidades:\n",
    "        crearEvaluarArbolDecision(caracteristicasConOheEstandarizadas, etiquetasConOhe_train, \\\n",
    "                          caracteristicasConOheEstandarizadas_test, etiquetasConOhe_test, criterio, maxProfundidad)\n",
    "        \n",
    "mostrarResultadosBitacora(tipoArbolDecision)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advisory-accordance",
   "metadata": {},
   "source": [
    "Los nuevos resultados señalan que una menor profundidad claramente mejora la exactitud. El criterio (gini o entropy) no parece afectar claramente en la mejora de exactitud. Se realizarán más pruebas con la profundidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "behavioral-judge",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>configuracion</th>\n",
       "      <th>accuracy_train</th>\n",
       "      <th>accuracy_test</th>\n",
       "      <th>f1_score_train</th>\n",
       "      <th>f1_score_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ArbolDecision_criterio=gini_profundidadMaxima=4</td>\n",
       "      <td>0.847100</td>\n",
       "      <td>0.832168</td>\n",
       "      <td>0.789346</td>\n",
       "      <td>0.809524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ArbolDecision_criterio=gini_profundidadMaxima=5</td>\n",
       "      <td>0.861160</td>\n",
       "      <td>0.825175</td>\n",
       "      <td>0.791557</td>\n",
       "      <td>0.778761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>ArbolDecision_criterio=entropy_profundidadMaxima=6</td>\n",
       "      <td>0.869947</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.802139</td>\n",
       "      <td>0.771930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>ArbolDecision_criterio=entropy_profundidadMaxima=4</td>\n",
       "      <td>0.841828</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.776119</td>\n",
       "      <td>0.790323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ArbolDecision_criterio=entropy_profundidadMaxima=3</td>\n",
       "      <td>0.833040</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.767726</td>\n",
       "      <td>0.790323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>ArbolDecision_criterio=entropy_profundidadMaxima=8</td>\n",
       "      <td>0.882250</td>\n",
       "      <td>0.811189</td>\n",
       "      <td>0.820375</td>\n",
       "      <td>0.765217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>ArbolDecision_criterio=entropy_profundidadMaxima=7</td>\n",
       "      <td>0.880492</td>\n",
       "      <td>0.811189</td>\n",
       "      <td>0.820106</td>\n",
       "      <td>0.765217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ArbolDecision_criterio=gini_profundidadMaxima=3</td>\n",
       "      <td>0.833040</td>\n",
       "      <td>0.804196</td>\n",
       "      <td>0.766585</td>\n",
       "      <td>0.770492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ArbolDecision_criterio=entropy_profundidadMaxima=9</td>\n",
       "      <td>0.894552</td>\n",
       "      <td>0.797203</td>\n",
       "      <td>0.839572</td>\n",
       "      <td>0.747826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>ArbolDecision_criterio=entropy_profundidadMaxima=5</td>\n",
       "      <td>0.850615</td>\n",
       "      <td>0.797203</td>\n",
       "      <td>0.789082</td>\n",
       "      <td>0.760331</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         configuracion  accuracy_train  \\\n",
       "15     ArbolDecision_criterio=gini_profundidadMaxima=4        0.847100   \n",
       "9      ArbolDecision_criterio=gini_profundidadMaxima=5        0.861160   \n",
       "21  ArbolDecision_criterio=entropy_profundidadMaxima=6        0.869947   \n",
       "20  ArbolDecision_criterio=entropy_profundidadMaxima=4        0.841828   \n",
       "11  ArbolDecision_criterio=entropy_profundidadMaxima=3        0.833040   \n",
       "23  ArbolDecision_criterio=entropy_profundidadMaxima=8        0.882250   \n",
       "22  ArbolDecision_criterio=entropy_profundidadMaxima=7        0.880492   \n",
       "8      ArbolDecision_criterio=gini_profundidadMaxima=3        0.833040   \n",
       "13  ArbolDecision_criterio=entropy_profundidadMaxima=9        0.894552   \n",
       "12  ArbolDecision_criterio=entropy_profundidadMaxima=5        0.850615   \n",
       "\n",
       "    accuracy_test  f1_score_train  f1_score_test  \n",
       "15       0.832168        0.789346       0.809524  \n",
       "9        0.825175        0.791557       0.778761  \n",
       "21       0.818182        0.802139       0.771930  \n",
       "20       0.818182        0.776119       0.790323  \n",
       "11       0.818182        0.767726       0.790323  \n",
       "23       0.811189        0.820375       0.765217  \n",
       "22       0.811189        0.820106       0.765217  \n",
       "8        0.804196        0.766585       0.770492  \n",
       "13       0.797203        0.839572       0.747826  \n",
       "12       0.797203        0.789082       0.760331  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Hiper-parámetros a experimentar\n",
    "criterios = ['gini', 'entropy']\n",
    "maxProfundidades = [2, 4, 6, 7, 8]\n",
    "\n",
    "for criterio in criterios:\n",
    "    for maxProfundidad in maxProfundidades:\n",
    "        crearEvaluarArbolDecision(caracteristicasConOheEstandarizadas, etiquetasConOhe_train, \\\n",
    "                          caracteristicasConOheEstandarizadas_test, etiquetasConOhe_test, criterio, maxProfundidad)\n",
    "        \n",
    "mostrarResultadosBitacora(tipoArbolDecision)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bizarre-yacht",
   "metadata": {},
   "source": [
    "Después de las pruebas realizadas, se elige un árbol de decisión con criterio __gini__ y profundidad máxima de __4__, pues reporta las mejores métricas de exactitud para los datos de prueba y una exactitud aceptable en los datos de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "activated-heather",
   "metadata": {},
   "outputs": [],
   "source": [
    "modeloArbolDecision, configuracionArbolDecision = crearEvaluarArbolDecision(caracteristicasConOheEstandarizadas, etiquetasConOhe_train, \\\n",
    "                          caracteristicasConOheEstandarizadas_test, etiquetasConOhe_test, 'gini', 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electric-tournament",
   "metadata": {},
   "source": [
    "### Guardar modelo de árbol de decisión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "considered-instruction",
   "metadata": {},
   "outputs": [],
   "source": [
    "def guardarModelo(modelo, configuracion):\n",
    "    directorioModelos = 'modelos/'\n",
    "    extensionModelos = '.modelo'\n",
    "    joblib.dump(modelo, directorioModelos + configuracion + extensionModelos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optimum-preservation",
   "metadata": {},
   "outputs": [],
   "source": [
    "guardarModelo(modeloArbolDecision, configuracionArbolDecision)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "authorized-context",
   "metadata": {},
   "source": [
    "## Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "qualified-peoples",
   "metadata": {},
   "outputs": [],
   "source": [
    "tipoSVM = 'SVM'\n",
    "# Función para la creación de modelo de support vector machine y registro de métricas en bitácora (en caso de no existir en la bitácora)\n",
    "def crearEvaluarSVM(X, y, X_test, y_test, regularizacionC=1.0, kernel='rbf', grado=3):\n",
    "    # Crear modelo con parámetros enviados\n",
    "    modelo = SVC(C=regularizacionC, kernel=kernel, degree=grado, random_state=2022)\n",
    "    modelo.fit(X, y.values.ravel())\n",
    "    # Crear cadena de configuración de árbol de decisión\n",
    "    configuracion = tipoSVM + '_regularizacionC=' + str(regularizacionC) + '_kernel=' + kernel\n",
    "    if(kernel == 'poly'):\n",
    "        configuracion += '_grado=' + str(grado)\n",
    "    configuracionYaExiste = existeConfiguracion(configuracion)\n",
    "\n",
    "    # Si no existe configuración en bitácora, agregar configuración a bitácora y métricas    \n",
    "    if(not configuracionYaExiste):\n",
    "        # Calcular errores de predicción para entrenamiento y test\n",
    "        y_pred_train = modelo.predict(X)\n",
    "        y_pred_test = modelo.predict(X_test)\n",
    "        manejarBitacora(tipoSVM, configuracion, y, y_pred_train, y_test, y_pred_test)\n",
    "\n",
    "    return modelo, configuracion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "horizontal-perspective",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>configuracion</th>\n",
       "      <th>accuracy_train</th>\n",
       "      <th>accuracy_test</th>\n",
       "      <th>f1_score_train</th>\n",
       "      <th>f1_score_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>SVM_regularizacionC=1.0_kernel=poly_grado=3</td>\n",
       "      <td>0.850615</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.793187</td>\n",
       "      <td>0.793651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>SVM_regularizacionC=1.5_kernel=poly_grado=3</td>\n",
       "      <td>0.852373</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.796117</td>\n",
       "      <td>0.793651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>SVM_regularizacionC=5_kernel=poly_grado=3</td>\n",
       "      <td>0.866432</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.803109</td>\n",
       "      <td>0.779661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>SVM_regularizacionC=0.25_kernel=rbf</td>\n",
       "      <td>0.834798</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.770732</td>\n",
       "      <td>0.790323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>SVM_regularizacionC=5_kernel=poly_grado=2</td>\n",
       "      <td>0.847100</td>\n",
       "      <td>0.811189</td>\n",
       "      <td>0.776350</td>\n",
       "      <td>0.773109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>SVM_regularizacionC=4.0_kernel=rbf</td>\n",
       "      <td>0.868190</td>\n",
       "      <td>0.811189</td>\n",
       "      <td>0.804178</td>\n",
       "      <td>0.765217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>SVM_regularizacionC=0.5_kernel=poly_grado=2</td>\n",
       "      <td>0.836555</td>\n",
       "      <td>0.811189</td>\n",
       "      <td>0.772616</td>\n",
       "      <td>0.784000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>SVM_regularizacionC=10_kernel=poly_grado=2</td>\n",
       "      <td>0.848858</td>\n",
       "      <td>0.811189</td>\n",
       "      <td>0.774869</td>\n",
       "      <td>0.756757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>SVM_regularizacionC=0.7_kernel=rbf</td>\n",
       "      <td>0.845343</td>\n",
       "      <td>0.804196</td>\n",
       "      <td>0.784314</td>\n",
       "      <td>0.774194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>SVM_regularizacionC=0.4_kernel=rbf</td>\n",
       "      <td>0.836555</td>\n",
       "      <td>0.804196</td>\n",
       "      <td>0.773723</td>\n",
       "      <td>0.774194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>SVM_regularizacionC=1.5_kernel=rbf</td>\n",
       "      <td>0.852373</td>\n",
       "      <td>0.804196</td>\n",
       "      <td>0.794118</td>\n",
       "      <td>0.774194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>SVM_regularizacionC=0.5_kernel=poly_grado=3</td>\n",
       "      <td>0.852373</td>\n",
       "      <td>0.804196</td>\n",
       "      <td>0.796117</td>\n",
       "      <td>0.777778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>SVM_regularizacionC=1.0_kernel=rbf</td>\n",
       "      <td>0.847100</td>\n",
       "      <td>0.804196</td>\n",
       "      <td>0.787286</td>\n",
       "      <td>0.774194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>SVM_regularizacionC=10_kernel=poly_grado=3</td>\n",
       "      <td>0.876977</td>\n",
       "      <td>0.804196</td>\n",
       "      <td>0.818653</td>\n",
       "      <td>0.758621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>SVM_regularizacionC=1.5_kernel=poly_grado=2</td>\n",
       "      <td>0.836555</td>\n",
       "      <td>0.804196</td>\n",
       "      <td>0.771499</td>\n",
       "      <td>0.774194</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  configuracion  accuracy_train  \\\n",
       "33  SVM_regularizacionC=1.0_kernel=poly_grado=3        0.850615   \n",
       "52  SVM_regularizacionC=1.5_kernel=poly_grado=3        0.852373   \n",
       "64    SVM_regularizacionC=5_kernel=poly_grado=3        0.866432   \n",
       "29          SVM_regularizacionC=0.25_kernel=rbf        0.834798   \n",
       "63    SVM_regularizacionC=5_kernel=poly_grado=2        0.847100   \n",
       "43           SVM_regularizacionC=4.0_kernel=rbf        0.868190   \n",
       "45  SVM_regularizacionC=0.5_kernel=poly_grado=2        0.836555   \n",
       "67   SVM_regularizacionC=10_kernel=poly_grado=2        0.848858   \n",
       "58           SVM_regularizacionC=0.7_kernel=rbf        0.845343   \n",
       "57           SVM_regularizacionC=0.4_kernel=rbf        0.836555   \n",
       "54           SVM_regularizacionC=1.5_kernel=rbf        0.852373   \n",
       "46  SVM_regularizacionC=0.5_kernel=poly_grado=3        0.852373   \n",
       "36           SVM_regularizacionC=1.0_kernel=rbf        0.847100   \n",
       "68   SVM_regularizacionC=10_kernel=poly_grado=3        0.876977   \n",
       "51  SVM_regularizacionC=1.5_kernel=poly_grado=2        0.836555   \n",
       "\n",
       "    accuracy_test  f1_score_train  f1_score_test  \n",
       "33       0.818182        0.793187       0.793651  \n",
       "52       0.818182        0.796117       0.793651  \n",
       "64       0.818182        0.803109       0.779661  \n",
       "29       0.818182        0.770732       0.790323  \n",
       "63       0.811189        0.776350       0.773109  \n",
       "43       0.811189        0.804178       0.765217  \n",
       "45       0.811189        0.772616       0.784000  \n",
       "67       0.811189        0.774869       0.756757  \n",
       "58       0.804196        0.784314       0.774194  \n",
       "57       0.804196        0.773723       0.774194  \n",
       "54       0.804196        0.794118       0.774194  \n",
       "46       0.804196        0.796117       0.777778  \n",
       "36       0.804196        0.787286       0.774194  \n",
       "68       0.804196        0.818653       0.758621  \n",
       "51       0.804196        0.771499       0.774194  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Hiper-parámetros a experimentar\n",
    "regularizacionesC = [0.25, 1.0, 4.0]\n",
    "kernels = ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "grados = [1,3,5,7]\n",
    "\n",
    "for regularizacionC in regularizacionesC:\n",
    "    for kernel in kernels:\n",
    "        if(kernel == 'poly'):\n",
    "            for grado in grados:\n",
    "                crearEvaluarSVM(caracteristicasConOheEstandarizadas, etiquetasConOhe_train, \\\n",
    "                          caracteristicasConOheEstandarizadas_test, etiquetasConOhe_test, regularizacionC, kernel, grado)\n",
    "        else:\n",
    "            crearEvaluarSVM(caracteristicasConOheEstandarizadas, etiquetasConOhe_train, \\\n",
    "                          caracteristicasConOheEstandarizadas_test, etiquetasConOhe_test, regularizacionC, kernel)\n",
    "        \n",
    "mostrarResultadosBitacora(tipoSVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "destroyed-checkout",
   "metadata": {},
   "source": [
    "Se observa que los mejores se obtuvieron con un kernel tipo __poly__ y __rbf__. Para un kernel de tipo __rbf__ parece que el coeficiente de regularización no impacta mucho en el poder de predicción; mientras que para un kernel de tipo __poly__ con grado __3__ parece ser que un valor cercano a __1.0__ es adecuado. Se explorarán más grados polinomiales cercanos a __3__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "finnish-petersburg",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>configuracion</th>\n",
       "      <th>accuracy_train</th>\n",
       "      <th>accuracy_test</th>\n",
       "      <th>f1_score_train</th>\n",
       "      <th>f1_score_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>SVM_regularizacionC=1.0_kernel=poly_grado=3</td>\n",
       "      <td>0.850615</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.793187</td>\n",
       "      <td>0.793651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>SVM_regularizacionC=1.5_kernel=poly_grado=3</td>\n",
       "      <td>0.852373</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.796117</td>\n",
       "      <td>0.793651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>SVM_regularizacionC=5_kernel=poly_grado=3</td>\n",
       "      <td>0.866432</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.803109</td>\n",
       "      <td>0.779661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>SVM_regularizacionC=0.25_kernel=rbf</td>\n",
       "      <td>0.834798</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.770732</td>\n",
       "      <td>0.790323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>SVM_regularizacionC=5_kernel=poly_grado=2</td>\n",
       "      <td>0.847100</td>\n",
       "      <td>0.811189</td>\n",
       "      <td>0.776350</td>\n",
       "      <td>0.773109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>SVM_regularizacionC=4.0_kernel=rbf</td>\n",
       "      <td>0.868190</td>\n",
       "      <td>0.811189</td>\n",
       "      <td>0.804178</td>\n",
       "      <td>0.765217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>SVM_regularizacionC=0.5_kernel=poly_grado=2</td>\n",
       "      <td>0.836555</td>\n",
       "      <td>0.811189</td>\n",
       "      <td>0.772616</td>\n",
       "      <td>0.784000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>SVM_regularizacionC=10_kernel=poly_grado=2</td>\n",
       "      <td>0.848858</td>\n",
       "      <td>0.811189</td>\n",
       "      <td>0.774869</td>\n",
       "      <td>0.756757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>SVM_regularizacionC=0.7_kernel=rbf</td>\n",
       "      <td>0.845343</td>\n",
       "      <td>0.804196</td>\n",
       "      <td>0.784314</td>\n",
       "      <td>0.774194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>SVM_regularizacionC=0.4_kernel=rbf</td>\n",
       "      <td>0.836555</td>\n",
       "      <td>0.804196</td>\n",
       "      <td>0.773723</td>\n",
       "      <td>0.774194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>SVM_regularizacionC=1.5_kernel=rbf</td>\n",
       "      <td>0.852373</td>\n",
       "      <td>0.804196</td>\n",
       "      <td>0.794118</td>\n",
       "      <td>0.774194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>SVM_regularizacionC=0.5_kernel=poly_grado=3</td>\n",
       "      <td>0.852373</td>\n",
       "      <td>0.804196</td>\n",
       "      <td>0.796117</td>\n",
       "      <td>0.777778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>SVM_regularizacionC=1.0_kernel=rbf</td>\n",
       "      <td>0.847100</td>\n",
       "      <td>0.804196</td>\n",
       "      <td>0.787286</td>\n",
       "      <td>0.774194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>SVM_regularizacionC=10_kernel=poly_grado=3</td>\n",
       "      <td>0.876977</td>\n",
       "      <td>0.804196</td>\n",
       "      <td>0.818653</td>\n",
       "      <td>0.758621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>SVM_regularizacionC=1.5_kernel=poly_grado=2</td>\n",
       "      <td>0.836555</td>\n",
       "      <td>0.804196</td>\n",
       "      <td>0.771499</td>\n",
       "      <td>0.774194</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  configuracion  accuracy_train  \\\n",
       "33  SVM_regularizacionC=1.0_kernel=poly_grado=3        0.850615   \n",
       "52  SVM_regularizacionC=1.5_kernel=poly_grado=3        0.852373   \n",
       "64    SVM_regularizacionC=5_kernel=poly_grado=3        0.866432   \n",
       "29          SVM_regularizacionC=0.25_kernel=rbf        0.834798   \n",
       "63    SVM_regularizacionC=5_kernel=poly_grado=2        0.847100   \n",
       "43           SVM_regularizacionC=4.0_kernel=rbf        0.868190   \n",
       "45  SVM_regularizacionC=0.5_kernel=poly_grado=2        0.836555   \n",
       "67   SVM_regularizacionC=10_kernel=poly_grado=2        0.848858   \n",
       "58           SVM_regularizacionC=0.7_kernel=rbf        0.845343   \n",
       "57           SVM_regularizacionC=0.4_kernel=rbf        0.836555   \n",
       "54           SVM_regularizacionC=1.5_kernel=rbf        0.852373   \n",
       "46  SVM_regularizacionC=0.5_kernel=poly_grado=3        0.852373   \n",
       "36           SVM_regularizacionC=1.0_kernel=rbf        0.847100   \n",
       "68   SVM_regularizacionC=10_kernel=poly_grado=3        0.876977   \n",
       "51  SVM_regularizacionC=1.5_kernel=poly_grado=2        0.836555   \n",
       "\n",
       "    accuracy_test  f1_score_train  f1_score_test  \n",
       "33       0.818182        0.793187       0.793651  \n",
       "52       0.818182        0.796117       0.793651  \n",
       "64       0.818182        0.803109       0.779661  \n",
       "29       0.818182        0.770732       0.790323  \n",
       "63       0.811189        0.776350       0.773109  \n",
       "43       0.811189        0.804178       0.765217  \n",
       "45       0.811189        0.772616       0.784000  \n",
       "67       0.811189        0.774869       0.756757  \n",
       "58       0.804196        0.784314       0.774194  \n",
       "57       0.804196        0.773723       0.774194  \n",
       "54       0.804196        0.794118       0.774194  \n",
       "46       0.804196        0.796117       0.777778  \n",
       "36       0.804196        0.787286       0.774194  \n",
       "68       0.804196        0.818653       0.758621  \n",
       "51       0.804196        0.771499       0.774194  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Hiper-parámetros a experimentar\n",
    "regularizacionesC = [0.5, 1.0, 1.5]\n",
    "kernels = ['poly', 'rbf']\n",
    "grados = [2,3,4]\n",
    "\n",
    "for regularizacionC in regularizacionesC:\n",
    "    for kernel in kernels:\n",
    "        if(kernel == 'poly'):\n",
    "            for grado in grados:\n",
    "                crearEvaluarSVM(caracteristicasConOheEstandarizadas, etiquetasConOhe_train, \\\n",
    "                          caracteristicasConOheEstandarizadas_test, etiquetasConOhe_test, regularizacionC, kernel, grado)\n",
    "        else:\n",
    "            crearEvaluarSVM(caracteristicasConOheEstandarizadas, etiquetasConOhe_train, \\\n",
    "                          caracteristicasConOheEstandarizadas_test, etiquetasConOhe_test, regularizacionC, kernel)\n",
    "        \n",
    "mostrarResultadosBitacora(tipoSVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "periodic-doubt",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>configuracion</th>\n",
       "      <th>accuracy_train</th>\n",
       "      <th>accuracy_test</th>\n",
       "      <th>f1_score_train</th>\n",
       "      <th>f1_score_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>SVM_regularizacionC=1.0_kernel=poly_grado=3</td>\n",
       "      <td>0.850615</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.793187</td>\n",
       "      <td>0.793651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>SVM_regularizacionC=1.5_kernel=poly_grado=3</td>\n",
       "      <td>0.852373</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.796117</td>\n",
       "      <td>0.793651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>SVM_regularizacionC=5_kernel=poly_grado=3</td>\n",
       "      <td>0.866432</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.803109</td>\n",
       "      <td>0.779661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>SVM_regularizacionC=0.25_kernel=rbf</td>\n",
       "      <td>0.834798</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.770732</td>\n",
       "      <td>0.790323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>SVM_regularizacionC=5_kernel=poly_grado=2</td>\n",
       "      <td>0.847100</td>\n",
       "      <td>0.811189</td>\n",
       "      <td>0.776350</td>\n",
       "      <td>0.773109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>SVM_regularizacionC=4.0_kernel=rbf</td>\n",
       "      <td>0.868190</td>\n",
       "      <td>0.811189</td>\n",
       "      <td>0.804178</td>\n",
       "      <td>0.765217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>SVM_regularizacionC=0.5_kernel=poly_grado=2</td>\n",
       "      <td>0.836555</td>\n",
       "      <td>0.811189</td>\n",
       "      <td>0.772616</td>\n",
       "      <td>0.784000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>SVM_regularizacionC=10_kernel=poly_grado=2</td>\n",
       "      <td>0.848858</td>\n",
       "      <td>0.811189</td>\n",
       "      <td>0.774869</td>\n",
       "      <td>0.756757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>SVM_regularizacionC=0.7_kernel=rbf</td>\n",
       "      <td>0.845343</td>\n",
       "      <td>0.804196</td>\n",
       "      <td>0.784314</td>\n",
       "      <td>0.774194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>SVM_regularizacionC=0.4_kernel=rbf</td>\n",
       "      <td>0.836555</td>\n",
       "      <td>0.804196</td>\n",
       "      <td>0.773723</td>\n",
       "      <td>0.774194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>SVM_regularizacionC=1.5_kernel=rbf</td>\n",
       "      <td>0.852373</td>\n",
       "      <td>0.804196</td>\n",
       "      <td>0.794118</td>\n",
       "      <td>0.774194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>SVM_regularizacionC=0.5_kernel=poly_grado=3</td>\n",
       "      <td>0.852373</td>\n",
       "      <td>0.804196</td>\n",
       "      <td>0.796117</td>\n",
       "      <td>0.777778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>SVM_regularizacionC=1.0_kernel=rbf</td>\n",
       "      <td>0.847100</td>\n",
       "      <td>0.804196</td>\n",
       "      <td>0.787286</td>\n",
       "      <td>0.774194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>SVM_regularizacionC=10_kernel=poly_grado=3</td>\n",
       "      <td>0.876977</td>\n",
       "      <td>0.804196</td>\n",
       "      <td>0.818653</td>\n",
       "      <td>0.758621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>SVM_regularizacionC=1.5_kernel=poly_grado=2</td>\n",
       "      <td>0.836555</td>\n",
       "      <td>0.804196</td>\n",
       "      <td>0.771499</td>\n",
       "      <td>0.774194</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  configuracion  accuracy_train  \\\n",
       "33  SVM_regularizacionC=1.0_kernel=poly_grado=3        0.850615   \n",
       "52  SVM_regularizacionC=1.5_kernel=poly_grado=3        0.852373   \n",
       "64    SVM_regularizacionC=5_kernel=poly_grado=3        0.866432   \n",
       "29          SVM_regularizacionC=0.25_kernel=rbf        0.834798   \n",
       "63    SVM_regularizacionC=5_kernel=poly_grado=2        0.847100   \n",
       "43           SVM_regularizacionC=4.0_kernel=rbf        0.868190   \n",
       "45  SVM_regularizacionC=0.5_kernel=poly_grado=2        0.836555   \n",
       "67   SVM_regularizacionC=10_kernel=poly_grado=2        0.848858   \n",
       "58           SVM_regularizacionC=0.7_kernel=rbf        0.845343   \n",
       "57           SVM_regularizacionC=0.4_kernel=rbf        0.836555   \n",
       "54           SVM_regularizacionC=1.5_kernel=rbf        0.852373   \n",
       "46  SVM_regularizacionC=0.5_kernel=poly_grado=3        0.852373   \n",
       "36           SVM_regularizacionC=1.0_kernel=rbf        0.847100   \n",
       "68   SVM_regularizacionC=10_kernel=poly_grado=3        0.876977   \n",
       "51  SVM_regularizacionC=1.5_kernel=poly_grado=2        0.836555   \n",
       "\n",
       "    accuracy_test  f1_score_train  f1_score_test  \n",
       "33       0.818182        0.793187       0.793651  \n",
       "52       0.818182        0.796117       0.793651  \n",
       "64       0.818182        0.803109       0.779661  \n",
       "29       0.818182        0.770732       0.790323  \n",
       "63       0.811189        0.776350       0.773109  \n",
       "43       0.811189        0.804178       0.765217  \n",
       "45       0.811189        0.772616       0.784000  \n",
       "67       0.811189        0.774869       0.756757  \n",
       "58       0.804196        0.784314       0.774194  \n",
       "57       0.804196        0.773723       0.774194  \n",
       "54       0.804196        0.794118       0.774194  \n",
       "46       0.804196        0.796117       0.777778  \n",
       "36       0.804196        0.787286       0.774194  \n",
       "68       0.804196        0.818653       0.758621  \n",
       "51       0.804196        0.771499       0.774194  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Explorar más opciones para rbf\n",
    "regularizacionesC = [0.05, 0.1, 0.4, 0.7]\n",
    "kernels = ['rbf']\n",
    "grados = []\n",
    "\n",
    "for regularizacionC in regularizacionesC:\n",
    "    for kernel in kernels:\n",
    "        if(kernel == 'poly'):\n",
    "            for grado in grados:\n",
    "                crearEvaluarSVM(caracteristicasConOheEstandarizadas, etiquetasConOhe_train, \\\n",
    "                          caracteristicasConOheEstandarizadas_test, etiquetasConOhe_test, regularizacionC, kernel, grado)\n",
    "        else:\n",
    "            crearEvaluarSVM(caracteristicasConOheEstandarizadas, etiquetasConOhe_train, \\\n",
    "                          caracteristicasConOheEstandarizadas_test, etiquetasConOhe_test, regularizacionC, kernel)\n",
    "        \n",
    "mostrarResultadosBitacora(tipoSVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "arranged-customs",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>configuracion</th>\n",
       "      <th>accuracy_train</th>\n",
       "      <th>accuracy_test</th>\n",
       "      <th>f1_score_train</th>\n",
       "      <th>f1_score_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>SVM_regularizacionC=1.0_kernel=poly_grado=3</td>\n",
       "      <td>0.850615</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.793187</td>\n",
       "      <td>0.793651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>SVM_regularizacionC=1.5_kernel=poly_grado=3</td>\n",
       "      <td>0.852373</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.796117</td>\n",
       "      <td>0.793651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>SVM_regularizacionC=5_kernel=poly_grado=3</td>\n",
       "      <td>0.866432</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.803109</td>\n",
       "      <td>0.779661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>SVM_regularizacionC=0.25_kernel=rbf</td>\n",
       "      <td>0.834798</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.770732</td>\n",
       "      <td>0.790323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>SVM_regularizacionC=5_kernel=poly_grado=2</td>\n",
       "      <td>0.847100</td>\n",
       "      <td>0.811189</td>\n",
       "      <td>0.776350</td>\n",
       "      <td>0.773109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>SVM_regularizacionC=4.0_kernel=rbf</td>\n",
       "      <td>0.868190</td>\n",
       "      <td>0.811189</td>\n",
       "      <td>0.804178</td>\n",
       "      <td>0.765217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>SVM_regularizacionC=0.5_kernel=poly_grado=2</td>\n",
       "      <td>0.836555</td>\n",
       "      <td>0.811189</td>\n",
       "      <td>0.772616</td>\n",
       "      <td>0.784000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>SVM_regularizacionC=10_kernel=poly_grado=2</td>\n",
       "      <td>0.848858</td>\n",
       "      <td>0.811189</td>\n",
       "      <td>0.774869</td>\n",
       "      <td>0.756757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>SVM_regularizacionC=0.7_kernel=rbf</td>\n",
       "      <td>0.845343</td>\n",
       "      <td>0.804196</td>\n",
       "      <td>0.784314</td>\n",
       "      <td>0.774194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>SVM_regularizacionC=0.4_kernel=rbf</td>\n",
       "      <td>0.836555</td>\n",
       "      <td>0.804196</td>\n",
       "      <td>0.773723</td>\n",
       "      <td>0.774194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>SVM_regularizacionC=1.5_kernel=rbf</td>\n",
       "      <td>0.852373</td>\n",
       "      <td>0.804196</td>\n",
       "      <td>0.794118</td>\n",
       "      <td>0.774194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>SVM_regularizacionC=0.5_kernel=poly_grado=3</td>\n",
       "      <td>0.852373</td>\n",
       "      <td>0.804196</td>\n",
       "      <td>0.796117</td>\n",
       "      <td>0.777778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>SVM_regularizacionC=1.0_kernel=rbf</td>\n",
       "      <td>0.847100</td>\n",
       "      <td>0.804196</td>\n",
       "      <td>0.787286</td>\n",
       "      <td>0.774194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>SVM_regularizacionC=10_kernel=poly_grado=3</td>\n",
       "      <td>0.876977</td>\n",
       "      <td>0.804196</td>\n",
       "      <td>0.818653</td>\n",
       "      <td>0.758621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>SVM_regularizacionC=1.5_kernel=poly_grado=2</td>\n",
       "      <td>0.836555</td>\n",
       "      <td>0.804196</td>\n",
       "      <td>0.771499</td>\n",
       "      <td>0.774194</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  configuracion  accuracy_train  \\\n",
       "33  SVM_regularizacionC=1.0_kernel=poly_grado=3        0.850615   \n",
       "52  SVM_regularizacionC=1.5_kernel=poly_grado=3        0.852373   \n",
       "64    SVM_regularizacionC=5_kernel=poly_grado=3        0.866432   \n",
       "29          SVM_regularizacionC=0.25_kernel=rbf        0.834798   \n",
       "63    SVM_regularizacionC=5_kernel=poly_grado=2        0.847100   \n",
       "43           SVM_regularizacionC=4.0_kernel=rbf        0.868190   \n",
       "45  SVM_regularizacionC=0.5_kernel=poly_grado=2        0.836555   \n",
       "67   SVM_regularizacionC=10_kernel=poly_grado=2        0.848858   \n",
       "58           SVM_regularizacionC=0.7_kernel=rbf        0.845343   \n",
       "57           SVM_regularizacionC=0.4_kernel=rbf        0.836555   \n",
       "54           SVM_regularizacionC=1.5_kernel=rbf        0.852373   \n",
       "46  SVM_regularizacionC=0.5_kernel=poly_grado=3        0.852373   \n",
       "36           SVM_regularizacionC=1.0_kernel=rbf        0.847100   \n",
       "68   SVM_regularizacionC=10_kernel=poly_grado=3        0.876977   \n",
       "51  SVM_regularizacionC=1.5_kernel=poly_grado=2        0.836555   \n",
       "\n",
       "    accuracy_test  f1_score_train  f1_score_test  \n",
       "33       0.818182        0.793187       0.793651  \n",
       "52       0.818182        0.796117       0.793651  \n",
       "64       0.818182        0.803109       0.779661  \n",
       "29       0.818182        0.770732       0.790323  \n",
       "63       0.811189        0.776350       0.773109  \n",
       "43       0.811189        0.804178       0.765217  \n",
       "45       0.811189        0.772616       0.784000  \n",
       "67       0.811189        0.774869       0.756757  \n",
       "58       0.804196        0.784314       0.774194  \n",
       "57       0.804196        0.773723       0.774194  \n",
       "54       0.804196        0.794118       0.774194  \n",
       "46       0.804196        0.796117       0.777778  \n",
       "36       0.804196        0.787286       0.774194  \n",
       "68       0.804196        0.818653       0.758621  \n",
       "51       0.804196        0.771499       0.774194  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Explorar más opciones para poly\n",
    "regularizacionesC = [0.1, 5, 10]\n",
    "kernels = ['poly']\n",
    "grados = [2,3,4,5]\n",
    "\n",
    "for regularizacionC in regularizacionesC:\n",
    "    for kernel in kernels:\n",
    "        if(kernel == 'poly'):\n",
    "            for grado in grados:\n",
    "                crearEvaluarSVM(caracteristicasConOheEstandarizadas, etiquetasConOhe_train, \\\n",
    "                          caracteristicasConOheEstandarizadas_test, etiquetasConOhe_test, regularizacionC, kernel, grado)\n",
    "        else:\n",
    "            crearEvaluarSVM(caracteristicasConOheEstandarizadas, etiquetasConOhe_train, \\\n",
    "                          caracteristicasConOheEstandarizadas_test, etiquetasConOhe_test, regularizacionC, kernel)\n",
    "        \n",
    "mostrarResultadosBitacora(tipoSVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daily-reunion",
   "metadata": {},
   "source": [
    "Después de las pruebas realizadas, se elige un modelo SVM con coeficiente C de regularización de __1.0__, kernel __poly__ de grado __3__, pues reporta las mejores métricas de exactitud para los datos de prueba y datos de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "electrical-football",
   "metadata": {},
   "outputs": [],
   "source": [
    "modeloSVM, configuracionSVM = crearEvaluarSVM(caracteristicasConOheEstandarizadas, etiquetasConOhe_train, \\\n",
    "                          caracteristicasConOheEstandarizadas_test, etiquetasConOhe_test, 1.0, 'poly', 3)\n",
    "guardarModelo(modeloSVM, configuracionSVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worst-accident",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 847,
   "id": "terminal-vacation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función que indicará si una feature en X es categórica o no\n",
    "def esCategorica(X, indexFeature):\n",
    "    if(type(X).__module__ != np.__name__):\n",
    "        X = np.array(X)\n",
    "        \n",
    "    if(len(np.unique(X[:,indexFeature])) < 11 and\\\n",
    "        len(np.unique(X[:,indexFeature]))/len(X[:,5]) < 0.4):\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 856,
   "id": "reported-animation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creará las probabilidades para todas las features categóricas y las organizará en un diccionario\n",
    "def crearProbabilidades(X,y):\n",
    "    if(type(X).__module__ != np.__name__):\n",
    "        X = np.array(X)\n",
    "    if(type(y).__module__ != np.__name__):\n",
    "        y = np.array(y)\n",
    "    \n",
    "    etiquetas = np.unique(y)\n",
    "    diccionarioProbabilidades = dict()\n",
    "    diccionarioProbabilidades['__labels__'] = etiquetas\n",
    "    # Para cada etiqueta guardar su probabilidad de ocurrencia\n",
    "    for etiqueta in etiquetas:\n",
    "        conteoConEtiqueta = np.count_nonzero(y == etiqueta)\n",
    "        diccionarioProbabilidades[str(etiqueta)] = conteoConEtiqueta / len(y)\n",
    "    \n",
    "    # Determinar para cada feature si es categórica\n",
    "    for indiceCaracteristica in range(X.shape[1]):\n",
    "        if(esCategorica(X, indiceCaracteristica)):\n",
    "            valores = np.unique(X[:,indiceCaracteristica])\n",
    "            # Para cada valor, determinar su probabilidad de ocurrencia\n",
    "            for valor in valores:\n",
    "                conteoConValor = np.count_nonzero(X[:,indiceCaracteristica] == valor)\n",
    "                diccionarioProbabilidades[str(indiceCaracteristica) + '_' \\\n",
    "                                          +str(round(valor,10))] = conteoConValor / len(X[:,indiceCaracteristica])\n",
    "    \n",
    "    # Determinar probabilidades condicionales de cada valor de los features dado los valores de resultados\n",
    "    for etiqueta in etiquetas:\n",
    "        indicesConEtiqueta = np.where(y == etiqueta)[0]\n",
    "        for indiceCaracteristica in range(X.shape[1]):\n",
    "            caracteristicaConEtiqueta = X[indicesConEtiqueta,indiceCaracteristica]\n",
    "            if(esCategorica(X, indiceCaracteristica)):\n",
    "                valores = np.unique(X[:,indiceCaracteristica])\n",
    "                # Para cada valor, determinar su probabilidad de ocurrencia\n",
    "                for valor in valores:\n",
    "                    conteoConValor = np.count_nonzero(caracteristicaConEtiqueta == valor)\n",
    "                    diccionarioProbabilidades[str(indiceCaracteristica) + '_' +str(round(valor,10)) + '|' + str(etiqueta)] = \\\n",
    "                        conteoConValor / len(caracteristicaConEtiqueta)\n",
    "            # Para características continuas, guardar media y desviación estándar de característica con la etiqueta específica\n",
    "            else:\n",
    "                diccionarioProbabilidades['media_'+ str(indiceCaracteristica) + '|' + str(etiqueta)] = np.mean(caracteristicaConEtiqueta)\n",
    "                diccionarioProbabilidades['desviacion_'+ str(indiceCaracteristica) + '|' + str(etiqueta)] = np.std(caracteristicaConEtiqueta)\n",
    "\n",
    "    return diccionarioProbabilidades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 857,
   "id": "alternate-program",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predecirNaiveBayes(diccionarioProbabilidades, X):\n",
    "    if(type(X).__module__ != np.__name__):\n",
    "        X = np.array(X)\n",
    "    etiquetas = diccionarioProbabilidades['__labels__']\n",
    "    y_pred = np.zeros(shape=(X.shape[0],1), dtype=np.float32)\n",
    "    for i in range(X.shape[0]):\n",
    "        resultados = np.ones_like(etiquetas, dtype=np.float32)\n",
    "        for indiceEtiqueta, etiqueta in enumerate(etiquetas):\n",
    "            resultados[indiceEtiqueta] = diccionarioProbabilidades[str(etiqueta)]\n",
    "            for indiceCaracteristica in range(X.shape[1]):\n",
    "                if(esCategorica(X, indiceCaracteristica)):\n",
    "                    if(str(indiceCaracteristica) + '_' + str(round(X[i,indiceCaracteristica], 10)) + '|' + str(etiqueta) \\\n",
    "                        in diccionarioProbabilidades.keys()):\n",
    "                        resultados[indiceEtiqueta] *= \\\n",
    "                            diccionarioProbabilidades[str(indiceCaracteristica) + '_' + \\\n",
    "                                                      str(round(X[i,indiceCaracteristica], 10)) + '|' + str(etiqueta)]\n",
    "                    else:\n",
    "                        resultados[indiceEtiqueta] = 0.0\n",
    "                        break\n",
    "                # Para características continuas se obtiene la probabilidad del valor (en un rango pequeño) para valor dado\n",
    "                else:\n",
    "                    if('media_' + str(indiceCaracteristica) + '|' + str(etiqueta) \\\n",
    "                        in diccionarioProbabilidades.keys()):\n",
    "                        margen = 0.001\n",
    "                        valorZ = (X[i,indiceCaracteristica] \\\n",
    "                                  - diccionarioProbabilidades['media_' + str(indiceCaracteristica) + '|' + str(etiqueta)])\\\n",
    "                                    / diccionarioProbabilidades['desviacion_' + str(indiceCaracteristica) + '|' + str(etiqueta)]\n",
    "                        probabilidad = st.norm.cdf(valorZ + margen) - st.norm.cdf(valorZ - margen)\n",
    "                        resultados[indiceEtiqueta] *= probabilidad\n",
    "\n",
    "        y_pred[i,0] = etiquetas[np.argmax(resultados)]\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 858,
   "id": "under-ownership",
   "metadata": {},
   "outputs": [],
   "source": [
    "tipoNaiveBayes = 'NaiveBayes'\n",
    "# Función para la creación de modelo de Naive Bayes\n",
    "def crearEvaluarNaiveBayes(X, y, X_test, y_test):\n",
    "    # Crear probabilidades con información enviada\n",
    "    diccionarioProbabilidades = crearProbabilidades(X, y)\n",
    "    # Crear cadena de configuración de naive bayes (no tiene hiperparámetros que ajustar)\n",
    "    configuracion = tipoNaiveBayes\n",
    "    configuracionYaExiste = existeConfiguracion(configuracion)\n",
    "\n",
    "    # Si no existe configuración en bitácora, agregar configuración a bitácora y métricas    \n",
    "    if(not configuracionYaExiste):\n",
    "        # Calcular errores de predicción para entrenamiento y test\n",
    "        y_pred_train = predecirNaiveBayes(diccionarioProbabilidades, X)\n",
    "        y_pred_test = predecirNaiveBayes(diccionarioProbabilidades, X_test)\n",
    "        manejarBitacora(tipoNaiveBayes, configuracion, y, y_pred_train, y_test, y_pred_test)\n",
    "\n",
    "    return diccionarioProbabilidades, configuracion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 859,
   "id": "crazy-newman",
   "metadata": {},
   "outputs": [],
   "source": [
    "def guardarInformacionConNumpy(informacion, configuracion):\n",
    "    directorioModelos = 'modelos/'\n",
    "    extensionModelos = '.modelo'\n",
    "    np.save(directorioModelos + configuracion + extensionModelos, informacion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "human-knife",
   "metadata": {},
   "source": [
    "Como el modelo de Naive Bayes implementado no tiene hiperparámetros que entrenar, se construirá el diccionario de probabilidades y se guardará directamente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 860,
   "id": "radical-corporation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>configuracion</th>\n",
       "      <th>accuracy_train</th>\n",
       "      <th>accuracy_test</th>\n",
       "      <th>f1_score_train</th>\n",
       "      <th>f1_score_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>NaiveBayes</td>\n",
       "      <td>0.778559</td>\n",
       "      <td>0.776224</td>\n",
       "      <td>0.697115</td>\n",
       "      <td>0.746032</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   configuracion  accuracy_train  accuracy_test  f1_score_train  f1_score_test\n",
       "92    NaiveBayes        0.778559       0.776224        0.697115       0.746032"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "probabilidadesNaiveBayes, configuracionNaiveBayes = crearEvaluarNaiveBayes(caracteristicasConOheEstandarizadas, etiquetasConOhe_train, \\\n",
    "                          caracteristicasConOheEstandarizadas_test, etiquetasConOhe_test)\n",
    "guardarInformacionConNumpy([probabilidadesNaiveBayes], configuracionNaiveBayes)\n",
    "mostrarResultadosBitacora(tipoNaiveBayes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mounted-parallel",
   "metadata": {},
   "source": [
    "## Regresión Logística"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "id": "responsible-payday",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcularLogits(parametros, matrizX):\n",
    "    return tf.matmul(matrizX, parametros)\n",
    "\n",
    "def calcularSigmoid(logits):\n",
    "    return tf.math.sigmoid(logits)\n",
    "\n",
    "def agregarColumnaUnos(x):\n",
    "    if len(x.shape) < 2:\n",
    "        x = np.array(x, dtype=np.float32).reshape(x.shape[0], -1)\n",
    "        \n",
    "    return np.append(x, np.ones(x.shape[0]).reshape(-1,1), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "id": "living-eight",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RegresionLogisticaPorMiniBatchGradientDescent(trainX, trainY, testX, testY, epochs, learningRate, batchSize = -1, alpha = 0.0):\n",
    "    # Agregar columna de sesgo (con unos)\n",
    "    trainX = agregarColumnaUnos(trainX)\n",
    "    # Se asume que columna de etiquetas ya se encuentra codificada\n",
    "    yOneHot = trainY\n",
    "    # Cantidad de datos, características y etiquetas enviadas\n",
    "    cantidadDatos = trainX.shape[0]\n",
    "    numCaracteristicas = trainX.shape[1]\n",
    "    # Parámetros\n",
    "    parametros = tf.Variable(np.zeros((numCaracteristicas, 1), dtype=np.float32), name=\"Parametros\")\n",
    "    # Dar formato adecuado a información de validación\n",
    "    testX = agregarColumnaUnos(testX)\n",
    "    # Se asume que columna de etiquetas ya se encuentra codificada\n",
    "    yTestOneHot = testY\n",
    "    \n",
    "    # Si el batch size es mayor que la cantidad de datos, simplemente se usa la cantidad de datos\n",
    "    batchSizeInicial = batchSize\n",
    "    if(batchSize > cantidadDatos or batchSize < 0):\n",
    "        batchSize = cantidadDatos\n",
    "    # Se determina número de iteraciones\n",
    "    totalIteraciones = cantidadDatos // batchSize\n",
    "    directorioModelo = './summaries/RegresionLogisticaMiniBatchGradientDescent' \\\n",
    "        + '_epochs=' + str(epochs) + '_lr=' + str(learningRate) + '_batchSize=' + str(batchSizeInicial) + '_alpha=' + str(alpha)\n",
    "\n",
    "    writer = tf.summary.create_file_writer(directorioModelo)\n",
    "    with writer.as_default():\n",
    "        for epoch in range(epochs):\n",
    "            for batch in range(totalIteraciones):\n",
    "                inicioBatch = batch * batchSize\n",
    "                finBatch = inicioBatch + batchSize\n",
    "                if (cantidadDatos - finBatch) < batchSize:\n",
    "                    finBatch = cantidadDatos\n",
    "                miniBatchX =  np.array(trainX[inicioBatch:finBatch,:], dtype=np.float32)\n",
    "                miniBatchY = np.array(yOneHot[inicioBatch:finBatch], dtype=np.float32)\n",
    "                \n",
    "                with tf.GradientTape() as grad_tape:\n",
    "                    grad_tape.watch(parametros)\n",
    "                    with tf.name_scope(\"Prediccion\"):\n",
    "                        logits = calcularLogits(parametros, miniBatchX)\n",
    "                    with tf.name_scope(\"Error\"):\n",
    "                        error = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(miniBatchY, logits))\n",
    "                        error = tf.reduce_mean(error + alpha * tf.math.abs(parametros))\n",
    "                \n",
    "                with tf.name_scope(\"Gradiente\"):\n",
    "                    gradiente = grad_tape.gradient(error, parametros)\n",
    "                \n",
    "                with tf.name_scope(\"Actualizar_Parametros\"):\n",
    "                    parametros.assign(parametros - learningRate * gradiente)\n",
    "                    \n",
    "            # Calcular error general de modelo en data set de entrenamiento\n",
    "            logitsTrain = calcularLogits(parametros, np.array(trainX, dtype=np.float32))\n",
    "            trainError = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(np.array(yOneHot, dtype=np.float32), logitsTrain))\n",
    "            # Calcular error general de modelo en data set de test\n",
    "            logits = calcularLogits(parametros, np.array(testX, dtype=np.float32))\n",
    "            testError = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(np.array(yTestOneHot, dtype=np.float32), logits))\n",
    "            \n",
    "            # Añadir errores a summary de tensorboard\n",
    "            tf.summary.scalar(\"Train_Error\", trainError, epoch)\n",
    "            tf.summary.scalar(\"Test_Error\", testError, epoch)\n",
    "            writer.flush()\n",
    "        \n",
    "    writer.close()\n",
    "\n",
    "    return parametros.numpy(), directorioModelo.replace('./summaries/', ''), trainError.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "id": "upset-charge",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predecirRegresionLogistica(X, parametros):\n",
    "    X = agregarColumnaUnos(X)\n",
    "    logits = np.matmul(X, parametros)\n",
    "    return np.round(calcularSigmoid(logits).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "id": "amazing-details",
   "metadata": {},
   "outputs": [],
   "source": [
    "tipoRegresionLogistica = 'RegresionLogistica'\n",
    "# Función para la creación de modelo de support vector machine y registro de métricas en bitácora (en caso de no existir en la bitácora)\n",
    "def crearEvaluarRegresionLogistica(X, y, X_test, y_test, epochs, learningRate, batchSize = -1, alpha=0.0):\n",
    "    # Crear modelo con parámetros enviados\n",
    "    parametros, configuracion, errorEntrenamiento = RegresionLogisticaPorMiniBatchGradientDescent(X, y, X_test, y_test, \\\n",
    "                                                           epochs, learningRate, batchSize, alpha)\n",
    "    configuracionYaExiste = existeConfiguracion(configuracion)\n",
    "\n",
    "    # Si no existe configuración en bitácora, agregar configuración a bitácora y métricas    \n",
    "    if(not configuracionYaExiste):\n",
    "        # Calcular errores de predicción para entrenamiento y test\n",
    "        y_pred_train = predecirRegresionLogistica(X, parametros)\n",
    "        y_pred_test = predecirRegresionLogistica(X_test, parametros)\n",
    "        manejarBitacora(tipoRegresionLogistica, configuracion, y, y_pred_train, y_test, y_pred_test, errorEntrenamiento)\n",
    "\n",
    "    return parametros, configuracion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "geographic-appointment",
   "metadata": {},
   "source": [
    "### Primeras pruebas para encontrar modelo de regresión logística\n",
    "\n",
    "- Primero se buscará un modelo que tenga un alto nivel de predicción en los datos de entrenamiento.\n",
    "- Después se variará el coeficiente de regularización para aumentar el nivel de predicción en los datos de prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "id": "stock-burke",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>configuracion</th>\n",
       "      <th>accuracy_train</th>\n",
       "      <th>accuracy_test</th>\n",
       "      <th>f1_score_train</th>\n",
       "      <th>f1_score_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>RegresionLogisticaMiniBatchGradientDescent_epochs=500_lr=0.01_batchSize=32_alpha=0.0</td>\n",
       "      <td>0.804921</td>\n",
       "      <td>0.839161</td>\n",
       "      <td>0.716113</td>\n",
       "      <td>0.813008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>RegresionLogisticaMiniBatchGradientDescent_epochs=500_lr=0.01_batchSize=8_alpha=0.0</td>\n",
       "      <td>0.794376</td>\n",
       "      <td>0.825175</td>\n",
       "      <td>0.708229</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>RegresionLogisticaMiniBatchGradientDescent_epochs=500_lr=0.01_batchSize=128_alpha=0.0</td>\n",
       "      <td>0.797891</td>\n",
       "      <td>0.790210</td>\n",
       "      <td>0.664723</td>\n",
       "      <td>0.722222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>RegresionLogisticaMiniBatchGradientDescent_epochs=500_lr=0.001_batchSize=8_alpha=0.0</td>\n",
       "      <td>0.801406</td>\n",
       "      <td>0.783217</td>\n",
       "      <td>0.685237</td>\n",
       "      <td>0.720721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>RegresionLogisticaMiniBatchGradientDescent_epochs=500_lr=0.001_batchSize=32_alpha=0.0</td>\n",
       "      <td>0.757469</td>\n",
       "      <td>0.720280</td>\n",
       "      <td>0.557692</td>\n",
       "      <td>0.565217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>RegresionLogisticaMiniBatchGradientDescent_epochs=500_lr=0.01_batchSize=-1_alpha=0.0</td>\n",
       "      <td>0.731107</td>\n",
       "      <td>0.685315</td>\n",
       "      <td>0.477816</td>\n",
       "      <td>0.470588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>RegresionLogisticaMiniBatchGradientDescent_epochs=500_lr=0.0001_batchSize=8_alpha=0.0</td>\n",
       "      <td>0.715290</td>\n",
       "      <td>0.657343</td>\n",
       "      <td>0.425532</td>\n",
       "      <td>0.395062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>RegresionLogisticaMiniBatchGradientDescent_epochs=500_lr=0.001_batchSize=128_alpha=0.0</td>\n",
       "      <td>0.694200</td>\n",
       "      <td>0.643357</td>\n",
       "      <td>0.345865</td>\n",
       "      <td>0.354430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>RegresionLogisticaMiniBatchGradientDescent_epochs=500_lr=0.0001_batchSize=32_alpha=0.0</td>\n",
       "      <td>0.680141</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.289062</td>\n",
       "      <td>0.266667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>RegresionLogisticaMiniBatchGradientDescent_epochs=500_lr=0.0001_batchSize=128_alpha=0.0</td>\n",
       "      <td>0.680141</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.283465</td>\n",
       "      <td>0.266667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>RegresionLogisticaMiniBatchGradientDescent_epochs=500_lr=0.0001_batchSize=-1_alpha=0.0</td>\n",
       "      <td>0.680141</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.283465</td>\n",
       "      <td>0.266667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>RegresionLogisticaMiniBatchGradientDescent_epochs=500_lr=0.001_batchSize=-1_alpha=0.0</td>\n",
       "      <td>0.680141</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.289062</td>\n",
       "      <td>0.266667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                              configuracion  \\\n",
       "76     RegresionLogisticaMiniBatchGradientDescent_epochs=500_lr=0.01_batchSize=32_alpha=0.0   \n",
       "73      RegresionLogisticaMiniBatchGradientDescent_epochs=500_lr=0.01_batchSize=8_alpha=0.0   \n",
       "79    RegresionLogisticaMiniBatchGradientDescent_epochs=500_lr=0.01_batchSize=128_alpha=0.0   \n",
       "72     RegresionLogisticaMiniBatchGradientDescent_epochs=500_lr=0.001_batchSize=8_alpha=0.0   \n",
       "75    RegresionLogisticaMiniBatchGradientDescent_epochs=500_lr=0.001_batchSize=32_alpha=0.0   \n",
       "82     RegresionLogisticaMiniBatchGradientDescent_epochs=500_lr=0.01_batchSize=-1_alpha=0.0   \n",
       "71    RegresionLogisticaMiniBatchGradientDescent_epochs=500_lr=0.0001_batchSize=8_alpha=0.0   \n",
       "78   RegresionLogisticaMiniBatchGradientDescent_epochs=500_lr=0.001_batchSize=128_alpha=0.0   \n",
       "74   RegresionLogisticaMiniBatchGradientDescent_epochs=500_lr=0.0001_batchSize=32_alpha=0.0   \n",
       "77  RegresionLogisticaMiniBatchGradientDescent_epochs=500_lr=0.0001_batchSize=128_alpha=0.0   \n",
       "80   RegresionLogisticaMiniBatchGradientDescent_epochs=500_lr=0.0001_batchSize=-1_alpha=0.0   \n",
       "81    RegresionLogisticaMiniBatchGradientDescent_epochs=500_lr=0.001_batchSize=-1_alpha=0.0   \n",
       "0                                                                                       NaN   \n",
       "1                                                                                       NaN   \n",
       "2                                                                                       NaN   \n",
       "\n",
       "    accuracy_train  accuracy_test  f1_score_train  f1_score_test  \n",
       "76        0.804921       0.839161        0.716113       0.813008  \n",
       "73        0.794376       0.825175        0.708229       0.800000  \n",
       "79        0.797891       0.790210        0.664723       0.722222  \n",
       "72        0.801406       0.783217        0.685237       0.720721  \n",
       "75        0.757469       0.720280        0.557692       0.565217  \n",
       "82        0.731107       0.685315        0.477816       0.470588  \n",
       "71        0.715290       0.657343        0.425532       0.395062  \n",
       "78        0.694200       0.643357        0.345865       0.354430  \n",
       "74        0.680141       0.615385        0.289062       0.266667  \n",
       "77        0.680141       0.615385        0.283465       0.266667  \n",
       "80        0.680141       0.615385        0.283465       0.266667  \n",
       "81        0.680141       0.615385        0.289062       0.266667  \n",
       "0              NaN            NaN             NaN            NaN  \n",
       "1              NaN            NaN             NaN            NaN  \n",
       "2              NaN            NaN             NaN            NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "batchSizes = [8, 32, 128, -1]\n",
    "learningRates = [0.0001, 0.001, 0.01]\n",
    "for batchSize in batchSizes:\n",
    "    for lr in learningRates:\n",
    "        crearEvaluarRegresionLogistica(caracteristicasConOheEstandarizadas, etiquetasConOhe_train, \\\n",
    "                                       caracteristicasConOheEstandarizadas_test, etiquetasConOhe_test, \\\n",
    "                                       epochs=500, learningRate=lr, batchSize=batchSize, alpha=0.0)\n",
    "\n",
    "mostrarResultadosBitacora(tipoRegresionLogistica)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ambient-marking",
   "metadata": {},
   "source": [
    "Basado en los errores de entrenamiento y datos de prueba de las gráficas de abajo, y las métricas de rendimiento de la tabla de arriba; los modelos con mejores métricas de predicción en los datos de pruebas, no tienen mejores métricas en los datos de entrenamiento (no muestra overfitting). Se seguirá buscando modelos con mejores métricas en datos de entrenamiento sin aún variar el coeficiente de regularización. También se mira que un learning rate mayor, es mejor.\n",
    "\n",
    "<img src='Regresion_logistica_train_error.png'>\n",
    "<img src='Regresion_logistica_test_error.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "id": "sudden-pattern",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>configuracion</th>\n",
       "      <th>accuracy_train</th>\n",
       "      <th>accuracy_test</th>\n",
       "      <th>f1_score_train</th>\n",
       "      <th>f1_score_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>RegresionLogisticaMiniBatchGradientDescent_epochs=500_lr=0.01_batchSize=32_alpha=0.0</td>\n",
       "      <td>0.804921</td>\n",
       "      <td>0.839161</td>\n",
       "      <td>0.716113</td>\n",
       "      <td>0.813008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>RegresionLogisticaMiniBatchGradientDescent_epochs=500_lr=0.02_batchSize=64_alpha=0.0</td>\n",
       "      <td>0.801406</td>\n",
       "      <td>0.839161</td>\n",
       "      <td>0.710997</td>\n",
       "      <td>0.813008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>RegresionLogisticaMiniBatchGradientDescent_epochs=500_lr=0.1_batchSize=64_alpha=0.0</td>\n",
       "      <td>0.794376</td>\n",
       "      <td>0.825175</td>\n",
       "      <td>0.708229</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>RegresionLogisticaMiniBatchGradientDescent_epochs=500_lr=0.02_batchSize=16_alpha=0.0</td>\n",
       "      <td>0.794376</td>\n",
       "      <td>0.825175</td>\n",
       "      <td>0.708229</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>RegresionLogisticaMiniBatchGradientDescent_epochs=500_lr=0.01_batchSize=8_alpha=0.0</td>\n",
       "      <td>0.794376</td>\n",
       "      <td>0.825175</td>\n",
       "      <td>0.708229</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>RegresionLogisticaMiniBatchGradientDescent_epochs=500_lr=0.05_batchSize=64_alpha=0.0</td>\n",
       "      <td>0.794376</td>\n",
       "      <td>0.825175</td>\n",
       "      <td>0.708229</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>RegresionLogisticaMiniBatchGradientDescent_epochs=500_lr=0.05_batchSize=16_alpha=0.0</td>\n",
       "      <td>0.794376</td>\n",
       "      <td>0.825175</td>\n",
       "      <td>0.708229</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>RegresionLogisticaMiniBatchGradientDescent_epochs=500_lr=0.02_batchSize=4_alpha=0.0</td>\n",
       "      <td>0.794376</td>\n",
       "      <td>0.825175</td>\n",
       "      <td>0.708229</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>RegresionLogisticaMiniBatchGradientDescent_epochs=500_lr=0.05_batchSize=4_alpha=0.0</td>\n",
       "      <td>0.794376</td>\n",
       "      <td>0.825175</td>\n",
       "      <td>0.708229</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>RegresionLogisticaMiniBatchGradientDescent_epochs=500_lr=0.1_batchSize=16_alpha=0.0</td>\n",
       "      <td>0.794376</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.708229</td>\n",
       "      <td>0.793651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>RegresionLogisticaMiniBatchGradientDescent_epochs=500_lr=0.1_batchSize=4_alpha=0.0</td>\n",
       "      <td>0.794376</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.708229</td>\n",
       "      <td>0.790323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>RegresionLogisticaMiniBatchGradientDescent_epochs=500_lr=0.01_batchSize=128_alpha=0.0</td>\n",
       "      <td>0.797891</td>\n",
       "      <td>0.790210</td>\n",
       "      <td>0.664723</td>\n",
       "      <td>0.722222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>RegresionLogisticaMiniBatchGradientDescent_epochs=500_lr=0.001_batchSize=8_alpha=0.0</td>\n",
       "      <td>0.801406</td>\n",
       "      <td>0.783217</td>\n",
       "      <td>0.685237</td>\n",
       "      <td>0.720721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>RegresionLogisticaMiniBatchGradientDescent_epochs=500_lr=0.001_batchSize=32_alpha=0.0</td>\n",
       "      <td>0.757469</td>\n",
       "      <td>0.720280</td>\n",
       "      <td>0.557692</td>\n",
       "      <td>0.565217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>RegresionLogisticaMiniBatchGradientDescent_epochs=500_lr=0.01_batchSize=-1_alpha=0.0</td>\n",
       "      <td>0.731107</td>\n",
       "      <td>0.685315</td>\n",
       "      <td>0.477816</td>\n",
       "      <td>0.470588</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                            configuracion  \\\n",
       "76   RegresionLogisticaMiniBatchGradientDescent_epochs=500_lr=0.01_batchSize=32_alpha=0.0   \n",
       "89   RegresionLogisticaMiniBatchGradientDescent_epochs=500_lr=0.02_batchSize=64_alpha=0.0   \n",
       "91    RegresionLogisticaMiniBatchGradientDescent_epochs=500_lr=0.1_batchSize=64_alpha=0.0   \n",
       "86   RegresionLogisticaMiniBatchGradientDescent_epochs=500_lr=0.02_batchSize=16_alpha=0.0   \n",
       "73    RegresionLogisticaMiniBatchGradientDescent_epochs=500_lr=0.01_batchSize=8_alpha=0.0   \n",
       "90   RegresionLogisticaMiniBatchGradientDescent_epochs=500_lr=0.05_batchSize=64_alpha=0.0   \n",
       "87   RegresionLogisticaMiniBatchGradientDescent_epochs=500_lr=0.05_batchSize=16_alpha=0.0   \n",
       "83    RegresionLogisticaMiniBatchGradientDescent_epochs=500_lr=0.02_batchSize=4_alpha=0.0   \n",
       "84    RegresionLogisticaMiniBatchGradientDescent_epochs=500_lr=0.05_batchSize=4_alpha=0.0   \n",
       "88    RegresionLogisticaMiniBatchGradientDescent_epochs=500_lr=0.1_batchSize=16_alpha=0.0   \n",
       "85     RegresionLogisticaMiniBatchGradientDescent_epochs=500_lr=0.1_batchSize=4_alpha=0.0   \n",
       "79  RegresionLogisticaMiniBatchGradientDescent_epochs=500_lr=0.01_batchSize=128_alpha=0.0   \n",
       "72   RegresionLogisticaMiniBatchGradientDescent_epochs=500_lr=0.001_batchSize=8_alpha=0.0   \n",
       "75  RegresionLogisticaMiniBatchGradientDescent_epochs=500_lr=0.001_batchSize=32_alpha=0.0   \n",
       "82   RegresionLogisticaMiniBatchGradientDescent_epochs=500_lr=0.01_batchSize=-1_alpha=0.0   \n",
       "\n",
       "    accuracy_train  accuracy_test  f1_score_train  f1_score_test  \n",
       "76        0.804921       0.839161        0.716113       0.813008  \n",
       "89        0.801406       0.839161        0.710997       0.813008  \n",
       "91        0.794376       0.825175        0.708229       0.800000  \n",
       "86        0.794376       0.825175        0.708229       0.800000  \n",
       "73        0.794376       0.825175        0.708229       0.800000  \n",
       "90        0.794376       0.825175        0.708229       0.800000  \n",
       "87        0.794376       0.825175        0.708229       0.800000  \n",
       "83        0.794376       0.825175        0.708229       0.800000  \n",
       "84        0.794376       0.825175        0.708229       0.800000  \n",
       "88        0.794376       0.818182        0.708229       0.793651  \n",
       "85        0.794376       0.818182        0.708229       0.790323  \n",
       "79        0.797891       0.790210        0.664723       0.722222  \n",
       "72        0.801406       0.783217        0.685237       0.720721  \n",
       "75        0.757469       0.720280        0.557692       0.565217  \n",
       "82        0.731107       0.685315        0.477816       0.470588  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "batchSizes = [4, 16, 64]\n",
    "learningRates = [0.02, 0.05, 0.1]\n",
    "for batchSize in batchSizes:\n",
    "    for lr in learningRates:\n",
    "        crearEvaluarRegresionLogistica(caracteristicasConOheEstandarizadas, etiquetasConOhe_train, \\\n",
    "                                       caracteristicasConOheEstandarizadas_test, etiquetasConOhe_test, \\\n",
    "                                       epochs=500, learningRate=lr, batchSize=batchSize, alpha=0.0)\n",
    "\n",
    "mostrarResultadosBitacora(tipoRegresionLogistica)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "committed-masters",
   "metadata": {},
   "source": [
    "Basado en errores (imágenes de abajo) y métricas (tabla de arriba); ninguno de los modelos anteriores desarrolló overfitting (todos mostraron un balance bastante fuerte entre las métricas en los datos de entrenamiento con respecto a los datos de prueba). Por lo que se escogerá el modelo con los hiperparámetros: epochs = __500__, learning rate = __0.01__, batch size = __32__, sin regularización.\n",
    "\n",
    "<img src='Regresion_logistica_train_error2.png'>\n",
    "<img src='Regresion_logistica_test_error2.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "id": "every-church",
   "metadata": {},
   "outputs": [],
   "source": [
    "parametrosRegresionLogistica, configuracionRegresionLogistica = \\\n",
    "    crearEvaluarRegresionLogistica(caracteristicasConOheEstandarizadas, etiquetasConOhe_train, \\\n",
    "                    caracteristicasConOheEstandarizadas_test, etiquetasConOhe_test, \\\n",
    "                                       500, learningRate=0.01, batchSize=32, alpha=0.0)\n",
    "guardarInformacionConNumpy(parametrosRegresionLogistica, configuracionRegresionLogistica)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hidden-sympathy",
   "metadata": {},
   "source": [
    "## Tabla de predicciones\n",
    "\n",
    "Se mostrará la tabla de predicciones de cada modelo y su combinación. Se optó por aplicar una media ponderada en lugar de moda porque podrían haber casos en que no había votación mayoritaria por ser un número par de modelos (4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 886,
   "id": "level-negative",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediccionesArbolDecision = modeloArbolDecision.predict(caracteristicasConOheEstandarizadas_test)\n",
    "prediccionesSVM = modeloSVM.predict(caracteristicasConOheEstandarizadas_test)\n",
    "prediccionesNaiveBayes = predecirNaiveBayes(probabilidadesNaiveBayes, caracteristicasConOheEstandarizadas_test)\n",
    "prediccionesRegresionLogistica = predecirRegresionLogistica(caracteristicasConOheEstandarizadas_test, parametrosRegresionLogistica)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 887,
   "id": "fossil-bottle",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediccionesNaiveBayes = prediccionesNaiveBayes.reshape(prediccionesNaiveBayes.shape[0])\n",
    "prediccionesRegresionLogistica = prediccionesRegresionLogistica.reshape(prediccionesRegresionLogistica.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 889,
   "id": "developmental-norway",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combinacionPredicciones(yArbolDecision, ySVM, yNaiveBayes, yRegresionLogistica):\n",
    "    prediccionTestArbolDecision = 0.8322\n",
    "    prediccionTestSVM = 0.8182\n",
    "    prediccionTestRegresionLogistica = 0.8130\n",
    "    prediccionTestNaiveBayes = 0.7762\n",
    "    totalPrediccion = prediccionTestArbolDecision + prediccionTestSVM + prediccionTestRegresionLogistica + prediccionTestNaiveBayes\n",
    "    pesoArbolDecision =  prediccionTestArbolDecision / totalPrediccion\n",
    "    pesoSVM = prediccionTestSVM / totalPrediccion\n",
    "    pesoRegresionLogistica = prediccionTestRegresionLogistica / totalPrediccion\n",
    "    pesoNaiveBayes = prediccionTestNaiveBayes / totalPrediccion\n",
    "    prediccionFinal = pesoArbolDecision * yArbolDecision \\\n",
    "                    + pesoSVM * ySVM \\\n",
    "                    + pesoRegresionLogistica * yRegresionLogistica \\\n",
    "                    + pesoNaiveBayes * yNaiveBayes\n",
    "    return np.round(prediccionFinal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 907,
   "id": "theoretical-pointer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Árbol Decisión</th>\n",
       "      <th>SVM</th>\n",
       "      <th>Naive Bayes</th>\n",
       "      <th>Regresión Logística</th>\n",
       "      <th>Combinación Final</th>\n",
       "      <th>Resultado Real</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>143 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Árbol Decisión  SVM  Naive Bayes  Regresión Logística  Combinación Final  \\\n",
       "0               0.0  0.0          0.0                  0.0                0.0   \n",
       "1               0.0  0.0          0.0                  0.0                0.0   \n",
       "2               0.0  0.0          0.0                  0.0                0.0   \n",
       "3               0.0  0.0          0.0                  0.0                0.0   \n",
       "4               0.0  0.0          0.0                  0.0                0.0   \n",
       "..              ...  ...          ...                  ...                ...   \n",
       "138             0.0  0.0          0.0                  0.0                0.0   \n",
       "139             0.0  0.0          0.0                  0.0                0.0   \n",
       "140             0.0  0.0          0.0                  0.0                0.0   \n",
       "141             1.0  0.0          0.0                  0.0                0.0   \n",
       "142             1.0  1.0          1.0                  1.0                1.0   \n",
       "\n",
       "     Resultado Real  \n",
       "0               0.0  \n",
       "1               0.0  \n",
       "2               0.0  \n",
       "3               0.0  \n",
       "4               0.0  \n",
       "..              ...  \n",
       "138             0.0  \n",
       "139             0.0  \n",
       "140             0.0  \n",
       "141             1.0  \n",
       "142             1.0  \n",
       "\n",
       "[143 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "combinacionPred = combinacionPredicciones(prediccionesArbolDecision, prediccionesSVM, prediccionesNaiveBayes, prediccionesRegresionLogistica)\n",
    "etiquetasConOheMostrar = np.array(etiquetasConOhe_test)\n",
    "etiquetasConOheMostrar = etiquetasConOheMostrar.reshape(etiquetasConOheMostrar.shape[0])\n",
    "resultados = np.array([prediccionesArbolDecision, prediccionesSVM, prediccionesNaiveBayes, prediccionesRegresionLogistica, \\\n",
    "                       combinacionPred, etiquetasConOheMostrar]).transpose().tolist()\n",
    "tablaResultados = pd.DataFrame(resultados, columns=['Árbol Decisión','SVM','Naive Bayes','Regresión Logística', \\\n",
    "                                                    'Combinación Final', 'Resultado Real'])\n",
    "display(tablaResultados)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "present-collaboration",
   "metadata": {},
   "source": [
    "## Tabla de métricas de evaluación entre combinación de predicciones vs resultados reales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 908,
   "id": "increased-korea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Métrica</th>\n",
       "      <th>valor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.825175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.819672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Recall</td>\n",
       "      <td>0.781250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>F1-score</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Métrica     valor\n",
       "0   Accuracy  0.825175\n",
       "1  Precision  0.819672\n",
       "2     Recall  0.781250\n",
       "3   F1-score  0.800000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "metricas = []\n",
    "metricas.append(['Accuracy', mts.accuracy_score(etiquetasConOheMostrar, combinacionPred)])\n",
    "metricas.append(['Precision', mts.precision_score(etiquetasConOheMostrar, combinacionPred)])\n",
    "metricas.append(['Recall', mts.recall_score(etiquetasConOheMostrar, combinacionPred)])\n",
    "metricas.append(['F1-score', mts.f1_score(etiquetasConOheMostrar, combinacionPred)])\n",
    "tablaMetricas = pd.DataFrame(metricas, columns=['Métrica', 'valor'])\n",
    "display(tablaMetricas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "documented-apartment",
   "metadata": {},
   "source": [
    "## Conclusiones\n",
    "\n",
    "- La combinación de todas las predicciones tuvo como resultado final una exactitud del __82.52%__ en los datos de prueba, mayor al 80% requerido; por lo que los modelos y el método de combinación serán utilizados en la parte de deployment (segundo notebook).\n",
    "- La estandarización de la información es una técnica de pre-procesamiento de información que ayudó en la métrica de exactitud (accuracy) especialmente en el modelo de regresión logística.\n",
    "- Tensorboard ayudó mucho en la comprensión de la evolución del error y exactitud de la regresión logística con los diferentes hiper-parámetros que se probaron.\n",
    "- Se pudo comprobar claramente como un árbol de decisión puede sobreajustarse fácilmente y que limitar el número de profundidad ayuda mucho a que este modelo disminuya su sobreajuste y mejore su nivel de predicción."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
